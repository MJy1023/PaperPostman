[
  {
    "id": null,
    "title": "Generalized Linear Mode Connectivity for Transformers",
    "authors": [
      "Alexander Theus",
      "Alessandro Cabodi",
      "Sotiris Anagnostidis",
      "Antonio Orvieto",
      "Sidak Pal Singh",
      "Valentina Boeva"
    ],
    "affiliations": [],
    "summary": "Understanding the geometry of neural network loss landscapes is a central question in deep learning, with implications for generalization and optimization. A striking phenomenon is $\\textit{linear mode connectivity}$ (LMC), where independently trained models can be connected by low- or zero-barrier paths, despite appearing to lie in separate loss basins. However, this is often obscured by symmetries in parameter space—such as neuron permutations—which make functionally equivalent models appear dissimilar. Prior work has predominantly focused on neuron reordering through permutations, but such approaches are limited in scope and fail to capture the richer symmetries exhibited by modern architectures such as Transformers. In this work, we introduce a unified framework that captures four symmetry classes—permutations, semi-permutations, orthogonal transformations, and general invertible maps—broadening the set of valid reparameterizations and subsuming many previous approaches as special cases. Crucially, this generalization enables, for the first time, the discovery of low- and zero-barrier linear interpolation paths between independently trained Vision Transformers and GPT-2 models. Furthermore, our framework extends beyond pairwise alignment, to multi-model and width-heterogeneous settings, enabling alignment across architectures of different sizes. These results reveal deeper structure in the loss landscape and underscore the importance of symmetry-aware analysis for understanding model space geometry.",
    "link": "/venue/KurYdcCbjv@OpenReview",
    "published_date": null,
    "conference": "NeurIPS.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "Deep Compositional Phase Diffusion for Long Motion Sequence Generation",
    "authors": [
      "Ho Yin Au",
      "Jie Chen",
      "Junkun Jiang",
      "Jingyu Xiang"
    ],
    "affiliations": [],
    "summary": "Recent research on motion generation has shown significant progress in generating semantically aligned motion with singular semantics. However, when employing these models to create composite sequences containing multiple semantically generated motion clips, they often struggle to preserve the continuity of motion dynamics at the transition boundaries between clips, resulting in awkward transitions and abrupt artifacts. To address these challenges, we present Compositional Phase Diffusion, which leverages the Semantic Phase Diffusion Module (SPDM) and Transitional Phase Diffusion Module (TPDM) to progressively incorporate semantic guidance and phase details from adjacent motion clips into the diffusion process. Specifically, SPDM and TPDM operate within the latent motion frequency domain established by the pre-trained Action-Centric Motion Phase Autoencoder (ACT-PAE). This allows them to learn semantically important and transition-aware phase information from variable-length motion clips during training. Experimental results demonstrate the competitive performance of our proposed framework in generating compositional motion sequences that align semantically with the input conditions, while preserving phase transitional continuity between preceding and succeeding motion clips. Additionally, motion inbetweening task is made possible by keeping the phase parameter of the input motion sequences fixed throughout the diffusion process, showcasing the potential for extending the proposed framework to accommodate various application scenarios. Codes are available at https://github.com/asdryau/TransPhase.",
    "link": "/venue/jzPQRbGkAq@OpenReview",
    "published_date": null,
    "conference": "NeurIPS.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "GnnXemplar: Exemplars to Explanations - Natural Language Rules for Global GNN Interpretability",
    "authors": [
      "Burouj Armgaan",
      "Eshan Jain",
      "Harsh Pandey",
      "Mahesh Chandran",
      "Sayan Ranu"
    ],
    "affiliations": [],
    "summary": "Graph Neural Networks (GNNs) are widely used for node classification, yet their opaque decision-making limits trust and adoption. While local explanations offer insights into individual predictions, global explanation methods—those that characterize an entire class—remain underdeveloped. Existing global explainers rely on motif discovery in small graphs, an approach that breaks down in large, real-world settings where subgraph repetition is rare, node attributes are high-dimensional, and predictions arise from complex structure-attribute interactions. We propose GnnXemplar, a novel global explainer inspired from Exemplar Theory from cognitive science. GnnXemplar identifies representative nodes in the GNN embedding space—exemplars—and explains predictions using natural language rules derived from their neighborhoods. Exemplar selection is framed as a coverage maximization problem over reverse $k$-nearest neighbors, for which we provide an efficient greedy approximation. To derive interpretable rules, we employ a self-refining prompt strategy using large language models (LLMs). Experiments across diverse benchmarks show that GnnXemplar significantly outperforms existing methods in fidelity, scalability, and human interpretability, as validated by a user study with 60 participants.",
    "link": "/venue/eafIjoZAHm@OpenReview",
    "published_date": null,
    "conference": "NeurIPS.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "RAG4GFM: Bridging Knowledge Gaps in Graph Foundation Models through Graph Retrieval Augmented Generation",
    "authors": [
      "Xingliang Wang",
      "Zemin Liu",
      "Junxiao Han",
      "Shuiguang Deng"
    ],
    "affiliations": [],
    "summary": "Graph Foundation Models (GFMs) have demonstrated remarkable potential across graph learning tasks but face significant challenges in knowledge updating and reasoning faithfulness. To address these issues, we introduce the Retrieval-Augmented Generation (RAG) paradigm for GFMs, which leverages graph knowledge retrieval. We propose RAG4GFM, an end-to-end framework that seamlessly integrates multi-level graph indexing, task-aware retrieval, and graph fusion enhancement. RAG4GFM implements a hierarchical graph indexing architecture, enabling multi-granular graph indexing while achieving efficient logarithmic-time retrieval. The task-aware retriever implements adaptive retrieval strategies for node, edge, and graph-level tasks to surface structurally and semantically relevant evidence. The graph fusion enhancement module fuses retrieved graph features with query features and augments the topology with sparse adjacency links that preserve structural and semantic proximity, yielding a fused graph for GFM inference. Extensive experiments conducted across diverse GFM applications demonstrate that RAG4GFM significantly enhances both the efficiency of knowledge updating and reasoning faithfulness\\footnote{Code: \\url{https://github.com/Matrixmax/RAG4GFM}.}.",
    "link": "/venue/tirl2l9oKg@OpenReview",
    "published_date": null,
    "conference": "NeurIPS.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "Agnostic Active Learning Is Always Better Than Passive Learning",
    "authors": [
      "Steve Hanneke"
    ],
    "affiliations": [],
    "summary": "We sharply characterize the optimal first-order query complexity of agnostic active learning for all concept classes, and propose a new general active learning algorithm which achieves it. Remarkably, the optimal query complexity admits a leading term which is always strictly smaller than the sample complexity of passive supervised learning (by a factor proportional to the best-in-class error rate). This was not previously known to be possible in the agnostic setting. For comparison, in all previous general analyses, the leading term exhibits an additional factor, such as the disagreement coefficient or related complexity measure, and therefore only provides improvements over passive learning in restricted cases. The present work completely removes such factors from the leading term, implying that $\\textit{every}$ concept class benefits from active learning in the non-realizable case. The results established in this work resolve an important long-standing open question central to the past two decades of research on the theory of agnostic active learning.",
    "link": "/venue/XPe55Uffd7@OpenReview",
    "published_date": null,
    "conference": "NeurIPS.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "Learning Linear Attention in Polynomial Time",
    "authors": [
      "Morris Yau",
      "Ekin Akyürek",
      "Jiayuan Mao",
      "Joshua B. Tenenbaum",
      "Stefanie Jegelka",
      "Jacob Andreas"
    ],
    "affiliations": [],
    "summary": "Previous research has explored the expressivity of Transformer models in simulating Boolean circuits or Turing machines. However, the efficient learnability of Transformers from data has remained an open question. Our study addresses this gap by providing the first polynomial-time learnability results (specifically strong, agnostic PAC learning) for single-layer Transformers with linear attention. We show that learning the optimal multi head linear attention can be recast as finding the optimal kernel predictor in a suitably defined RKHS. Moving to generalization, we construct an algorithm that, given a dataset, checks in polynomial time whether the set of best fit multi head linear attention networks on this data all perform an identical computation--a powerful notion for out of distribution generalization. We empirically validate our theoretical findings on several canonical tasks: learning random linear attention networks, key--value associations, and learning to execute finite automata. Our findings bridge a critical gap between theoretical expressivity and learnability of Transformer models.",
    "link": "/venue/QN0E0KX2LM@OpenReview",
    "published_date": null,
    "conference": "NeurIPS.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "Optimal Mistake Bounds for Transductive Online Learning",
    "authors": [
      "Zachary Chase",
      "Steve Hanneke",
      "Shay Moran",
      "Jonathan Shafer"
    ],
    "affiliations": [],
    "summary": "We resolve a 30-year-old open problem concerning the power of unlabeled data in online learning by tightly quantifying the gap between transductive and standard online learning. We prove that for every concept class $\\mathcal{H}$ with Littlestone dimension $d$, the transductive mistake bound is at least $\\Omega(\\sqrt{d})$. This establishes an exponential improvement over previous lower bounds of $\\Omega(\\log \\log d)$, $\\Omega(\\sqrt{\\log d})$, and $\\Omega(\\log d)$, respectively due to Ben-David, Kushilevitz, and Mansour (1995, 1997) and Hanneke, Moran, and Shafer (2023). We also show that our bound is tight: for every $d$, there exists a class of Littlestone dimension $d$ with transductive mistake bound $O(\\sqrt{d})$. Our upper bound also improves the previous best known upper bound of $(2/3) \\cdot d$ from Ben-David et al. (1997). These results demonstrate a quadratic gap between transductive and standard online learning, thereby highlighting the benefit of advanced access to the unlabeled instance sequence. This stands in stark contrast to the PAC setting, where transductive and standard learning exhibit similar sample complexities.",
    "link": "/venue/EoebmBe9fG@OpenReview",
    "published_date": null,
    "conference": "NeurIPS.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "State Entropy Regularization for Robust Reinforcement Learning",
    "authors": [
      "Yonatan Ashlag",
      "Uri Koren",
      "Mirco Mutti",
      "Esther Derman",
      "Pierre-Luc Bacon",
      "Shie Mannor"
    ],
    "affiliations": [],
    "summary": "State entropy regularization has empirically shown better exploration and sample complexity in reinforcement learning (RL). However, its theoretical guarantees have not been studied. In this paper, we show that state entropy regularization improves robustness to structured and spatially correlated perturbations. These types of variation are common in transfer learning but often overlooked by standard robust RL methods, which typically focus on small, uncorrelated changes. We provide a comprehensive characterization of these robustness properties, including formal guarantees under reward and transition uncertainty, as well as settings where the method performs poorly. Much of our analysis contrasts state entropy with the widely used policy entropy regularization, highlighting their different benefits. Finally, from a practical standpoint, we illustrate that compared with policy entropy, the robustness advantages of state entropy are more sensitive to the number of rollouts used for policy evaluation.",
    "link": "/venue/rtG7n93Ru8@OpenReview",
    "published_date": null,
    "conference": "NeurIPS.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "On the Closed-Form of Flow Matching: Generalization Does Not Arise from Target Stochasticity",
    "authors": [
      "Quentin Bertrand",
      "Anne Gagneux",
      "Mathurin Massias",
      "Rémi Emonet"
    ],
    "affiliations": [],
    "summary": "Modern deep generative models can now produce high-quality synthetic samples that are often indistinguishable from real training data. A growing body of research aims to understand why recent methods, such as diffusion and flow matching techniques, generalize so effectively. Among the proposed explanations are the inductive biases of deep learning architectures and the stochastic nature of the conditional flow matching loss. In this work, we rule out the noisy nature of the loss as a key factor driving generalization in flow matching. First, we empirically show that in high-dimensional settings, the stochastic and closed-form versions of the flow matching loss yield nearly equivalent losses. Then, using state-of-the-art flow matching models on standard image datasets, we demonstrate that both variants achieve comparable statistical performance, with the surprising observation that using the closed-form can even improve performance.",
    "link": "/venue/kVz9uvqUna@OpenReview",
    "published_date": null,
    "conference": "NeurIPS.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "Why Diffusion Models Don’t Memorize: The Role of Implicit Dynamical Regularization in Training",
    "authors": [
      "Tony Bonnaire",
      "Raphaël Urfin",
      "Giulio Biroli",
      "Marc Mezard"
    ],
    "affiliations": [],
    "summary": "Diffusion models have achieved remarkable success across a wide range of generative tasks. A key challenge is understanding the mechanisms that prevent their memorization of training data and allow generalization. In this work, we investigate the role of the training dynamics in the transition from generalization to memorization. Through extensive experiments and theoretical analysis, we identify two distinct timescales: an early time $\\tau_\\mathrm{gen}$ at which models begin to generate high-quality samples, and a later time $\\tau_\\mathrm{mem}$ beyond which memorization emerges. Crucially, we find that $\\tau_\\mathrm{mem}$ increases linearly with the training set size $n$, while $\\tau_\\mathrm{gen}$ remains constant. This creates a growing window of training times with $n$ where models generalize effectively, despite showing strong memorization if training continues beyond it. It is only when $n$ becomes larger than a model-dependent threshold that overfitting disappears at infinite training times. These findings reveal a form of implicit dynamical regularization in the training dynamics, which allow to avoid memorization even in highly overparameterized settings. Our results are supported by numerical experiments with standard U-Net architectures on realistic and synthetic datasets, and by a theoretical analysis using a tractable random features model studied in the high-dimensional limit.",
    "link": "/venue/BSZqpqgqM0@OpenReview",
    "published_date": null,
    "conference": "NeurIPS.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "Adjoint Schrödinger Bridge Sampler",
    "authors": [
      "Guan-Horng Liu",
      "Jaemoo Choi",
      "Yongxin Chen",
      "Benjamin Kurt Miller",
      "Ricky T. Q. Chen"
    ],
    "affiliations": [],
    "summary": "Computational methods for learning to sample from the Boltzmann distribution—where the target distribution is known only up to an unnormalized energy function—have advanced significantly recently. Due to the lack of explicit target samples, however, prior diffusion-based methods, known as _diffusion samplers_, often require importance-weighted estimation or complicated learning processes. Both trade off scalability with extensive evaluations of the energy and model, thereby limiting their practical usage. In this work, we propose **Adjoint Schrödinger Bridge Sampler (ASBS)**, a new diffusion sampler that employs simple and scalable matching-based objectives yet without the need to estimate target samples during training. ASBS is grounded on a mathematical model—the Schrödinger Bridge—which enhances sampling efficiency via kinetic-optimal transportation. Through a new lens of stochastic optimal control theory, we demonstrate how SB-based diffusion samplers can be learned at scale via Adjoint Matching and prove convergence to the global solution. Notably, ASBS generalizes the recent Adjoint Sampling (Havens et al., 2025) to arbitrary source distributions by relaxing the so-called memoryless condition that largely restricts the design space. Through extensive experiments, we demonstrate the effectiveness of ASBS on sampling from classical energy functions, amortized conformer generation, and molecular Boltzmann distributions. Codes are available at https://github.com/facebookresearch/adjoint_samplers",
    "link": "/venue/rMhQBlhh4c@OpenReview",
    "published_date": null,
    "conference": "NeurIPS.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "Breaking the Performance Ceiling in Reinforcement Learning requires Inference Strategies",
    "authors": [
      "Felix Chalumeau",
      "Daniel Rajaonarivonivelomanantsoa",
      "Ruan John de Kock",
      "Juan Claude Formanek",
      "Sasha Abramowitz",
      "Omayma Mahjoub",
      "Wiem Khlifi",
      "Simon Verster Du Toit",
      "Louay Ben Nessir",
      "Refiloe Shabe",
      "Arnol Manuel Fokam",
      "Siddarth Singh",
      "Ulrich Armel Mbou Sob",
      "Arnu Pretorius"
    ],
    "affiliations": [],
    "summary": "Reinforcement learning (RL) systems have countless applications, from energy-grid management to protein design. However, such real-world scenarios are often extremely difficult, combinatorial in nature, and require complex coordination between multiple agents. This level of complexity can cause even state-of-the-art RL systems, trained until convergence, to hit a performance ceiling which they are unable to break out of with zero-shot inference. Meanwhile, many digital or simulation-based applications allow for an inference phase that utilises a specific time and compute budget to explore multiple attempts before outputting a final solution. In this work, we show that such an inference phase employed at execution time, and the choice of a corresponding inference strategy, are key to breaking the performance ceiling observed in complex multi-agent RL problems. Our main result is striking: we can obtain up to a 126% and, on average, a 45% improvement over the previous state-of-the-art across 17 tasks, using only a couple seconds of extra wall-clock time during execution. We also demonstrate promising compute scaling properties, supported by over 60k experiments, making it the largest study on inference strategies for complex RL to date. We make all of our experimental data and code available.",
    "link": "/venue/RxkCwOKVKa@OpenReview",
    "published_date": null,
    "conference": "NeurIPS.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "High-Dimensional Calibration from Swap Regret",
    "authors": [
      "Maxwell Fishelson",
      "Noah Golowich",
      "Mehryar Mohri",
      "Jon Schneider"
    ],
    "affiliations": [],
    "summary": "We study the online calibration of multi-dimensional forecasts over an arbitrary convex set $\\mathcal{P} \\subset \\mathbb{R}^d$ relative to an arbitrary norm $\\Vert\\cdot\\Vert$. We connect this with the problem of external regret minimization for online linear optimization, showing that if it is possible to guarantee $O(\\sqrt{\\rho T})$ worst-case regret after $T$ rounds when actions are drawn from $\\mathcal{P}$ and losses are drawn from the dual $\\Vert \\cdot \\Vert_*$ unit norm ball, then it is also possible to obtain $\\epsilon$-calibrated forecasts after $T = \\exp(O(\\rho /\\epsilon^2))$ rounds. When $\\mathcal{P}$ is the $d$-dimensional simplex and $\\Vert \\cdot \\Vert$ is the $\\ell_1$-norm, the existence of $O(\\sqrt{T\\log d})$ algorithms for learning with experts implies that it is possible to obtain $\\epsilon$-calibrated forecasts after $T = \\exp(O(\\log{d}/\\epsilon^2)) = d^{O(1/\\epsilon^2)}$ rounds, recovering a recent result of Peng 2025. Interestingly, our algorithm obtains this guarantee without requiring access to any online linear optimization subroutine or knowledge of the optimal rate $\\rho$ -- in fact, our algorithm is identical for every setting of $\\mathcal{P}$ and $\\Vert \\cdot \\Vert$. Instead, we show that the optimal regularizer for the above OLO problem can be used to upper bound the above calibration error by a swap regret, which we then minimize by running the recent TreeSwap algorithm with Follow-The-Leader as a subroutine. The resulting algorithm is highly efficient and plays a distribution over simple averages of past observations in each round. Finally, we prove that any online calibration algorithm that guarantees $\\epsilon T$ $\\ell_1$-calibration error over the $d$-dimensional simplex requires $T \\geq \\exp(\\mathrm{poly}(1/\\epsilon))$ (assuming $d \\geq \\mathrm{poly}(1/\\epsilon)$). This strengthens the corresponding $d^{\\Omega(\\log{1/\\epsilon})}$ lower bound of Peng 2025, and shows that an exponential dependence on $1/\\epsilon$ is necessary.",
    "link": "/venue/UVDihUz0iT@OpenReview",
    "published_date": null,
    "conference": "NeurIPS.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "In Search of Adam’s Secret Sauce",
    "authors": [
      "Antonio Orvieto",
      "Robert M. Gower"
    ],
    "affiliations": [],
    "summary": "Understanding the remarkable efficacy of Adam when training transformer-based language models has become a central research topic within the optimization community. To gain deeper insights, several simplifications of Adam have been proposed, such as the signed gradient and signed momentum methods. In this work, we conduct an extensive empirical study — training over 1,500 language models across different data configurations and scales — comparing Adam to several known simplified variants. We find that signed momentum methods are faster than SGD, but consistently underperform relative to Adam, even after careful tuning of momentum, clipping setting and learning rates. However, our analysis reveals a compelling option that preserves near-optimal performance while allowing for new insightful reformulations: constraining the Adam momentum parameters to be equal, $\\beta_1=\\beta_2$. Beyond robust performance, this choice affords new theoretical insights, highlights the \"secret sauce\" on top of signed momentum, and grants a precise statistical interpretation: we show that Adam in this setting implements a natural online algorithm for estimating the mean and variance of gradients—one that arises from a mean-field Gaussian variational inference perspective.",
    "link": "/venue/CH72XyZs4y@OpenReview",
    "published_date": null,
    "conference": "NeurIPS.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "An Optimized Franz-Parisi Criterion and its Equivalence with SQ Lower Bounds",
    "authors": [
      "Siyu Chen",
      "Theodor Misiakiewicz",
      "Ilias Zadik",
      "Peiyuan Zhang"
    ],
    "affiliations": [],
    "summary": "Bandeira et al. (2022) introduced the Franz-Parisi (FP) criterion for characterizing the computational hard phases in statistical detection problems. The FP criterion, based on an annealed version of the celebrated Franz-Parisi potential from statistical physics, was shown to be equivalent to low-degree polynomial (LDP) lower bounds for Gaussian additive models, thereby connecting two distinct approaches to understanding the computational hardness in statistical inference. In this paper, we propose a refined FP criterion that aims to better capture the geometric ``overlap\" structure of statistical models. Our main result establishes that this optimized FP criterion is equivalent to Statistical Query (SQ) lower bounds---another foundational framework in computational complexity of statistical inference. Crucially, this equivalence holds under a mild, verifiable assumption satisfied by a broad class of statistical models, including Gaussian additive models, planted sparse models, as well as non-Gaussian component analysis (NGCA), single-index (SI) models, and convex truncation detection settings. For instance, in the case of convex truncation tasks, the assumption is equivalent with the Gaussian correlation inequality (Royen, 2014) from convex geometry. In addition to the above, our equivalence not only unifies and simplifies the derivation of several known SQ lower bounds—such as for the NGCA model (Diakonikolas et al., 2017) and the SI model (Damian et al., 2024)—but also yields new SQ lower bounds of independent interest, including for the computational gaps in mixed sparse linear regression (Arpino et al., 2023) and convex truncation (De et al., 2023).",
    "link": "/venue/U8BwT6Rmw4@OpenReview",
    "published_date": null,
    "conference": "NeurIPS.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "MaxSup: Overcoming Representation Collapse in Label Smoothing",
    "authors": [
      "Yuxuan Zhou",
      "Heng Li",
      "Zhi-Qi Cheng",
      "Xudong Yan",
      "Yifei Dong",
      "Mario Fritz",
      "Margret Keuper"
    ],
    "affiliations": [],
    "summary": "Label Smoothing (LS) is widely adopted to reduce overconfidence in neural network predictions and improve generalization. Despite these benefits, recent studies reveal two critical issues with LS. First, LS induces overconfidence in misclassified samples. Second, it compacts feature representations into overly tight clusters, diluting intra-class diversity, although the precise cause of this phenomenon remained elusive. In this paper, we analytically decompose the LS-induced loss, exposing two key terms: (i) a regularization term that dampens overconfidence only when the prediction is correct, and (ii) an error-amplification term that arises under misclassifications. This latter term compels the network to reinforce incorrect predictions with undue certainty, exacerbating representation collapse. To address these shortcomings, we propose Max Suppression (MaxSup), which applies uniform regularization to both correct and incorrect predictions by penalizing the top-1 logit rather than the ground-truth logit. Through extensive feature-space analyses, we show that MaxSup restores intra-class variation and sharpens inter-class boundaries. Experiments on large-scale image classification and multiple downstream tasks confirm that MaxSup is a more robust alternative to LS.Code and reproducibility scripts are available at https://github.com/ZhouYuxuanYX/Maximum-Suppression-Regularization.",
    "link": "/venue/efOq8wHH9o@OpenReview",
    "published_date": null,
    "conference": "NeurIPS.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "Memory Mosaics at scale",
    "authors": [
      "Jianyu Zhang",
      "Leon Bottou"
    ],
    "affiliations": [],
    "summary": "Memory Mosaics, networks of associative memories, have demonstrated appealing compositional and in-context learning capabilities on medium-scale networks (GPT-2 scale) and synthetic small datasets. This work shows that these favorable properties remain when we scale memory mosaics to large language model sizes (llama-8B scale) and real-world datasets. To this end, we scale memory mosaics to 10B size, we train them on one trillion tokens, we introduce a couple architectural modifications (*memory mosaics v2*), we assess their capabilities across three evaluation dimensions: training-knowledge storage, new-knowledge storage, and in-context learning. Throughout the evaluation, memory mosaics v2 match transformers on the learning of training knowledge (first dimension) and significantly outperforms transformers on carrying out new tasks at inference time (second and third dimensions). These improvements cannot be easily replicated by simply increasing the training data for transformers. A memory mosaics v2 trained on one trillion tokens still perform better on these tasks than a transformer trained on eight trillion tokens.",
    "link": "/venue/IfD2MKTmWv@OpenReview",
    "published_date": null,
    "conference": "NeurIPS.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "The emergence of sparse attention: impact of data distribution and benefits of repetition",
    "authors": [
      "Nicolas Zucchet",
      "Francesco D'Angelo",
      "Andrew Kyle Lampinen",
      "Stephanie C.Y. Chan"
    ],
    "affiliations": [],
    "summary": "Emergence is a fascinating property of large language models and neural networks more broadly: as models scale and train for longer, they sometimes develop new abilities in sudden ways. Despite initial studies, we still lack a comprehensive understanding of how and when these abilities emerge. To address this gap, we study the emergence over training of sparse attention, a critical and frequently observed attention pattern in Transformers. By combining theoretical analysis of a toy model with empirical observations on small Transformers trained on a linear regression variant, we uncover the mechanics driving sparse attention emergence and reveal that emergence timing follows power laws based on task structure, architecture, and optimizer choice. We additionally find that repetition can greatly speed up emergence. Finally, we confirm these results on a well-studied in-context associative recall task. Our findings provide a simple, theoretically grounded framework for understanding how data distributions and model design influence the learning dynamics behind one form of emergence.",
    "link": "/venue/jMhRbV47pS@OpenReview",
    "published_date": null,
    "conference": "NeurIPS.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "ControlFusion: A Controllable Image Fusion Network with Language-Vision Degradation Prompts",
    "authors": [
      "Linfeng Tang",
      "Yeda Wang",
      "Zhanchuan Cai",
      "Junjun Jiang",
      "Jiayi Ma"
    ],
    "affiliations": [],
    "summary": "Current image fusion methods struggle with real-world composite degradations and lack the flexibility to accommodate user-specific needs. To address this, we propose ControlFusion, a controllable fusion network guided by language-vision prompts that adaptively mitigates composite degradations. On the one hand, we construct a degraded imaging model based on physical mechanisms, such as the Retinex theory and atmospheric scattering principle, to simulate composite degradations and provide a data foundation for addressing realistic degradations. On the other hand, we devise a prompt-modulated restoration and fusion network that dynamically enhances features according to degradation prompts, enabling adaptability to varying degradation levels. To support user-specific preferences in visual quality, a text encoder is incorporated to embed user-defined degradation types and levels as degradation prompts. Moreover, a spatial-frequency collaborative visual adapter is designed to autonomously perceive degradations from source images, thereby reducing complete reliance on user instructions. Extensive experiments demonstrate that ControlFusion outperforms SOTA fusion methods in fusion quality and degradation handling, particularly under real-world and compound degradations.",
    "link": "/venue/aLhA7AYLLR@OpenReview",
    "published_date": null,
    "conference": "NeurIPS.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "Identifiability of Deep Polynomial Neural Networks",
    "authors": [
      "Konstantin Usevich",
      "Ricardo Augusto Borsoi",
      "Clara Dérand",
      "Marianne Clausel"
    ],
    "affiliations": [],
    "summary": "Polynomial Neural Networks (PNNs) possess a rich algebraic and geometric structure. However, their identifiability-a key property for ensuring interpretability-remains poorly understood. In this work, we present a comprehensive analysis of the identifiability of deep PNNs, including architectures with and without bias terms. Our results reveal an intricate interplay between activation degrees and layer widths in achieving identifiability. As special cases, we show that architectures with non-increasing layer widths are generically identifiable under mild conditions, while encoder-decoder networks are identifiable when the decoder widths do not grow too rapidly compared to the activation degrees. Our proofs are constructive and center on a connection between deep PNNs and low-rank tensor decompositions, and Kruskal-type uniqueness theorems. We also settle an open conjecture on the dimension of PNN's neurovarieties, and provide new bounds on the activation degrees required for it to reach the expected dimension.",
    "link": "/venue/MrUsZfQ9pC@OpenReview",
    "published_date": null,
    "conference": "NeurIPS.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "Understanding and Mitigating Numerical Sources of Nondeterminism in LLM Inference",
    "authors": [
      "Jiayi Yuan",
      "Hao Li",
      "Xinheng Ding",
      "Wenya Xie",
      "Yu-Jhe Li",
      "Wentian Zhao",
      "Kun Wan",
      "Jing Shi",
      "Xia Hu",
      "Zirui Liu"
    ],
    "affiliations": [],
    "summary": "Large Language Models (LLMs) are now integral across various domains and have demonstrated impressive performance. Progress, however, rests on the premise that benchmark scores are both accurate and reproducible. We demonstrate that the reproducibility of LLM performance is fragile: changing system configuration, such as evaluation batch size, GPU count, and GPU version, can introduce significant differences in the generated responses. This issue is especially pronounced in reasoning models, where minor rounding differences in early tokens can cascade into divergent chains of thought, ultimately affecting accuracy. For instance, under bfloat16 precision with greedy decoding, a reasoning model like DeepSeek-R1-Distill-Qwen-7B can exhibit up to 9\\% variation in accuracy and 9,000 tokens difference in response length due to differences in GPU count, type, and evaluation batch size. We trace the root cause of this variability to the non-associative nature of floating-point arithmetic under limited numerical precision. This work presents the first systematic investigation into how numerical precision affects reproducibility in LLM inference. Through carefully controlled experiments across various hardware, software, and precision settings, we quantify when and how model outputs diverge. Our analysis reveals that floating-point precision—while critical for reproducibility—is often neglected in evaluation practices. Inspired by this, we develop a lightweight inference pipeline, dubbed LayerCast, that stores weights in 16-bit precision but performs all computations in FP32, balancing memory efficiency with numerical stability. Code is available at https://github.com/nanomaoli/llm_reproducibility.",
    "link": "/venue/Q3qAsZAEZw@OpenReview",
    "published_date": null,
    "conference": "NeurIPS.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "PRIMT: Preference-based Reinforcement Learning with Multimodal Feedback and Trajectory Synthesis from Foundation Models",
    "authors": [
      "Ruiqi Wang",
      "Dezhong Zhao",
      "Ziqin Yuan",
      "Tianyu Shao",
      "Guohua Chen",
      "Dominic Kao",
      "Sungeun Hong",
      "Byung-Cheol Min"
    ],
    "affiliations": [],
    "summary": "Preference-based reinforcement learning (PbRL) has emerged as a promising paradigm for teaching robots complex behaviors without reward engineering. However, its effectiveness is often limited by two critical challenges: the reliance on extensive human input and the inherent difficulties in resolving query ambiguity and credit assignment during reward learning. In this paper, we introduce PRIMT, a PbRL framework designed to overcome these challenges by leveraging foundation models (FMs) for multimodal synthetic feedback and trajectory synthesis. Unlike prior approaches that rely on single-modality FM evaluations, PRIMT employs a hierarchical neuro-symbolic fusion strategy, integrating the complementary strengths of vision-language models (VLMs) and large language models (LLMs) in evaluating robot behaviors for more reliable and comprehensive feedback. PRIMT also incorporates foresight trajectory generation to warm-start the trajectory buffer with bootstrapped samples, reducing early-stage query ambiguity, and hindsight trajectory augmentation for counterfactual reasoning with a causal auxiliary loss to improve credit assignment. We evaluate PRIMT on 2 locomotion and 6 manipulation tasks on various benchmarks, demonstrating superior performance over FM-based and scripted baselines. Website at https://primt25.github.io/.",
    "link": "/venue/4xvE6Iy77Y@OpenReview",
    "published_date": null,
    "conference": "NeurIPS.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "A is for Absorption: Studying Feature Splitting and Absorption in Sparse Autoencoders",
    "authors": [
      "David Chanin",
      "James Wilken-Smith",
      "Tomáš Dulka",
      "Hardik Bhatnagar",
      "Satvik Golechha",
      "Joseph Isaac Bloom"
    ],
    "affiliations": [],
    "summary": "Sparse Autoencoders (SAEs) aim to decompose the activation space of large language models (LLMs) into human-interpretable latent directions or features. As we increase the number of features in the SAE, hierarchical features tend to split into finer features (“math” may split into “algebra”, “geometry”, etc.), a phenomenon referred to as feature splitting. However, we show that sparse decomposition and splitting of hierarchical features is not robust. Specifically, we show that seemingly monosemantic features fail to fire where they should, and instead get “absorbed” into their children features. We coin this phenomenon feature absorption, and show that it is caused by optimizing for sparsity in SAEs whenever the underlying features form a hierarchy. We introduce a metric to detect absorption in SAEs, and validate our findings empirically on hundreds of LLM SAEs. Our investigation suggests that varying SAE sizes or sparsity is insufficient to solve this issue. We discuss the implications of feature absorption in SAEs and some potential approaches to solve the fundamental theoretical issues before SAEs can be used for interpreting LLMs robustly and at scale.",
    "link": "/venue/R73ybUciQF@OpenReview",
    "published_date": null,
    "conference": "NeurIPS.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "EvoLM: In Search of Lost Language Model Training Dynamics",
    "authors": [
      "Zhenting Qi",
      "Fan Nie",
      "Alexandre Alahi",
      "James Zou",
      "Himabindu Lakkaraju",
      "Yilun Du",
      "Eric P. Xing",
      "Sham M. Kakade",
      "Hanlin Zhang"
    ],
    "affiliations": [],
    "summary": "Modern language model (LM) training has been divided into multiple stages, making it difficult for downstream developers to evaluate the impact of design choices made at each stage. We present EvoLM, a model suite that enables systematic and transparent analysis of LMs' training dynamics across pre-training, continued pre-training, supervised fine-tuning, and reinforcement learning. By training over 100 LMs with 1B and 4B parameters from scratch, we rigorously evaluate both upstream (language modeling) and downstream (problem-solving) reasoning capabilities, including considerations of both in-domain and out-of-domain generalization. Key insights highlight the diminishing returns from excessive pre-training and post-training, the importance and practices of mitigating forgetting during domain-specific continued pre-training, the crucial role of continued pre-training in bridging pre-training and post-training phases, and various intricate trade-offs when configuring supervised fine-tuning and reinforcement learning. To facilitate open research and reproducibility, we release all pre-trained and post-trained models, training datasets for all stages, and our entire training and evaluation pipeline.",
    "link": "/venue/B6bE2GC71a@OpenReview",
    "published_date": null,
    "conference": "NeurIPS.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "Analog In-memory Training on General Non-ideal Resistive Elements: The Impact of Response Functions",
    "authors": [
      "Zhaoxian Wu",
      "Quan Xiao",
      "Tayfun Gokmen",
      "Omobayode Fagbohungbe",
      "Tianyi Chen"
    ],
    "affiliations": [],
    "summary": "As the economic and environmental costs of training and deploying large vision or language models increase dramatically, analog in-memory computing (AIMC) emerges as a promising energy-efficient solution. However, the training perspective, especially its training dynamic, is underexplored. In AIMC hardware, the trainable weights are represented by the conductance of resistive elements and updated using consecutive electrical pulses. While the conductance changes by a constant in response to each pulse, in reality, the change is scaled by asymmetric and non-linear response functions, leading to a non-ideal training dynamic. This paper provides a theoretical foundation for gradient-based training on AIMC hardware with non-ideal response functions. We demonstrate that asymmetric response functions negatively impact Analog SGD by imposing an implicit penalty on the objective. To overcome the issue, we propose residual learning algorithm, which provably converges exactly to a critical point by solving a bilevel optimization problem. We show that the proposed method can be extended to deal with other hardware imperfections like limited response granularity. As far as we know, it is the first paper to investigate the impact of a class of generic non-ideal response functions. The conclusion is supported by simulations validating our theoretical insights.",
    "link": "/venue/WhEPg4mUs6@OpenReview",
    "published_date": null,
    "conference": "NeurIPS.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "Benchmarking Empirical Privacy Protection for Adaptations of Large Language Models",
    "authors": [
      "Bartłomiej Marek",
      "Lorenzo Rossi",
      "Vincent Hanke",
      "Xun Wang",
      "Michael Backes",
      "Franziska Boenisch",
      "Adam Dziedzic"
    ],
    "affiliations": [],
    "summary": "Recent work has applied differential privacy (DP) to adapt large language models (LLMs) for sensitive applications, offering theoretical guarantees. However, its practical effectiveness remains unclear, partly due to LLM pretraining, where overlaps and interdependencies with adaptation data can undermine privacy despite DP efforts. To analyze this issue in practice, we investigate privacy risks under DP adaptations in LLMs using state-of-the-art attacks such as robust membership inference and canary data extraction. We benchmark these risks by systematically varying the adaptation data distribution, from exact overlaps with pretraining data, through in-distribution (IID) cases, to entirely out-of-distribution (OOD) examples. Additionally, we evaluate how different adaptation methods and different privacy regimes impact the vulnerability. Our results show that distribution shifts strongly influence privacy vulnerability: the closer the adaptation data is to the pretraining distribution, the higher the practical privacy risk at the same theoretical guarantee, even without direct data overlap. We find that parameter-efficient fine-tuning methods, such as LoRA, achieve the highest empirical privacy protection for OOD data. Our benchmark identifies key factors for achieving practical privacy in DP LLM adaptation, providing actionable insights for deploying customized models in sensitive settings. Looking forward, we propose a structured framework for holistic privacy assessment beyond adaptation privacy, to identify and evaluate risks across the full pretrain-adapt pipeline of LLMs.",
    "link": "/venue/jY7fAo9rfK@OpenReview",
    "published_date": null,
    "conference": "ICLR.2026",
    "conference_year": 2026,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "MedAgentGym: A Scalable Agentic Training Environment for Code-Centric Reasoning in Biomedical Data Science",
    "authors": [
      "Ran Xu",
      "Yuchen Zhuang",
      "Yishan Zhong",
      "Yue Yu",
      "Zifeng Wang",
      "Xiangru Tang",
      "Hang Wu",
      "May Dongmei Wang",
      "Peifeng Ruan",
      "Donghan Yang",
      "Tao Wang",
      "Guanghua Xiao",
      "Xin Liu",
      "Carl Yang",
      "Yang Xie",
      "Wenqi Shi"
    ],
    "affiliations": [],
    "summary": "We introduce MedAgentGym, a scalable and interactive training environment designed to enhance coding-based biomedical reasoning capabilities in large language model (LLM) agents. MedAgentGym comprises 72,413 task instances across 129 categories derived from 12 authentic real-world biomedical scenarios. Tasks are encapsulated within executable sandbox environments, each featuring detailed task specifications, interactive feedback mechanisms, verifiable ground truth annotations, and scalable training trajectory generation. Extensive benchmarking of 29 LLMs reveals substantial performance disparities in biomedical data science between commercial and open-source LLMs. Leveraging efficient multi-threaded and multi-turn trajectory sampling in MedAgentGym, Med-Copilot achieves performance gains of +43.02% and +45.28% from offline and online reinforcement learning, respectively, demonstrating MedAgentGym as an effective training ground while establishing itself as a cost-effective, privacy-preserving alternative competitive with proprietary LLMs (gpt-4o). By offering a unified execution environment with a comprehensive benchmark and accessible, extensible training resources, MedAgentGym delivers an integrated platform to develop LLM-based coding assistants for advanced biomedical data science.",
    "link": "/venue/jHDZEUgS4r@OpenReview",
    "published_date": null,
    "conference": "ICLR.2026",
    "conference_year": 2026,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "RAIN-Merging: A Gradient-Free Method to Enhance Instruction Following in Large Reasoning Models with Preserved Thinking Format",
    "authors": [
      "Zhehao Huang",
      "Yuhang Liu",
      "Baijiong Lin",
      "Yixin Lou",
      "Zhengbao He",
      "Hanling Tian",
      "Tao Li",
      "Xiaolin Huang"
    ],
    "affiliations": [],
    "summary": "Large reasoning models (LRMs) excel at a long chain of reasoning but often fail to faithfully follow instructions regarding output format, constraints, or specific requirements. We investigate whether this gap can be closed by integrating an instruction-tuned model (ITM) into an LRM. Analyzing their differences in parameter space, namely task vectors, we find that their principal subspaces are nearly orthogonal across key modules, suggesting a lightweight merging with minimal interference. However, we also demonstrate that naïve merges are fragile because they overlook the output format mismatch between LRMs (with explicit *thinking* and *response* segments) and ITMs (answers-only). We introduce **RAIN-Merging** (Reasoning-Aware Instruction-attention guided Null-space projection Merging), a gradient-free method that integrates instruction following while preserving thinking format and reasoning performance. First, with a small reasoning calibration set, we project the ITM task vector onto the null space of forward features at thinking special tokens, which preserves the LRM's structured reasoning mechanisms. Second, using a small instruction calibration set, we estimate instruction attention to derive module-specific scaling that amplifies instruction-relevant components and suppresses leakage. Across four instruction-following benchmarks and nine reasoning & general capability benchmarks, RAIN-Merging substantially improves instruction adherence while maintaining reasoning quality. The gains are consistent across model scales and architectures, translating to improved performance in agent settings.",
    "link": "/venue/PO2iULmu5e@OpenReview",
    "published_date": null,
    "conference": "ICLR.2026",
    "conference_year": 2026,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "On the Wasserstein Geodesic Principal Component Analysis of probability measures",
    "authors": [
      "Nina Vesseron",
      "Elsa Cazelles",
      "Alice Le Brigant",
      "Klein Thierry"
    ],
    "affiliations": [],
    "summary": "This paper focuses on Geodesic Principal Component Analysis (GPCA) on a collection of probability distributions using the Otto-Wasserstein geometry. The goal is to identify geodesic curves in the space of probability measures that best capture the modes of variation of the underlying dataset. We first address the case of a collection of Gaussian distributions, and show how to lift the computations in the space of invertible linear maps. For the more general setting of absolutely continuous probability measures, we leverage a novel approach to parameterizing geodesics in Wasserstein space with neural networks. Finally, we compare to classical tangent PCA through various examples and provide illustrations on real-world datasets.",
    "link": "/venue/OJupg4mDjS@OpenReview",
    "published_date": null,
    "conference": "ICLR.2026",
    "conference_year": 2026,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "Radiometrically Consistent Gaussian Surfels for Inverse Rendering",
    "authors": [
      "Kyu Beom Han",
      "Jaeyoon Kim",
      "Woo Jae Kim",
      "Jinhwan Seo",
      "Sung-eui Yoon"
    ],
    "affiliations": [],
    "summary": "Inverse rendering with Gaussian Splatting has advanced rapidly, but accurately disentangling material properties from complex global illumination effects, particularly indirect illumination, remains a major challenge. Existing methods often query indirect radiance from Gaussian primitives pre-trained for novel-view synthesis. However, these pre-trained Gaussian primitives are supervised only towards limited training viewpoints, thus lack supervision for modeling indirect radiances from unobserved views. To address this issue, we introduce radiometric consistency, a novel physically-based constraint that provides supervision towards unobserved views by minimizing the residual between each Gaussian primitive’s learned radiance and its physically-based rendered counterpart. Minimizing the residual for unobserved views establishes a self-correcting feedback loop that provides supervision from both physically-based rendering and novel-view synthesis, enabling accurate modeling of inter-reflection. We then propose Radiometrically Consistent Gaussian Surfels (RadioGS), an inverse rendering framework built upon our principle by efficiently integrating radiometric consistency by utilizing Gaussian surfels and 2D Gaussian ray tracing. We further propose a finetuning-based relighting strategy that adapts Gaussian surfel radiances to new illuminations within minutes, achieving low rendering cost ($<$10ms). Extensive experiments on existing inverse rendering benchmarks show that RadioGS outperforms existing Gaussian-based methods in inverse rendering, while retaining the computational efficiency.",
    "link": "/venue/lKqE7UuMvp@OpenReview",
    "published_date": null,
    "conference": "ICLR.2026",
    "conference_year": 2026,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "Optimistic Task Inference for Behavior Foundation Models",
    "authors": [
      "Thomas Rupf",
      "Marco Bagatella",
      "Marin Vlastelica",
      "Andreas Krause"
    ],
    "affiliations": [],
    "summary": "Behavior Foundation Models (BFMs) are capable of retrieving high-performing policy for any reward function specified directly at test-time, commonly referred to as zero-shot reinforcement learning (RL). While this is a very efficient process in terms of compute, it can be less so in terms of data: as a standard assumption, BFMs require computing rewards over a non-negligible inference dataset, assuming either access to a functional form of rewards, or significant labeling efforts. To alleviate these limitations, we tackle the problem of task inference purely through interaction with the environment at test-time. We propose OpTI-BFM, an optimistic decision criterion that directly models uncertainty over reward functions and guides BFMs in data collection for task inference. Formally, we provide a regret bound for well- trained BFMs through a direct connection to upper-confidence algorithms for linear bandits. Empirically, we evaluate OpTI-BFM on established zero-shot benchmarks, and observe that it enables successor-features-based BFMs to identify and optimize an unseen reward function in a handful of episodes with minimal compute overhead.",
    "link": "/venue/m5byThUSNE@OpenReview",
    "published_date": null,
    "conference": "ICLR.2026",
    "conference_year": 2026,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "Premise Selection for a Lean Hammer",
    "authors": [
      "Thomas Zhu",
      "Joshua Clune",
      "Jeremy Avigad",
      "Qiaochu Jiang",
      "Sean Welleck"
    ],
    "affiliations": [],
    "summary": "Neural methods are transforming automated reasoning for proof assistants, yet integrating these advances into practical verification workflows remains challenging. A $\\textit{hammer}$ is a tool that integrates premise selection, translation to external automatic theorem provers, and proof reconstruction into one overarching tool to automate tedious reasoning steps. We present LeanPremise, a novel neural premise selection system, and we combine it with existing translation and proof reconstruction components to create LeanHammer, the first end-to-end domain general hammer for the Lean proof assistant. Unlike existing Lean premise selectors, LeanPremise is specifically trained for use with a hammer in dependent type theory. It also dynamically adapts to user-specific contexts, enabling it to effectively recommend premises from libraries outside LeanPremise's training data as well as lemmas defined by the user locally. With comprehensive evaluations, we show that LeanPremise enables LeanHammer to solve 21\\% more goals than existing premise selectors and generalizes well to diverse domains. Our work helps bridge the gap between neural retrieval and symbolic reasoning, making formal verification more accessible to researchers and practitioners.",
    "link": "/venue/m04JJNeRK6@OpenReview",
    "published_date": null,
    "conference": "ICLR.2026",
    "conference_year": 2026,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "Planner Aware Path Learning in Diffusion Language Models Training",
    "authors": [
      "Zhangzhi Peng",
      "Zachary Bezemek",
      "Jarrid Rector-Brooks",
      "Shuibai Zhang",
      "Michael Bronstein",
      "Anru Zhang",
      "Joey Bose",
      "Alexander Tong"
    ],
    "affiliations": [],
    "summary": "Diffusion language models have emerged as a powerful alternative to autoregressive models, enabling fast inference through more flexible and parallel generation paths. This flexibility of sampling is unlocked by new engineered sampling strategies, or *planners*, that select more favorable generation paths by iteratively planning---versus uniformly at random---where to denoise along the sequence. However, by modifying the reverse paths via planning, planners create an irrevocable mismatch between the uniformly random denoising paths during training and planning-based inference. In this paper, we systematically investigate the mismatch of discrete diffusion training and inference under planning and theoretically prove that the standard discrete diffusion training evidence lower bound (ELBO) does not accurately describe a denoiser that uses a non-uniform planner. To address this gap, we derive a new planned evidence lower bound (P-ELBO) that incorporates planner-based reverse dynamics directly into the training objective. Using the P-ELBO, we introduce *Planner Aware Path Learning* (PAPL), a novel training scheme that aligns training and inference under a planned denoiser. PAPL is implemented as a simple yet effective modification to the standard masked discrete diffusion loss, making it widely applicable and easy to adopt. Empirically, we show PAPL delivers consistent gains across domains, including a 40\\% relative improvement in protein sequences, improved text generation with up to a $4\\times$ relative MAUVE gain, and 23\\% relative improvement in code generation HumanEval pass@10.",
    "link": "/venue/lAlI5FuIf7@OpenReview",
    "published_date": null,
    "conference": "ICLR.2026",
    "conference_year": 2026,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "AutoEP: LLMs-Driven Automation of Hyperparameter Evolution for Metaheuristic Algorithms",
    "authors": [
      "Zhenxing Xu",
      "Yizhe Zhang",
      "Weidong Bao",
      "Hao Wang",
      "Ming Chen",
      "Haoran Ye",
      "Wenzheng Jiang",
      "Hui Yan",
      "Ji Wang"
    ],
    "affiliations": [],
    "summary": "Dynamically configuring algorithm hyperparameters is a fundamental challenge in computational intelligence. While learning-based methods offer automation, they suffer from prohibitive sample complexity and poor generalization. We introduce AutoEP, a novel framework that bypasses training entirely by leveraging Large Language Models (LLMs) as zero-shot reasoning engines for algorithm control. AutoEP's core innovation lies in a tight synergy between two components: (1) an online Exploratory Landscape Analysis (ELA) module that provides real-time, quantitative feedback on the search dynamics, and (2) a multi-LLM reasoning chain that interprets this feedback to generate adaptive hyperparameter strategies. This approach grounds high-level reasoning in empirical data, mitigating hallucination. Evaluated on three distinct metaheuristics across diverse combinatorial optimization benchmarks, AutoEP consistently outperforms state-of-the-art tuners, including neural evolution and other LLM-based methods. Notably, our framework enables open-source models like Qwen3-30B to match the performance of GPT-4, demonstrating a powerful and accessible new paradigm for automated hyperparameter design.Our code is available at https://anonymous.4open.science/r/AutoEP-3E11.",
    "link": "/venue/hit3hGBheP@OpenReview",
    "published_date": null,
    "conference": "ICLR.2026",
    "conference_year": 2026,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "P-GenRM: Personalized Generative Reward Model with Test-time User-based Scaling",
    "authors": [
      "Pinyi Zhang",
      "Ting-En Lin",
      "Yuchuan Wu",
      "Jingyang Chen",
      "Zongqi Wang",
      "Hua Yang",
      "Bing Zhao",
      "Fei Huang",
      "Yongbin Li",
      "Kai Zhang"
    ],
    "affiliations": [],
    "summary": "Personalized alignment of large language models seeks to adapt responses to individual user preferences, typically via reinforcement learning. A key challenge is obtaining accurate, user-specific reward signals in open-ended scenarios. Existing personalized reward models face two persistent limitations: (1) oversimplifying diverse, scenario-specific preferences into a small, fixed set of evaluation principles, and (2) struggling with generalization to new users with limited feedback. To this end, we propose **P-GenRM**, the first **P**ersonalized **Gen**erative **R**eward **M**odel with test-time user-based scaling. P-GenRM transforms preference signals into structured evaluation chains that derive adaptive personas and scoring rubrics across various scenarios. It further clusters users into User Prototypes and introduces a dual-granularity scaling mechanism: at the individual level, it adaptively scales and aggregates each user’s scoring scheme; at the prototype level, it incorporates preferences from similar users. This design mitigates noise in inferred preferences and enhances generalization to unseen users through prototype-based transfer. Empirical results show that P-GenRM achieves state-of-the-art results on widely-used personalized reward model benchmarks, with an average improvement of ~2.31\\%, and demonstrates strong generalization on an out-of-distribution dataset. Notably, Test-time User-based scaling provides an additional ~3\\% boost, demonstrating stronger personalized alignment with test-time scalability.",
    "link": "/venue/hXNApWLBZG@OpenReview",
    "published_date": null,
    "conference": "ICLR.2026",
    "conference_year": 2026,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "Universal Inverse Distillation for Matching Models with Real-Data Supervision (No GANs)",
    "authors": [
      "Nikita Kornilov",
      "David Li",
      "Tikhon Mavrin",
      "Aleksei Leonov",
      "Nikita Gushchin",
      "Evgeny Burnaev",
      "Iaroslav Koshelev",
      "Aleksandr Korotin"
    ],
    "affiliations": [],
    "summary": "While achieving exceptional generative quality, modern diffusion, flow, and other matching models suffer from slow inference, as they require many steps of iterative generation. Recent distillation methods address this by training efficient one-step generators under the guidance of a pre-trained teacher model. However, these methods are often constrained to only one specific framework, e.g., only to diffusion or only to flow models. Furthermore, these methods are naturally data-free, and to benefit from the usage of real data, it is required to use an additional complex adversarial training with an extra discriminator model. In this paper, we present \\textbf{RealUID}, a unified distillation framework for all matching models that seamlessly incorporates real data into the distillation procedure without GANs. Our \\textbf{RealUID} approach offers a simple theoretical foundation that covers previous distillation methods for Flow Matching and Diffusion models, and is also extended to their modifications, such as Bridge Matching and Stochastic Interpolants.",
    "link": "/venue/8NuN5UzXLC@OpenReview",
    "published_date": null,
    "conference": "ICLR.2026",
    "conference_year": 2026,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "Fast Escape, Slow Convergence: Learning Dynamics of Phase Retrieval under Power-Law Data",
    "authors": [
      "Guillaume Braun",
      "Bruno Loureiro",
      "Minh Ha Quang",
      "Masaaki Imaizumi"
    ],
    "affiliations": [],
    "summary": "Scaling laws describe how learning performance improves with data, compute, or training time, and have become a central theme in modern deep learning. We study this phenomenon in a canonical nonlinear model: phase retrieval with anisotropic Gaussian inputs whose covariance spectrum follows a power law. Unlike the isotropic case, where dynamics collapse to a two-dimensional system, anisotropy yields a qualitatively new regime in which an infinite hierarchy of coupled equations governs the evolution of the summary statistics. We develop a tractable reduction that reveals a three-phase trajectory: (i) fast escape from low alignment, (ii) slow convergence of the summary statistics, and (iii) spectral-tail learning in low-variance directions. From this decomposition, we derive explicit scaling laws for the mean-squared error, showing how spectral decay dictates convergence times and error curves. Experiments confirm the predicted phases and exponents. These results provide the first rigorous characterization of scaling laws in nonlinear regression with anisotropic data, highlighting how anisotropy reshapes learning dynamics.",
    "link": "/venue/Ae4eZpkXBX@OpenReview",
    "published_date": null,
    "conference": "ICLR.2026",
    "conference_year": 2026,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "Verifying Chain-of-Thought Reasoning via its Computational Graph",
    "authors": [
      "Zheng Zhao",
      "Yeskendir Koishekenov",
      "Xianjun Yang",
      "Naila Murray",
      "Nicola Cancedda"
    ],
    "affiliations": [],
    "summary": "Current Chain-of-Thought (CoT) verification methods predict reasoning correctness based on outputs (black-box) or activations (gray-box), but offer limited insight into \\textit{why} a computation fails. We introduce a white-box method: \\textbf{Circuit-based Reasoning Verification (CRV)}. We hypothesize that attribution graphs of correct CoT steps, viewed as \\textit{execution traces} of the model's latent reasoning circuits, possess distinct structural fingerprints from those of incorrect steps. By training a classifier on structural features of these graphs, we show that these traces contain a powerful signal of reasoning errors. Our white-box approach yields novel scientific insights unattainable by other methods. (1) We demonstrate that structural signatures of error are highly predictive, establishing the viability of verifying reasoning directly via its computational graph. (2) We find these signatures to be highly domain-specific, revealing that failures in different reasoning tasks manifest as distinct computational patterns. (3) We provide evidence that these signatures are not merely correlational; by using our analysis to guide targeted interventions on individual transcoder features, we successfully correct the model's faulty reasoning. Our work shows that, by scrutinizing a model's computational process, we can move from simple error detection to a deeper, causal understanding of LLM reasoning.",
    "link": "/venue/CxiNICq0Rr@OpenReview",
    "published_date": null,
    "conference": "ICLR.2026",
    "conference_year": 2026,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "MemAgent: Reshaping Long-Context LLM with Multi-Conv RL-based Memory Agent",
    "authors": [
      "Hongli Yu",
      "Tinghong Chen",
      "Jiangtao Feng",
      "Jiangjie Chen",
      "Weinan Dai",
      "Qiying Yu",
      "Ya-Qin Zhang",
      "Wei-Ying Ma",
      "Jingjing Liu",
      "Mingxuan Wang",
      "Hao Zhou"
    ],
    "affiliations": [],
    "summary": "Despite improvements by length extrapolation, efficient attention and memory modules, handling infinitely long documents without performance degradation during extrapolation remains the ultimate challenge in long-text processing. To solve this problem, We introduce a novel agent workflow, \\method, which processes text in segments and updates memory through an overwrite strategy, addressing the challenge of long-context task through enhanced memory management. We further extend the DAPO algorithm to directly optimize memory ability in an end-to-end fashion, facilitating training via independent-context multi-conversation generation. Experimental results demonstrate that MemAgent has superb long-context capabilities, being able to extrapolate from an 8K context to a 3.5M QA task with a performance loss of less than 10\\% and achieving over 95\\% on the 512K NIAH test.",
    "link": "/venue/k5nIOvYGCL@OpenReview",
    "published_date": null,
    "conference": "ICLR.2026",
    "conference_year": 2026,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "Exploratory Diffusion Model for Unsupervised Reinforcement Learning",
    "authors": [
      "Chengyang Ying",
      "Huayu Chen",
      "Xinning Zhou",
      "Zhongkai Hao",
      "Hang Su",
      "Jun Zhu"
    ],
    "affiliations": [],
    "summary": "Unsupervised reinforcement learning (URL) pre-trains agents by exploring diverse states in reward-free environments, aiming to enable efficient adaptation to various downstream tasks. Without extrinsic rewards, prior methods rely on intrinsic objectives, but heterogeneous exploration data demand strong modeling capacity for both intrinsic reward design and policy learning. We introduce the **Ex**ploratory **D**iffusion **M**odel (**ExDM**), which leverages the expressive power of diffusion models to fit diverse replay-buffer distributions, thus providing accurate density estimates and a score-based intrinsic reward that drives exploration into under-visited regions. This mechanism substantially broadens state coverage and yields robust pre-trained policies. Beyond exploration, ExDM offers theoretical guarantees and practical algorithms for fine-tuning diffusion policies under limited interactions, overcoming instability and computational overhead from multi-step sampling. Extensive experiments on Maze2d and URLB show that ExDM achieves superior exploration and faster downstream adaptation, establishing new state-of-the-art results, particularly in environments with complex structure or cross-embodiment settings.",
    "link": "/venue/k0Kb1ynFbt@OpenReview",
    "published_date": null,
    "conference": "ICLR.2026",
    "conference_year": 2026,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "EigenBench: A Comparative Behavioral Measure of Value Alignment",
    "authors": [
      "Jonathn Chang",
      "Leonhard Piff",
      "Suvadip Sana",
      "Jasmine Li",
      "Lionel Levine"
    ],
    "affiliations": [],
    "summary": "Aligning AI with human values is a pressing unsolved problem. To address the lack of quantitative metrics for value alignment, we propose EigenBench: a black-box method for comparatively benchmarking language models’ values. Given an ensemble of models, a constitution describing a value system, and a dataset of scenarios, our method returns a vector of scores quantifying each model’s alignment to the given constitution. To produce these scores, each model judges the outputs of other models across many scenarios, and these judgments are aggregated with EigenTrust (Kamvar et al., 2003), yielding scores that reflect a weighted consensus judgment of the whole ensemble. EigenBench uses no ground truth labels, as it is designed to quantify subjective traits for which reasonable judges may disagree on the correct label. Hence, to validate our method, we collect human judgments on the same ensemble of models and show that EigenBench’s judgments align closely with those of human evaluators. We further demonstrate that EigenBench can recover model rankings on the GPQA benchmark without access to objective labels, supporting its viability as a framework for evaluating subjective values for which no ground truths exist.",
    "link": "/venue/fm79KXJIUQ@OpenReview",
    "published_date": null,
    "conference": "ICLR.2026",
    "conference_year": 2026,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "TileLang: Bridge Programmability and Performance in Modern Neural Kernels",
    "authors": [
      "Lei Wang",
      "Yu Cheng",
      "Yining Shi",
      "Zhiwen Mo",
      "Zhengju Tang",
      "Wenhao Xie",
      "Tong Wu",
      "Lingxiao Ma",
      "Yuqing Xia",
      "Jilong Xue",
      "Fan Yang",
      "Zhi Yang"
    ],
    "affiliations": [],
    "summary": "Modern AI algorithms increasingly adopt fused kernels for performance, but implementing them remains complex due to the lack of fine-grained control in existing compilers like Triton. We introduce TileLang, a controllable programming system for fused neural kernels. TileLang provides explicit tile-level primitives for memory placement, data movement, and parallel scheduling. To guide developers in hardware-aware programming, the TileLang introduces two key techniques: tile inference which models tile programs as fused graphs and automatically deduces tile configuration from partial annotations; and tile recommendation that suggests efficient tile configurations based on hardware profiles and heuristics. TileLang makes it easy to express a wide range of fused attention kernels in under 80 lines of Python code, reducing code size by up to 90% compared to manual implementations. Evaluations show that TileLang achieves up to 5x speedup over Triton on NVIDIA H100 and up to 6 on AMD GPUs, demonstrating its ability to bridge programmability and performance.",
    "link": "/venue/Jb1WkNSfUB@OpenReview",
    "published_date": null,
    "conference": "ICLR.2026",
    "conference_year": 2026,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "Mixture-of-Experts Can Surpass Dense LLMs Under Strictly Equal Resource",
    "authors": [
      "Houyi Li",
      "Ka Man Lo",
      "Shijie Xuyang",
      "Ziqi Wang",
      "Wenzhen Zheng",
      "Haocheng Zhang",
      "Zhao Li",
      "Shuigeng Zhou",
      "Xiangyu Zhang",
      "Daxin Jiang"
    ],
    "affiliations": [],
    "summary": "Mixture-of-Experts (MoE) language models dramatically expand model capacity and achieve remarkable performance without increasing per-token compute. However, can MoEs surpass dense architectures under strictly equal resource constraints — that is, when the total parameter count, training compute, and data budget are identical? This question remains under-explored despite its significant practical value and potential. In this paper, we propose a novel perspective and methodological framework to study this question thoroughly. First, we comprehensively investigate the architecture of MoEs and achieve an optimal model design that maximizes the performance. Based on this, we subsequently find that an MoE model with activation rate in an optimal region is able to outperform its dense counterpart under the same total parameter, training compute and data resource. More importantly, this optimal region remains consistent across different model sizes. Although additional amount of data turns out to be a trade-off for enhanced performance, we show that this can be resolved via reusing data. We validate our findings through extensive experiments, training nearly 200 language models at 2B scale and over 50 at 7B scale, cumulatively processing 50 trillion tokens. All code and models will be released publicly.",
    "link": "/venue/oIdzliJAeA@OpenReview",
    "published_date": null,
    "conference": "ICLR.2026",
    "conference_year": 2026,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "In-The-Flow Agentic System Optimization for Effective Planning and Tool Use",
    "authors": [
      "Zhuofeng Li",
      "Haoxiang Zhang",
      "Seungju Han",
      "Sheng Liu",
      "Jianwen Xie",
      "Yu Zhang",
      "Yejin Choi",
      "James Y Zou",
      "Pan Lu"
    ],
    "affiliations": [],
    "summary": "Outcome-driven reinforcement learning has advanced reasoning in large language models (LLMs), but prevailing tool-augmented approaches train a single, monolithic policy that interleaves thoughts and tool calls under full context; this scales poorly with long horizons and diverse tools and generalizes weakly to new scenarios. Agentic systems offer a promising alternative by decomposing work across specialized modules, yet most remain training-free or rely on offline training decoupled from the live dynamics of multi-turn interaction. We introduce AgentFlow, a trainable, *in-the-flow* agentic framework that coordinates four modules (planner, executor, verifier, generator) through an evolving memory and directly optimizes its planner inside the multi-turn loop. To train on-policy in live environments, we propose *Flow-based Group Refined Policy Optimization* (Flow-GRPO), which tackles long-horizon, sparse-reward credit assignment by converting multi-turn optimization into a sequence of tractable single-turn policy updates. It broadcasts a single, verifiable trajectory-level outcome to every turn to align local planner decisions with global success and stabilizes learning with group-normalized advantages. Across ten benchmarks, AgentFlow with a 7B-scale backbone outperforms top-performing baselines with average accuracy gains of 14.9% on search, 14.0% on agentic, 14.5% on mathematical, and 4.1% on scientific tasks, even surpassing larger proprietary models like GPT-4o. Further analyses confirm the benefits of in-the-flow optimization, showing improved planning, enhanced tool-calling reliability, and positive scaling with model size and reasoning turns. Codebase is available at https://anonymous.4open.science/r/agentflow.",
    "link": "/venue/Mf5AleTUVK@OpenReview",
    "published_date": null,
    "conference": "ICLR.2026",
    "conference_year": 2026,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "Mamba-3: Improved Sequence Modeling using State Space Principles",
    "authors": [
      "Aakash Sunil Lahoti",
      "Kevin Li",
      "Berlin Chen",
      "Caitlin Wang",
      "Aviv Bick",
      "Zico Kolter",
      "Tri Dao",
      "Albert Gu"
    ],
    "affiliations": [],
    "summary": "The recent scaling of test-time compute for LLMs has restricted the practical deployment of models to those with strong capabilities that can generate high-quality outputs in an inference-efficient manner. While current Transformer-based models are the standard, their quadratic compute and linear memory bottlenecks have spurred the development of sub-quadratic models with linear-scaling compute with constant memory requirements. However, many recent linear-style models lack certain capabilities or lag behind in quality, and even their linear-time inference is not hardware-efficient. Guided by an inference-first perspective, we introduce three core methodological improvements inspired by the state-space model viewpoint of linear models. We combine a: 1) more expressive recurrence, 2) complex state update rule that enables richer state tracking, and 3) multi-input, multi-output formulation together, resulting in a stronger model that better exploits hardware parallelism during decoding. Together with architectural refinements, our **Mamba-3** model achieves significant gains across retrieval, state-tracking, and downstream language modeling tasks. Our new architecture sets the Pareto-frontier for performance under a fixed inference budget and outperforms strong baselines in a head-to-head comparison.",
    "link": "/venue/HwCvaJOiCj@OpenReview",
    "published_date": null,
    "conference": "ICLR.2026",
    "conference_year": 2026,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "WAVE: Learning Unified & Versatile Audio-Visual Embeddings with Multimodal LLM",
    "authors": [
      "Changli Tang",
      "Qinfan Xiao",
      "Ke Mei",
      "Tianyi Wang",
      "Fengyun Rao",
      "Chao Zhang"
    ],
    "affiliations": [],
    "summary": "While embeddings from multimodal large language models (LLMs) excel as general-purpose representations, their application to dynamic modalities like audio and video remains underexplored. We introduce WAVE (\\textbf{u}nified \\& \\textbf{v}ersatile \\textbf{a}udio-\\textbf{v}isual \\textbf{e}mbeddings), the first LLM-based embedding that creates a unified representation space for text, audio, and video modalities. WAVE employs a novel hierarchical feature fusion strategy and a joint multi-modal, multi-task training approach to enable two key capabilities: any-to-any cross-modal retrieval and the generation of prompt-aware embeddings tailored to user instructions. Experimentally, WAVE sets a new state-of-the-art on the MMEB-v2 video benchmark and achieves superior results in audio and video-to-audio retrieval. Its prompt-aware nature also yields remarkable performance in multimodal question answering, significantly outperforming existing embedding models. Ablation studies validate our joint training strategy, demonstrating improved performance across all modalities. With a newly introduced benchmark for versatile audio-visual learning, WAVE opens up broad possibilities for cross-modal, any-to-any applications. Our code, checkpoints, and data will be released.",
    "link": "/venue/MiV3WXDYJb@OpenReview",
    "published_date": null,
    "conference": "ICLR.2026",
    "conference_year": 2026,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "Mean Flow Policy with Instantaneous Velocity Constraint for One-step Action Generation",
    "authors": [
      "Guojian Zhan",
      "Letian Tao",
      "Pengcheng Wang",
      "Yixiao Wang",
      "Yuxin Chen",
      "Yiheng Li",
      "Hongyang Li",
      "Masayoshi Tomizuka",
      "Shengbo Li"
    ],
    "affiliations": [],
    "summary": "Learning expressive and efficient policy functions is a promising direction in reinforcement learning (RL). While flow-based policies have recently proven effective in modeling complex action distributions with a fast deterministic sampling process, they still face a trade-off between expressiveness and computational burden, which is typically controlled by the number of flow steps. In this work, we propose mean flow policy (MFP), a new generative policy function that models the mean velocity field to achieve the fastest one-step action generation. To ensure its high expressiveness, an instantaneous velocity constraint (IVC) is introduced on the mean velocity field during training. We theoretically prove that this design explicitly serves as a crucial boundary condition, thereby improving learning accuracy and enhancing policy expressiveness. Empirically, our MFP achieves state-of-the-art success rates across several challenging robotic manipulation tasks from Robomimic and OGBench. It also delivers substantial improvements in training and inference speed over existing flow-based policy baselines.",
    "link": "/venue/mIeKe74W43@OpenReview",
    "published_date": null,
    "conference": "ICLR.2026",
    "conference_year": 2026,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "In-Place Test-Time Training",
    "authors": [
      "Guhao Feng",
      "Shengjie Luo",
      "Kai Hua",
      "Ge Zhang",
      "Wenhao Huang",
      "Di He",
      "Tianle Cai"
    ],
    "affiliations": [],
    "summary": "The static \"train then deploy\" paradigm fundamentally limits Large Language Models (LLMs) from dynamically adapting their weights in response to continuous streams of new information inherent in real-world tasks. Test-Time Training (TTT) offers a compelling alternative by updating a subset of model parameters (fast weights) at inference time, yet its potential in the current LLM ecosystem is hindered by critical barriers including architectural incompatibility, computational inefficiency and misaligned fast weight objectives for language modeling. In this work, we introduce **In-Place Test-Time Training (In-Place TTT)**, a framework that seamlessly endows LLMs with Test-Time Training ability. In-Place TTT treats the final projection matrix of the ubiquitous MLP blocks as its adaptable fast weights, enabling a ``drop-in\" enhancement for LLMs without costly retraining from scratch. Furthermore, we replace TTT's generic reconstruction objective with a tailored, theoretically-grounded objective explicitly aligned with the Next-Token-Prediction task governing autoregressive language modeling. This principled objective, combined with an efficient chunk-wise update mechanism, results in a highly scalable algorithm compatible with context parallelism. Extensive experiments validate our framework's effectiveness: as an in-place enhancement, it enables a 4B-parameter model to achieve superior performance on tasks with contexts up to 128k, and when pretrained from scratch, it consistently outperforms competitive TTT-related approaches. Ablation study results further provide deeper insights on our design choices. Collectively, our results establish In-Place TTT as a promising step towards a paradigm of continual learning in LLMs.",
    "link": "/venue/dTWfCLSoyl@OpenReview",
    "published_date": null,
    "conference": "ICLR.2026",
    "conference_year": 2026,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "FIRE: Frobenius-Isometry Reinitialization for Balancing the Stability–Plasticity Tradeoff",
    "authors": [
      "Isaac Han",
      "Sangyeon Park",
      "Seungwon Oh",
      "Donghu Kim",
      "Hojoon Lee",
      "KyungJoong Kim"
    ],
    "affiliations": [],
    "summary": "Deep neural networks trained on nonstationary data must balance stability (i.e., retaining prior knowledge) and plasticity (i.e., adapting to new tasks). Standard reinitialization methods, which reinitialize weights toward their original values, are widely used but difficult to tune: conservative reinitializations fail to restore plasticity, while aggressive ones erase useful knowledge. We propose FIRE, a principled reinitialization method that explicitly balances the stability–plasticity tradeoff. FIRE quantifies stability through Squared Frobenius Error (SFE), measuring proximity to past weights, and plasticity through Deviation from Isometry (DfI), reflecting weight isotropy. The reinitialization point is obtained by solving a constrained optimization problem, minimizing SFE subject to DfI being zero, which is efficiently approximated by Newton–Schulz iteration. FIRE is evaluated on continual visual learning (CIFAR-10 with ResNet-18), language modeling (OpenWebText with GPT-0.1B), and reinforcement learning (HumanoidBench with SAC and Atari games with DQN). Across all domains, FIRE consistently outperforms both naive training without intervention and standard reinitialization methods, demonstrating effective balancing of the stability–plasticity tradeoff.",
    "link": "/venue/CfZLxT3zIZ@OpenReview",
    "published_date": null,
    "conference": "ICLR.2026",
    "conference_year": 2026,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "Addressing divergent representations from causal interventions on neural networks",
    "authors": [
      "Satchel Grant",
      "Simon Jerome Han",
      "Alexa Tartaglini",
      "Christopher Potts"
    ],
    "affiliations": [],
    "summary": "A common approach to mechanistic interpretability is to causally manipulate model representations via targeted interventions in order to understand what those representations encode. Here we ask whether such interventions create out-of-distribution (divergent) representations, and whether this raises concerns about how faithful their resulting explanations are to the target model in its natural state. First, we demonstrate theoretically and empirically that common causal intervention techniques often do shift internal representations away from the natural distribution of the target model. Then, we provide a theoretical analysis of two cases of such divergences: \"harmless\" divergences that occur in the behavioral null-space of the layer(s) of interest, and \"pernicious\" divergences that activate hidden network pathways and cause dormant behavioral changes. Finally, in an effort to mitigate the pernicious cases, we apply and modify the Counterfactual Latent (CL) loss from Grant (2025) allowing representations from causal interventions to remain closer to the natural distribution, reducing the likelihood of harmful divergences while preserving the interpretive power of the interventions. Together, these results highlight a path towards more reliable interpretability methods.",
    "link": "/venue/cZrTMqYVL6@OpenReview",
    "published_date": null,
    "conference": "ICLR.2026",
    "conference_year": 2026,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "Real-Time Calibration Model for Low-Cost Sensor in Fine-Grained Time Series",
    "authors": [
      "Seokho Ahn",
      "Hyungjin Kim",
      "Sungbok Shin",
      "Young-Duk Seo"
    ],
    "affiliations": [],
    "summary": "Precise measurements from sensors are crucial, but data is usually collected from low-cost, low-tech systems, which are often inaccurate. Thus, they require further calibrations. To that end, we first identify three requirements for effective calibration under practical low-tech sensor conditions. Based on the requirements, we develop a model called TESLA, Transformer for effective sensor calibration utilizing logarithmic-binned attention. TESLA uses a high-performance deep learning model, Transformers, to calibrate and capture non-linear components. At its core, it employs logarithmic binning, to minimize attention complexity. TESLA achieves consistent real-time calibration, even with longer sequences and finer-grained time series in hardware-constrained systems. Experiments show that TESLA outperforms existing novel deep learning and newly crafted linear models in accuracy, calibration speed, and energy efficiency.",
    "link": "/venue/31974@AAAI",
    "published_date": null,
    "conference": "AAAI.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "BayesCNS: A Unified Bayesian Approach to Address Cold Start and Non-Stationarity in Search Systems at Scale",
    "authors": [
      "Randy Ardywibowo",
      "Rakesh Sunki",
      "Shin Tsz Lucy Kuo",
      "Sankalp Nayak"
    ],
    "affiliations": [],
    "summary": "Information Retrieval (IR) systems used in search and recommendation platforms frequently employ Learning-to-Rank (LTR) models to rank items in response to user queries. These models heavily rely on features derived from user interactions, such as clicks and engagement data. This dependence introduces cold start issues for items lacking user engagement and poses challenges in adapting to non-stationary shifts in user behavior over time. We address both challenges holistically as an online learning problem and propose BayesCNS, a Bayesian approach designed to handle cold start and non-stationary distribution shifts in search systems at scale. BayesCNS achieves this by estimating prior distributions for user-item interactions, which are continuously updated with new user interactions gathered online. This online learning procedure is guided by a ranker model, enabling efficient exploration of relevant items using contextual information provided by the ranker. We successfully deployed BayesCNS in a large-scale search system and demonstrated its efficacy through comprehensive offline and online experiments. Notably, an online A/B experiment showed a 10.60% increase in new item interactions and a 1.05% improvement in overall success metrics over the existing production baseline.",
    "link": "/venue/31975@AAAI",
    "published_date": null,
    "conference": "AAAI.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "Scalable Surrogate Verification of Image-Based Neural Network Control Systems Using Composition and Unrolling",
    "authors": [
      "Feiyang Cai",
      "Chuchu Fan",
      "Stanley Bak"
    ],
    "affiliations": [],
    "summary": "Verifying safety of neural network control systems that use images as input is a difficult problem because, from a given system state, there is no known way to mathematically model what images are possible in the real-world. We build upon recent work that considers a surrogate verification approach, training a conditional generative adversarial network (cGAN) as an image generator in place of the real world. This setup enables set-based formal analysis of the closed-loop system, providing analysis beyond simulation and testing. While existing work is effective on small examples, excessive overapproximation both within a single control period (one-step error) and across multiple periods (multi-step error) limits its scalability. We propose approaches to overcome these errors. First, we address one-step error by composing the system's dynamics along with the cGAN and neural network controller, without losing the dependencies between input states and the control outputs as in the monotonic analysis of the system dynamics. Second, we reduce multi-step error by repeating the single-step composition, essentially unrolling multiple steps of the control loop into a large neural network. We then leverage existing network verification algorithms to compute accurate reachable sets for multiple steps, avoiding the accumulation of abstraction error at each step.We demonstrate the effectiveness of our approach in terms of both accuracy and scalability using two case studies. On the aircraft taxiing system, the converged reachable set is 175% larger using the prior baseline method compared with our proposed approach. On the emergency braking system, with 24x the number of image output variables from the cGAN, the baseline method fails to prove any states are safe, whereas our improvements enable set-based safety analysis.",
    "link": "/venue/31976@AAAI",
    "published_date": null,
    "conference": "AAAI.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "External Reliable Information-enhanced Multimodal Contrastive Learning for Fake News Detection",
    "authors": [
      "Biwei Cao",
      "Qihang Wu",
      "Jiuxin Cao",
      "Bo Liu",
      "Jie Gui"
    ],
    "affiliations": [],
    "summary": "With the rapid development of the Internet, the information dissemination paradigm has changed and the efficiency has been improved greatly. While this also brings the quick spread of fake news and leads to negative impacts on cyberspace. Currently, the information presentation formats have evolved gradually, with the news formats shifting from texts to multimodal contents. As a result, detecting multimodal fake news has become one of the research hotspots. However, multimodal fake news detection research field still faces two main challenges: the inability to fully and effectively utilize multimodal information for detection, and the low credibility or static nature of the introduced external information, which limits dynamic updates. To bridge the gaps, we propose ERIC-FND, an external reliable information-enhanced multimodal contrastive learning framework for fake news detection. ERIC-FND strengthens the representation of news contents by entity-enriched external information enhancement method. It also enriches the multimodal news information via multimodal semantic interaction method where the multimodal constrative learning is employed to make different modality representations learn from each other. Moreover, an adaptive fusion method is taken to integrate the news representations from different dimensions for the eventual classification. Experiments are done on two commonly used datasets in different languages, X (Twitter) and Weibo. Experiment results demonstrate that our proposed model ERIC-FND outperforms existing state-of-the-art fake news detection methods under the same settings.",
    "link": "/venue/31977@AAAI",
    "published_date": null,
    "conference": "AAAI.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "Holistic Semantic Representation for Navigational Trajectory Generation",
    "authors": [
      "Ji Cao",
      "Tongya Zheng",
      "Qinghong Guo",
      "Yu Wang",
      "Junshu Dai",
      "Shunyu Liu",
      "Jie Yang",
      "Jie Song",
      "Mingli Song"
    ],
    "affiliations": [],
    "summary": "Trajectory generation has garnered significant attention from researchers in the field of spatio-temporal analysis, as it can generate substantial synthesized human mobility trajectories that enhance user privacy and alleviate data scarcity. However, existing trajectory generation methods often focus on improving trajectory generation quality from a singular perspective, lacking a comprehensive semantic understanding across various scales. Consequently, we are inspired to develop a HOlistic SEmantic Representation (HOSER) framework for navigational trajectory generation. Given an origin-and-destination (OD) pair and the starting time point of a latent trajectory, we first propose a Road Network Encoder to expand the receptive field of road- and zone-level semantics. Second, we design a Multi-Granularity Trajectory Encoder to integrate the spatio-temporal semantics of the generated trajectory at both the point and trajectory levels. Finally, we employ a Destination-Oriented Navigator to seamlessly integrate destination-oriented guidance. Extensive experiments on three real-world datasets demonstrate that HOSER outperforms state-of-the-art baselines by a significant margin. Moreover, the model's performance in few-shot learning and zero-shot learning scenarios further verifies the effectiveness of our holistic semantic representation.",
    "link": "/venue/31978@AAAI",
    "published_date": null,
    "conference": "AAAI.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "SQLFixAgent: Towards Semantic-Accurate Text-to-SQL Parsing via Consistency-Enhanced Multi-Agent Collaboration",
    "authors": [
      "Jipeng Cen",
      "Jiaxin Liu",
      "Zhixu Li",
      "Jingjing Wang"
    ],
    "affiliations": [],
    "summary": "While fine-tuned large language models (LLMs) excel in generating grammatically valid SQL in Text-to-SQL parsing, they often struggle to ensure semantic accuracy in queries, leading to user confusion and diminished system usability. To tackle this challenge, we introduce SQLFixAgent, a new consistency-enhanced multi-agent collaborative framework designed for detecting and repairing erroneous SQL. Our framework comprises a core agent, SQLRefiner, alongside two auxiliary agents: SQLReviewer and QueryCrafter. The SQLReviewer agent employs the rubber duck debugging method to identify potential semantic mismatches between SQL and user query. If the error is detected, the QueryCrafter agent generates multiple SQL as candidate repairs using a fine-tuned SQLTool. Subsequently, leveraging similar repair retrieval and failure memory reflection, the SQLRefiner agent selects the most fitting SQL statement from the candidates as the final repair. We evaluated our proposed framework on five Text-to-SQL benchmarks. The experimental results show that our method consistently enhances the performance of the baseline model, specifically achieving an execution accuracy improvement of over 3% on the Bird benchmark. Our framework also has a higher token efficiency compared to other advanced methods, making it more competitive.",
    "link": "/venue/31979@AAAI",
    "published_date": null,
    "conference": "AAAI.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "mmFAS: Multimodal Face Anti-Spoofing Using Multi-Level Alignment and Switch-Attention Fusion",
    "authors": [
      "Geng Chen",
      "Wuyuan Xie",
      "Di Lin",
      "Ye Liu",
      "Miaohui Wang"
    ],
    "affiliations": [],
    "summary": "The increasing number of presentation attacks on reliable face matching has raised concerns and garnered attention towards face anti-spoofing (FAS). However, existing methods for FAS modeling commonly fuse multiple visual modalities (e.g., RGB, Depth, and Infrared) in a straightforward manner, disregarding latent feature gaps that can hinder representation learning. To address this challenge, we propose a novel multimodal FAS framework (mmFAS) that focuses on explicit alignment and fusion of latent features across different modalities. Specifically, we develop a multimodal alignment module to alleviate the latent feature gap by using instance-level contrastive learning and class-level matching simultaneously. Further, we explore a new switch-attention based fusion module to automatically aggregate complementary information and control model complexity. To evaluate the anti-spoofing performance more effectively, we adopt a challenging yet meaningful cross-database protocol involving four benchmark multimodal FAS datasets to simulate realworld scenarios. Extensive experimental results demonstrate the effectiveness of mmFAS in improving the accuracy of FAS systems, outperforming 10 representative methods.",
    "link": "/venue/31980@AAAI",
    "published_date": null,
    "conference": "AAAI.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "CLEP: A Novel Contrastive Learning Method for Evolutionary Reentrancy Vulnerability Detection",
    "authors": [
      "Jie Chen",
      "Liangmin Wang",
      "Huijuan Zhu",
      "Victor S. Sheng"
    ],
    "affiliations": [],
    "summary": "Reentrancy vulnerabilities in smart contracts have been exploited to steal enormous amounts of money, thus detecting reentrancy vulnerabilities is a hotspot issue in security research. However, a new attack is emerging in which attackers continuously release new reentrancy patterns to exploit fresh vulnerabilities and obfuscate existing ones. Existing detection methods neglect the time-series evolution of vulnerabilities across different smart contract versions, leading to a gradual decline in their effectiveness over time. We investigate the time-series correlations among vulnerabilities in various versions and refer to these as Evolutionary Reentrancy Vulnerabilities (ERVs). We summarize that ERVs detection faces two key challenges: (i) capturing the evolving pattern of ERVs along a complete evolutionary chain and (ii) detecting fresh reentrancy vulnerabilities in new versions. To address these challenges, we propose CLEP, a novel Contrastive Learning with Evolving Pairs detection method. It can effectively capture the evolving patterns by discerning similarities and differences across versions. Specifically, we first modified the sample distribution by incorporating version declarations as time-series evolution information. Then, leveraging the hierarchical similarity, we design an evolving pairs scheme to form negative and positive contract pairs across versions. Finally, we build a complete evolutionary chain by proposing a version-aware contrastive sampler. Our experimental results show that CLEP not only outperforms state-of-the-art baselines in version-specific scenarios but also shows promising performance in cross-version evolution scenarios.",
    "link": "/venue/31981@AAAI",
    "published_date": null,
    "conference": "AAAI.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "GLCF: A Global-Local Multimodal Coherence Analysis Framework for Talking Face Generation Detection",
    "authors": [
      "Xiaocan Chen",
      "Qilin Yin",
      "Jiarui Liu",
      "Wei Lu",
      "Xiangyang Luo",
      "Jiantao Zhou"
    ],
    "affiliations": [],
    "summary": "Talking face generation (TFG) allows for producing lifelike talking videos of any character using only facial images and accompanying text. Abuse of this technology could pose significant risks to society, creating the urgent need for research into corresponding detection methods. However, research in this field has been hindered by the lack of public datasets. In this paper, we construct the first large-scale multi-scenario talking face dataset (MSTF), which contains 22 audio and video forgery techniques, filling the gap of datasets in this field. The dataset covers 11 generation scenarios and more than 20 semantic scenarios, closer to the practical application scenario of TFG. Besides, we also propose a TFG detection framework, which leverages the analysis of both global and local coherence in the multimodal content of TFG videos. Therefore, a region-focused smoothness detection module (RSFDM) and a discrepancy capture-time frame aggregation module (DCTAM) are introduced to evaluate the global temporal coherence of TFG videos, aggregating multi-grained spatial information. Additionally, a visual-audio fusion module (V-AFM) is designed to evaluate audiovisual coherence within a localized temporal perspective. Comprehensive experiments demonstrate the reasonableness and challenges of our datasets, while also indicating the superiority of our proposed method compared to the state-of-the-art deepfake detection approaches.",
    "link": "/venue/31982@AAAI",
    "published_date": null,
    "conference": "AAAI.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "ReactGPT: Understanding of Chemical Reactions via In-Context Tuning",
    "authors": [
      "Zhe Chen",
      "Zhe Fang",
      "Wenhao Tian",
      "Zhaoguang Long",
      "Changzhi Sun",
      "Yuefeng Chen",
      "Hao Yuan",
      "Honglin Li",
      "Man Lan"
    ],
    "affiliations": [],
    "summary": "The interdisciplinary field of chemistry and artificial intelligence (AI) is an active area of research aimed at accelerating scientific discovery. Large language Models (LLMs) have shown significant promise in biochemical tasks, especially the molecule caption translation, which aims to align between molecules and natural language texts. However, existing works mainly focus on single molecules, while alignment between chemical reactions and natural language text remains largely unexplored. Additionally, the description of reactions is an essential part in biochemical patents and literature, and research on this aspect not only can help better understand chemical reactions but also promote research on automating chemical synthesis and retrosynthesis. In this work, we propose \\textbf{ReactGPT}, a framework aiming to bridge the gap between chemical reaction and text. ReactGPT allows a new task: reaction captioning, by adapting LLMs to learn reaction-text alignment from context examples via In-Context Tuning. Specifically, ReactGPT jointly leverages a Fingerprints-based Reaction Retrieval module, a Domain-Specific Prompt Design module, and a two-stage In-Context Tuning module. We evaluate the effectiveness of ReactGPT on reaction captioning and experimental procedure prediction, both of these tasks can reflect the understanding of chemical reactions. Experimental results show that compared to previous models, ReactGPT exhibits competitive capabilities in resolving chemical reactions and generating high-quality text with correct structure.",
    "link": "/venue/31983@AAAI",
    "published_date": null,
    "conference": "AAAI.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "4D Diffusion for Dynamic Protein Structure Prediction with Reference and Motion Guidance",
    "authors": [
      "Kaihui Cheng",
      "Ce Liu",
      "Qingkun Su",
      "Jun Wang",
      "Liwei Zhang",
      "Yining Tang",
      "Yao Yao",
      "Siyu Zhu",
      "Yuan Qi"
    ],
    "affiliations": [],
    "summary": "Protein structure prediction is pivotal for understanding the structure-function relationship of proteins, advancing biological research, and facilitating pharmaceutical development and experimental design. While deep learning methods and the expanded availability of experimental 3D protein structures have accelerated structure prediction, the dynamic nature of protein structures has received limited attention. This study introduces an innovative 4D diffusion model incorporating molecular dynamics (MD) simulation data to learn dynamic protein structures. Our approach is distinguished by the following components: (1) a unified diffusion model capable of generating dynamic protein structures, including both the backbone and side chains, utilizing atomic grouping and side-chain dihedral angle predictions; (2) a reference network that enhances structural consistency by integrating the latent embeddings of the initial 3D protein structures; and (3) a motion alignment module aimed at improving temporal structural coherence across multiple time steps. To our knowledge, this is the first diffusion-based model aimed at predicting protein trajectories across multiple time steps simultaneously. Validation on benchmark datasets demonstrates that our model exhibits high accuracy in predicting dynamic 3D structures of proteins containing up to 256 amino acids over 32 time steps, effectively capturing both local flexibility in stable states and significant conformational changes.",
    "link": "/venue/31984@AAAI",
    "published_date": null,
    "conference": "AAAI.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "G2LDetect: A Global-to-Local Approach for Hallucination Detection",
    "authors": [
      "Xiaoxia Cheng",
      "Zeqi Tan",
      "Zhe Zheng",
      "Weiming Lu"
    ],
    "affiliations": [],
    "summary": "Hallucination detection has attracted considerable interest due to the tendency of language models to generate texts that contain hallucinations. Most existing methods start with specific local details directly extracted from text, then aggregate to form the final conclusion. However, this direct extraction approach ignores the global context, leading to isolated details, and is prone to missed or over-detections. In this paper, we present a global-to-local approach for hallucination detection (G2LDetect), which considers the global information of the text before identifying local details. We first construct a global representation of the text by transforming it into a hierarchical tree structure. Afterward, we obtain specific local details from the global tree representation using path-wise identification and perform detection on them. This global-to-local detection process ensures that local details are context-aware and complete, thus making more accurate and reliable detection results. Experimental results show that our global-to-local method outperforms existing methods, especially for longer texts.",
    "link": "/venue/31985@AAAI",
    "published_date": null,
    "conference": "AAAI.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "3D Denoisers Are Good 2D Teachers: Molecular Pretraining via Denoising and Cross-Modal Distillation",
    "authors": [
      "Sungjun Cho",
      "Dae-Woong Jeong",
      "Sung Moon Ko",
      "Jinwoo Kim",
      "Sehui Han",
      "Seunghoon Hong",
      "Honglak Lee",
      "Moontae Lee"
    ],
    "affiliations": [],
    "summary": "Pretraining molecular representations from large unlabeled data is essential for molecular property prediction due to the high cost of obtaining ground-truth labels. While there exist various 2D graph-based molecular pretraining approaches, these methods struggle to show statistically significant gains in predictive performance. Recent work have thus instead proposed 3D conformer-based pretraining under the task of denoising, leading to promising results. During downstream finetuning, however, models trained with 3D conformers require accurate atom-coordinates of previously unseen molecules, which are computationally expensive to acquire at scale. In this paper, we propose a simple solution of denoise-and-distill (D&D), a self-supervised molecular representation learning method that pretrains a 2D graph encoder by distilling representations from a 3D denoiser. With denoising followed by cross-modal knowledge distillation, our approach enjoys use of knowledge obtained from denoising as well as painless application to downstream tasks with no access to 3D conformers. Experiments on real-world molecular property prediction datasets show that the graph encoder trained via D&D can infer 3D information based on the 2D graph and shows superior performance and label-efficiency against previous methods.",
    "link": "/venue/31986@AAAI",
    "published_date": null,
    "conference": "AAAI.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "CryoDomain: Sequence-free Protein Domain Identification from Low-resolution Cryo-EM Density Maps",
    "authors": [
      "Muzhi Dai",
      "Zhuoer Dong",
      "Weining Fu",
      "Kui Xu",
      "Qiangfeng Cliff Zhang"
    ],
    "affiliations": [],
    "summary": "Cryo-electron microscopy (cryo-EM) has revolutionized the field of structural biology, determining structures of large protein machines and sharpening the understanding of fundamental biological processes. Despite cryo-EM’s unique capacity to discover novel proteins from unpurified samples and reveal the intricate structures of protein complexes within native cellular environments, the advancement of protein identification methods for cryo-EM lags behind. Without prior knowledge, such as sequence, protein identification from low-resolution density maps remains challenging. Here we introduce CryoDomain, an innovative method for identifying protein domains — conserved constituent units of proteins — from low-resolution cryo-EM density maps without requiring prior knowledge of protein sequences. CryoDomain leverages cross-modal alignment to correlate cryo-EM density maps with atomic structures, transferring the knowledge learned on a large atomic structure dataset to a sparse density map dataset. On two protein domain benchmarks constructed from CATH and SCOPe, CryoDomain significantly outperforms the state-of-the-art methods for domain identification from low-resolution density maps. CryoDomain liberates structural biologists from the tedious tasks of density inspection and database searching during protein identification. It has the potential to extend the border of unbiased structure discovery and cellular landscape investigation using cryo-EM.",
    "link": "/venue/31987@AAAI",
    "published_date": null,
    "conference": "AAAI.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "Less Is More: Adaptive Program Repair with Bug Localization and Preference Learning",
    "authors": [
      "Zhenlong Dai",
      "Bingrui Chen",
      "Zhuoluo Zhao",
      "Xiu Tang",
      "Sai Wu",
      "Chang Yao",
      "Zhipeng Gao",
      "Jingyuan Chen"
    ],
    "affiliations": [],
    "summary": "Automated Program Repair (APR) is a task to automatically generate patches for the buggy code. However, most research focuses on generating correct patches while ignoring the consistency between the fixed code and the original buggy code. How to conduct adaptive bug fixing and generate patches with minimal modifications have seldom been investigated. To bridge this gap, we first introduce a novel task, namely AdaPR (Adaptive Program Repair). We then propose a two-stage approach AdaPatcher (Adaptive Patch Generator) to enhance program repair while maintaining the consistency. In the first stage, we utilize a Bug Locator with self-debug learning to accurately pinpoint bug locations. In the second stage, we train a Program Modifier to ensure consistency between the post-modified fixed code and the pre-modified buggy code. The Program Modifier is enhanced with a location-aware repair learning strategy to generate patches based on identified buggy lines, a hybrid training strategy for selective reference and an adaptive preference learning to prioritize fewer changes. The experimental results show that our approach outperforms a set of baselines by a large margin, validating the effectiveness of our two-stage framework for the newly proposed AdaPR task.",
    "link": "/venue/31988@AAAI",
    "published_date": null,
    "conference": "AAAI.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "Improving Cancer Gene Prediction by Enhancing Common Information Between the PPI Network and Gene Functional Association",
    "authors": [
      "Chao Deng",
      "Hongdong Li",
      "Jianxin Wang"
    ],
    "affiliations": [],
    "summary": "Identifying cancer genes is crucial for treatment and understanding pathogenesis. Recent methods typically leverage protein-protein interaction (PPI) networks or gene functional association data from annotated gene sets. There may be some shared neighborhood structure information between these two types of gene association data. While this common information may contain more accurate gene association information, existing methods often overlook this potential. To address this gap, we introduce DISFusion, which integrates multi-omics cancer data, PPI networks, and gene functional associations to identify cancer genes. A key innovation of DISFusion is the cross-view decorrelation loss, which enhances the common information between PPI networks and gene functional associations, thereby improving prediction accuracy. Extensive experiments indicate that DISFusion outperforms state-of-the-art methods and exhibits greater generalization ability. Moreover, analysis of CPTAC pan-cancer proteomic data highlights significant associations between the 30 novel cancer genes predicted by DISFusion and multiple cancer types, underscoring its practical utility. These findings validate the effectiveness of enhancing common information and provide new insights into cancer gene identification.",
    "link": "/venue/31989@AAAI",
    "published_date": null,
    "conference": "AAAI.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "AutoSciLab: A Self-Driving Laboratory for Interpretable Scientific Discovery",
    "authors": [
      "Saaketh Desai",
      "Sadhvikas Addamane",
      "Jeffrey Y. Tsao",
      "Igal Brener",
      "Laura P. Swiler",
      "Remi Dingreville",
      "Prasad P. Iyer"
    ],
    "affiliations": [],
    "summary": "Advances in robotic control and sensing have propelled the rise of automated scientific laboratories capable of high-throughput experiments. However, automated scientific laboratories are currently limited by human intuition in their ability to efficiently design and interpret experiments in high-dimensional spaces, throttling scientific discovery. We present AutoSciLab, a machine learning framework for driving autonomous scientific experiments, forming a surrogate researcher purposed for scientific discovery in high-dimensional spaces. AutoSciLab autonomously follows the scientific method in four steps: (i) generating high-dimensional experiments (x) using a variational autoencoder (ii) selecting optimal experiments by forming hypotheses using active learning (iii) distilling the experimental results to discover relevant low-dimensional latent variables (z) with a ‘directional autoencoder’ and (iv) learning a human interpretable equation connecting the discovered latent variables with a quantity of interest (y = f (z)), using a neural network equation learner. We validate the generalizability of AutoSciLab by rediscovering a) the principles of projectile motion and b) the phase-transitions within the spin-states of the Ising model (NP-hard problem). Applying our framework to an open-ended nanophotonics problem, AutoSciLab discovers a new way to steer incoherent light emission beyond current state-of-the-art, defining a new structure(material)-property(light-emission) relationship governing the physical process using closed-loop noisy experimental feedback.",
    "link": "/venue/31990@AAAI",
    "published_date": null,
    "conference": "AAAI.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "RingFormer: A Ring-Enhanced Graph Transformer for Organic Solar Cell Property Prediction",
    "authors": [
      "Zhihao Ding",
      "Ting Zhang",
      "Yiran Li",
      "Jieming Shi",
      "Chen Jason Zhang"
    ],
    "affiliations": [],
    "summary": "Organic Solar Cells (OSCs) are a promising technology for sustainable energy production. However, the identification of molecules with desired OSC properties typically involves laborious experimental research. To accelerate progress in the field, it is crucial to develop machine learning models capable of accurately predicting the properties of OSC molecules. While graph representation learning has demonstrated success in molecular property prediction, it remains underexplored for OSC-specific tasks. Existing methods fail to capture the unique structural features of OSC molecules, particularly the intricate ring systems that critically influence OSC properties, leading to suboptimal performance. To fill the gap, we present RingFormer, a novel graph transformer framework specially designed to capture both atom and ring level structural patterns in OSC molecules. RingFormer constructs a hierarchical graph that integrates atomic and ring structures and employs a combination of local message passing and global attention mechanisms to generate expressive graph representations for accurate OSC property prediction. We evaluate RingFormer's effectiveness on five curated OSC molecule datasets through extensive experiments. The results demonstrate that RingFormer consistently outperforms existing methods, achieving a 22.77% relative improvement over the nearest competitor on the CEPDB dataset.",
    "link": "/venue/31991@AAAI",
    "published_date": null,
    "conference": "AAAI.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "Knowledge Is Power: Harnessing Large Language Models for Enhanced Cognitive Diagnosis",
    "authors": [
      "Zhiang Dong",
      "Jingyuan Chen",
      "Fei Wu"
    ],
    "affiliations": [],
    "summary": "Cognitive Diagnosis Models (CDMs) are designed to assess students' cognitive states by analyzing their performance across a series of exercises. However, existing CDMs often struggle with diagnosing infrequent students and exercises due to a lack of rich prior knowledge. With the advancement in large language models (LLMs), which possess extensive domain knowledge, their integration into cognitive diagnosis presents a promising opportunity. Despite this potential, integrating LLMs with CDMs poses significant challenges. LLMs are not well-suited for capturing the fine-grained collaborative interactions between students and exercises, and the disparity between the semantic space of LLMs and the behavioral space of CDMs hinders effective integration. To address these issues, we propose a novel Knowledge-enhanced Cognitive Diagnosis (KCD) framework, which is a model-agnostic framework utilizing LLMs to enhance CDMs and compatible with various CDM architectures. The KCD framework operates in two stages: LLM Diagnosis and Cognitive Level Alignment. In the LLM Diagnosis stage, both students and exercises are diagnosed to achieve comprehensive and detailed modeling. In the Cognitive Level Alignment stage, we bridge the gap between the CDMs' behavioral space and the LLMs' semantic space using contrastive learning and mask-reconstruction approaches. Experiments on several real-world datasets demonstrate the effectiveness of our proposed framework.",
    "link": "/venue/31992@AAAI",
    "published_date": null,
    "conference": "AAAI.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "FactorGCL: A Hypergraph-Based Factor Model with Temporal Residual Contrastive Learning for Stock Returns Prediction",
    "authors": [
      "Yitong Duan",
      "Weiran Wang",
      "Jian Li"
    ],
    "affiliations": [],
    "summary": "As a fundamental method in economics and finance, the factor model has been extensively utilized in quantitative investment. In recent years, there has been a paradigm shift from traditional linear models with expert-designed factors to more flexible nonlinear machine learning-based models with data-driven factors, aiming to enhance the effectiveness of these factor models. However, due to the low signal-to-noise ratio in market data, mining effective factors in data-driven models remains challenging. In this work, we propose a hypergraph-based factor model with temporal residual contrastive learning (FactorGCL) that employs a hypergraph structure to better capture high-order nonlinear relationships among stock returns and factors. To mine hidden factors that supplement human-designed prior factors for predicting stock returns, we design a cascading residual hypergraph architecture, in which the hidden factors are extracted from the residual information after removing the influence of prior factors. Additionally, we propose a temporal residual contrastive learning method to guide the extraction of effective and comprehensive hidden factors by contrasting stock-specific residual information over different time periods. Our extensive experiments on real stock market data demonstrate that FactorGCL not only outperforms existing state-of-the-art methods but also mines effective hidden factors for predicting stock returns.",
    "link": "/venue/31993@AAAI",
    "published_date": null,
    "conference": "AAAI.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "How to Re-enable PDE Loss for Physical Systems Modeling Under Partial Observation",
    "authors": [
      "Haodong Feng",
      "Yue Wang",
      "Dixia Fan"
    ],
    "affiliations": [],
    "summary": "In science and engineering, machine learning techniques are increasingly successful in physical systems modeling (predicting future states of physical systems). Effectively integrating PDE loss as a constraint of system transition can improve the model's prediction by overcoming generalization issues due to data scarcity, especially when data acquisition is costly. However, in many real-world scenarios, due to sensor limitations, the data we can obtain is often only partial observation, making the calculation of PDE loss seem to be infeasible, as the PDE loss heavily relies on high-resolution states. We carefully study this problem and propose a novel framework named Re-enable PDE Loss under Partial Observation (RPLPO). The key idea is that although enabling PDE loss to constrain system transition solely is infeasible, we can re-enable PDE loss by reconstructing the learnable high-resolution state and constraining system transition simultaneously. Specifically, RPLPO combines an encoding module for reconstructing learnable high-resolution states with a transition module for predicting future states. The two modules are jointly trained by data and PDE loss. We conduct experiments in various physical systems to demonstrate that RPLPO has significant improvement in generalization, even when observation is sparse, irregular, noisy, and PDE is inaccurate.",
    "link": "/venue/31994@AAAI",
    "published_date": null,
    "conference": "AAAI.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "APIRL: Deep Reinforcement Learning for REST API Fuzzing",
    "authors": [
      "Myles Foley",
      "Sergio Maffeis"
    ],
    "affiliations": [],
    "summary": "REST APIs have become key components of web services. However, they often contain logic flaws resulting in server side errors or security vulnerabilities. HTTP requests are used as test cases to find and mitigate such issues. Existing methods to modify requests, including those using deep learning, suffer from limited performance and precision, relying on undirected search or making limited usage of the contextual information. In this paper we propose APIRL, a fully automated deep reinforcement learning tool for testing REST APIs. A key novelty of our approach is the use of feedback from a transformer module pre-trained on JSON-structured data, akin to that used in API responses. This allows APIRL to learn the subtleties relating to test outcomes, and generalise to unseen API endpoints. We show APIRL can find significantly more bugs than the state-of-the-art in real world REST APIs while minimising the number of required test cases. We also study how reward functions, and other key design choices, affect learnt policies with a thorough ablation study.",
    "link": "/venue/31995@AAAI",
    "published_date": null,
    "conference": "AAAI.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "A Theoretical Framework for an Efficient Normalizing Flow-Based Solution to the Electronic Schrödinger Equation",
    "authors": [
      "Daniel Freedman",
      "Eyal Rozenberg",
      "Alex Bronstein"
    ],
    "affiliations": [],
    "summary": "A central problem in quantum mechanics involves solving the Electronic Schrödinger Equation for a molecule or material. The Variational Monte Carlo approach to this problem approximates a particular variational objective via sampling, and then optimizes this approximated objective over a chosen parameterized family of wavefunctions, known as the ansatz. Recently neural networks have been used as the ansatz, with accompanying success. However, sampling from such wavefunctions has required the use of a Markov Chain Monte Carlo approach, which is inherently inefficient. In this work, we propose a solution to this problem via an ansatz which is cheap to sample from, yet satisfies the requisite quantum mechanical properties. We prove that a normalizing flow using the following two essential ingredients satisfies our requirements: (a) a base distribution which is constructed from Determinantal Point Processes; (b) flow layers which are equivariant to a particular subgroup of the permutation group. We then show how to construct both continuous and discrete normalizing flows which satisfy the requisite equivariance. We further demonstrate the manner in which the non-smooth nature (``cusps'') of the wavefunction may be captured, and how the framework may be generalized to provide induction across multiple molecules. The resulting theoretical framework entails an efficient approach to solving the Electronic Schrödinger Equation.",
    "link": "/venue/31996@AAAI",
    "published_date": null,
    "conference": "AAAI.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "EWMoE: An Effective Model for Global Weather Forecasting with Mixture-of-Experts",
    "authors": [
      "Lihao Gan",
      "Xin Man",
      "Chenghong Zhang",
      "Jie Shao"
    ],
    "affiliations": [],
    "summary": "Weather forecasting is a crucial task for meteorologic research, with direct social and economic impacts. Recently, data-driven weather forecasting models based on deep learning have shown great potential, achieving superior performance compared with traditional numerical weather prediction methods. However, these models often require massive training data and computational resources. In this paper, we propose EWMoE, an effective model for accurate global weather forecasting, which requires significantly less training data and computational resources. Our model incorporates three key components to enhance prediction accuracy: 3D absolute position embedding, a core Mixture-of-Experts (MoE) layer, and two specific loss functions. We conduct our evaluation on the ERA5 dataset using only two years of training data. Extensive experiments demonstrate that EWMoE outperforms current models such as FourCastNet and ClimaX at all forecast time, achieving competitive performance compared with the state-of-the-art models Pangu-Weather and GraphCast in evaluation metrics such as Anomaly Correlation Coefficient (ACC) and Root Mean Square Error (RMSE). Additionally, ablation studies indicate that applying the MoE architecture to weather forecasting offers significant advantages in improving accuracy and resource efficiency.",
    "link": "/venue/31997@AAAI",
    "published_date": null,
    "conference": "AAAI.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "FoldToken: Learning Protein Language via Vector Quantization and Beyond",
    "authors": [
      "Zhangyang Gao",
      "Cheng Tan",
      "Jue Wang",
      "Yufei Huang",
      "Lirong Wu",
      "Stan Z. Li"
    ],
    "affiliations": [],
    "summary": "Is there a foreign language describing protein sequences and structures simultaneously? Protein structures, represented by continuous 3D points, have long posed a challenge due to the contrasting modeling paradigms of discrete sequences. We introduce FoldTokenizer to represent protein sequence-structure as discrete symbols. This approach involves projecting residue types and structures into a discrete space, guided by a reconstruction loss for information preservation. We name the learned discrete symbols as FoldToken, and the sequence of FoldTokens serves as a new protein language, transforming the protein sequence-structure into a unified modality. We apply the created protein language on general backbone inpainting task, building the first GPT-style model (FoldGPT) for sequence-structure co-generation with promising results. Key to our success is the substantial enhancement of the vector quantization module, Soft Conditional Vector Quantization (SoftCVQ).",
    "link": "/venue/31998@AAAI",
    "published_date": null,
    "conference": "AAAI.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "How Do Large Language Monkeys Get Their Power (Laws)?",
    "authors": [
      "Rylan Schaeffer",
      "Joshua Kazdan",
      "John Hughes",
      "Jordan Juravsky",
      "Sara Price",
      "Aengus Lynch",
      "Erik Jones",
      "Robert Kirk",
      "Azalia Mirhoseini",
      "Sanmi Koyejo"
    ],
    "affiliations": [],
    "summary": "Recent research across mathematical problem solving, proof assistant programming and multimodal jailbreaking documents a striking finding: when (multimodal) language model tackle a suite of tasks with multiple attempts per task -- succeeding if any attempt is correct -- then the negative log of the average success rate scales a power law in the number of attempts.In this work, we identify an apparent puzzle: a simple mathematical calculation predicts that on each problem, the failure rate should fall exponentially with the number of attempts.We confirm this prediction empirically, raising a question: from where does aggregate polynomial scaling emerge?We then answer this question by demonstrating per-problem exponential scaling can be made consistent with aggregate polynomial scaling if the distribution of single-attempt success probabilities is heavy tailed such that a small fraction of tasks with extremely low success probabilities collectively warp the aggregate success trend into a power law - even as each problem scales exponentially on its own.We further demonstrate that this distributional perspective explains previously observed deviations from power law scaling, and provides a simple method for forecasting the power law exponent with an order of magnitude lower relative error, or equivalently, ${\\sim}2-4$ orders of magnitude less inference compute.Overall, our work contributes to a better understanding of how neural language model performance improves with scaling inference compute and the development of scaling-predictable evaluations of (multimodal) language models.",
    "link": "/venue/QqVZ28qems@OpenReview",
    "published_date": null,
    "conference": "ICML.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "Layer by Layer: Uncovering Hidden Representations in Language Models",
    "authors": [
      "Oscar Skean",
      "Md Rifat Arefin",
      "Dan Zhao",
      "Niket Patel",
      "Jalal Naghiyev",
      "Yann LeCun",
      "Ravid Shwartz-Ziv"
    ],
    "affiliations": [],
    "summary": "From extracting features to generating text, the outputs of large language models (LLMs) typically rely on their final layers, following the conventional wisdom that earlier layers capture only low-level cues. However, our analysis shows that intermediate layers can encode even richer representations, often improving performance on a wide range of downstream tasks. To explain and quantify these hidden-layer properties, we propose a unified framework of representation quality metrics based on information theory, geometry, and invariance to input perturbations. Our framework highlights how each model layer balances information compression and signal preservation, revealing why mid-depth embeddings can exceed the last layer’s performance. Through extensive experiments on 32 text-embedding tasks across various architectures (transformers, state-space models) and domains (language, vision), we demonstrate that intermediate layers consistently provide stronger features, challenging the standard view on final-layer embeddings and opening new directions on using mid-layer representations for more robust and accurate representations.",
    "link": "/venue/WGXb7UdvTX@OpenReview",
    "published_date": null,
    "conference": "ICML.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "An Online Adaptive Sampling Algorithm for Stochastic Difference-of-convex Optimization with Time-varying Distributions",
    "authors": [
      "Yuhan Ye",
      "Ying Cui",
      "Jingyi Wang"
    ],
    "affiliations": [],
    "summary": "We propose an online adaptive sampling algorithm for solving stochastic nonsmooth difference-of-convex (DC) problems under time-varying distributions. At each iteration, the algorithm relies solely on data generated from the current distribution and employs distinct adaptive sampling rates for the convex and concave components of the DC function, a novel design guided by our theoretical analysis. We show that, under proper conditions on the convergence of distributions, the algorithm converges subsequentially to DC critical points almost surely. Furthermore, the sample size requirement of our proposed algorithm matches the results achieved in the smooth case or when a measurable subgradient selector is available, both under static distributions. A key element of this analysis is the derivation of a novel $O(\\sqrt{p/n})$ pointwise convergence rate (modulo logarithmic factors) for the sample average approximation of subdifferential mappings, where $p$ is the dimension of the variable and $n$ is the sample size -- a result of independent interest. Numerical experiments confirm that the proposed algorithm is both efficient and effective for addressing stochastic nonsmooth problems.",
    "link": "/venue/QmIzUuspWo@OpenReview",
    "published_date": null,
    "conference": "ICML.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "On Path to Multimodal Generalist: General-Level and General-Bench",
    "authors": [
      "Hao Fei",
      "Yuan Zhou",
      "Juncheng Li",
      "Xiangtai Li",
      "Qingshan Xu",
      "Bobo Li",
      "Shengqiong Wu",
      "Yaoting Wang",
      "Junbao Zhou",
      "Jiahao Meng",
      "Qingyu Shi",
      "Zhiyuan Zhou",
      "Liangtao Shi",
      "Minghe Gao",
      "Daoan Zhang",
      "Zhiqi Ge",
      "Siliang Tang",
      "Kaihang Pan",
      "Yaobo Ye",
      "Haobo Yuan",
      "Tao Zhang",
      "Weiming Wu",
      "Tianjie Ju",
      "Zixiang Meng",
      "Shilin Xu",
      "Liyu Jia",
      "Wentao Hu",
      "Meng Luo",
      "Jiebo Luo",
      "Tat-Seng Chua",
      "Shuicheng YAN",
      "Hanwang Zhang"
    ],
    "affiliations": [],
    "summary": "The Multimodal Large Language Model (MLLM) is currently experiencing rapid growth, driven by the advanced capabilities of language-based LLMs. Unlike their specialist predecessors, existing MLLMs are evolving towards a Multimodal Generalist paradigm. Initially limited to understanding multiple modalities, these models have advanced to not only comprehend but also generate across modalities. Their capabilities have expanded from coarse-grained to fine-grained multimodal understanding and from supporting singular modalities to accommodating a wide array of or even arbitrary modalities. To assess the capabilities of various MLLMs, a diverse array of benchmark test sets has been proposed. This leads to a critical question: *Can we simply assume that higher performance across tasks indicates a stronger MLLM capability, bringing us closer to human-level AI?*We argue that the answer is not as straightforward as it seems. In this project, we introduce an evaluation framework to delineate the capabilities and behaviors of current multimodal generalists. This framework, named **General-Level**, establishes 5-scale levels of MLLM performance and generality, offering a methodology to compare MLLMs and gauge the progress of existing systems towards more robust multimodal generalists and, ultimately, towards AGI (Artificial General Intelligence). Central to our framework is the use of **Synergy** as the evaluative criterion, categorizing capabilities based on whether MLLMs preserve synergy across comprehension and generation, as well as across multimodal interactions.To evaluate the comprehensive abilities of various generalists, we present a massive multimodal benchmark, **General-Bench**, which encompasses a broader spectrum of skills, modalities, formats, and capabilities, including over 700 tasks and 325,800 instances. The evaluation results that involve over 100 existing state-of-the-art MLLMs uncover the capability rankings of generalists, highlighting the challenges in reaching genuine AI. We expect this project to pave the way for future research on next-generation multimodal foundation models, providing a robust infrastructure to accelerate the realization of AGI.Project Page: https://generalist.top/,Leaderboard: https://generalist.top/leaderboard/,Benchmark: https://huggingface.co/General-Level/.",
    "link": "/venue/VsJ1K2HV3k@OpenReview",
    "published_date": null,
    "conference": "ICML.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "All-Purpose Mean Estimation over R: Optimal Sub-Gaussianity with Outlier Robustness and Low Moments Performance",
    "authors": [
      "Jasper Lee",
      "Walter McKelvie",
      "Maoyuan Song",
      "Paul Valiant"
    ],
    "affiliations": [],
    "summary": "We consider the basic statistical challenge of designing an \"all-purpose\" mean estimation algorithm that is recommendable across a variety of settings and models.Recent work by [Lee and Valiant 2022] introduced the first 1-d mean estimator whose error in the standard finite-variance+i.i.d. setting is optimal even in its constant factors; experimental demonstration of its good performance was shown by [Gobet et al. 2022].Yet, unlike for classic (but not necessarily practical) estimators such as median-of-means and trimmed mean, this new algorithm lacked proven robustness guarantees in other settings, including the settings of adversarial data corruption and heavy-tailed distributions with infinite variance.Such robustness is important for practical use cases.This raises a research question: is it possible to have a mean estimator that is robust, *without* sacrificing provably optimal performance in the standard i.i.d. setting?In this work, we show that Lee and Valiant's estimator is in fact an \"all-purpose\" mean estimator by proving:(A) It is robust to an $\\eta$-fraction of data corruption, even in the strong contamination model; it has optimal estimation error $O(\\sigma\\sqrt{\\eta})$ for distributions with variance $\\sigma^2$.(B) For distributions with finite $z^\\text{th}$ moment, for $z \\in (1,2)$, it has optimal estimation error, matching the lower bounds of [Devroye et al. 2016] up to constants.We further show (C) that outlier robustness for 1-d mean estimators in fact implies neighborhood optimality, a notion of beyond worst-case and distribution-dependent optimality recently introduced by [Dang et al. 2023].Previously, such an optimality guarantee was only known for median-of-means, but now it holds also for all estimators that are simultaneously *robust* and *sub-Gaussian*, including Lee and Valiant's, resolving a question raised by Dang et al.Lastly, we show (D) the asymptotic normality and efficiency of Lee and Valiant's estimator, as further evidence for its performance across many settings.",
    "link": "/venue/qR7YsQdFxV@OpenReview",
    "published_date": null,
    "conference": "ICML.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "Position: The AI Conference Peer Review Crisis Demands Author Feedback and Reviewer Rewards",
    "authors": [
      "Jaeho Kim",
      "Yunseok Lee",
      "Seulki Lee"
    ],
    "affiliations": [],
    "summary": "The peer review process in major artificial intelligence (AI) conferences faces unprecedented challenges with the surge of paper submissions (exceeding 10,000 submissions per venue), accompanied by growing concerns over review quality and reviewer responsibility. This position paper argues for **the need to transform the traditional one-way review system into a bi-directional feedback loop where authors evaluate review quality and reviewers earn formal accreditation, creating an accountability framework that promotes a sustainable, high-quality peer review system.** The current review system can be viewed as an interaction between three parties: the authors, reviewers, and system (i.e., conference), where we posit that all three parties share responsibility for the current problems. However, issues with authors can only be addressed through policy enforcement and detection tools, and ethical concerns can only be corrected through self-reflection. As such, this paper focuses on reforming reviewer accountability with systematic rewards through two key mechanisms: (1) a two-stage bi-directional review system that allows authors to evaluate reviews while minimizing retaliatory behavior, (2) a systematic reviewer reward system that incentivizes quality reviewing. We ask for the community's strong interest in these problems and the reforms that are needed to enhance the peer review process.",
    "link": "/venue/l8QemUZaIA@OpenReview",
    "published_date": null,
    "conference": "ICML.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "Improving the Scaling Laws of Synthetic Data with Deliberate Practice",
    "authors": [
      "Reyhane Askari Hemmat",
      "Mohammad Pezeshki",
      "Elvis Dohmatob",
      "Florian Bordes",
      "Pietro Astolfi",
      "Melissa Hall",
      "Jakob Verbeek",
      "Michal Drozdzal",
      "Adriana Romero-Soriano"
    ],
    "affiliations": [],
    "summary": "Inspired by the principle of deliberate practice in human learning, we propose Deliberate Practice for Synthetic Data Generation (DP), a novel framework that improves sample efficiency through dynamic synthetic data generation. Prior work has shown that scaling synthetic data is inherently challenging, as naively adding new data leads to diminishing returns. To address this, pruning has been identified as a key mechanism for improving scaling, enabling models to focus on the most informative synthetic samples. Rather than generating a large dataset and pruning it afterward, DP efficiently approximates the direct generation of informative samples. We theoretically show how training on challenging, informative examples improves scaling laws and empirically validate that DP achieves better scaling performance with significantly fewer training samples and iterations. On ImageNet-100, DP generates 3.4x fewer samples and requires six times fewer iterations, while on ImageNet-1k, it generates 8x fewer samples with a 30% reduction in iterations, all while achieving superior performance compared to prior work.",
    "link": "/venue/0LZRtvK871@OpenReview",
    "published_date": null,
    "conference": "ICML.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "The Value of Prediction in Identifying the Worst-Off",
    "authors": [
      "Unai Fischer Abaigar",
      "Christoph Kern",
      "Juan Perdomo"
    ],
    "affiliations": [],
    "summary": "Machine learning is increasingly used in government programs to identify and support the most vulnerable individuals, prioritizing assistance for those at greatest risk over optimizing aggregate outcomes. This paper examines the welfare impacts of prediction in equity-driven contexts, and how they compare to other policy levers, such as expanding bureaucratic capacity. Through mathematical models and a real-world case study on long-term unemployment amongst German residents, we develop a comprehensive understanding of the relative effectiveness of prediction in surfacing the worst-off. Our findings provide clear analytical frameworks and practical, data-driven tools that empower policymakers to make principled decisions when designing these systems.",
    "link": "/venue/26JsumCG0z@OpenReview",
    "published_date": null,
    "conference": "ICML.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "Normalizing Flows are Capable Generative Models",
    "authors": [
      "Shuangfei Zhai",
      "Ruixiang Zhang",
      "Preetum Nakkiran",
      "David Berthelot",
      "Jiatao Gu",
      "Huangjie Zheng",
      "Tianrong Chen",
      "Miguel Angel Bautista Martin",
      "Navdeep Jaitly",
      "Joshua M Susskind"
    ],
    "affiliations": [],
    "summary": "Normalizing Flows (NFs) are likelihood-based models for continuous inputs. They have demonstrated promising results on both density estimation and generative modeling tasks, but have received relatively little attention in recent years. In this work, we demonstrate that NFs are more powerful than previously believed. We present TarFlow: a simple and scalable architecture that enables highly performant NF models. TarFlow can be thought of as a Transformer-based variant of Masked Autoregressive Flows (MAFs): it consists of a stack of autoregressive Transformer blocks on image patches, alternating the autoregression direction between layers. TarFlow is straightforward to train end-to-end, and capable of directly modeling and generating pixels. We also propose three key techniques to improve sample quality: Gaussian noise augmentation during training, a post training denoising procedure, and an effective guidance method for both class-conditional and unconditional settings. Putting these together, TarFlow sets new state-of-the-art results on likelihood estimation for images, beating the previous best methods by a large margin, and generates samples with quality and diversity comparable to diffusion models, for the first time with a stand-alone NF model. We make our code available at https://github.com/apple/ml-tarflow.",
    "link": "/venue/2uheUFcFsM@OpenReview",
    "published_date": null,
    "conference": "ICML.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "Emergence in non-neural models: grokking modular arithmetic via average gradient outer product",
    "authors": [
      "Neil Mallinar",
      "Daniel Beaglehole",
      "Libin Zhu",
      "Adityanarayanan Radhakrishnan",
      "Parthe Pandit",
      "Misha Belkin"
    ],
    "affiliations": [],
    "summary": "Neural networks trained to solve modular arithmetic tasks exhibit grokking, a phenomenon where the test accuracy starts improving long after the model achieves 100% training accuracy in the training process. It is often taken as an example of \"emergence\", where model ability manifests sharply through a phase transition. In this work, we show that the phenomenon of grokking is not specific to neural networks nor to gradient descent-based optimization. Specifically, we show that this phenomenon occurs when learning modular arithmetic with Recursive Feature Machines (RFM), an iterative algorithm that uses the Average Gradient Outer Product (AGOP) to enable task-specific feature learning with general machine learning models. When used in conjunction with kernel machines, iterating RFM results in a fast transition from random, near zero, test accuracy to perfect test accuracy. This transition cannot be predicted from the training loss, which is identically zero, nor from the test loss, which remains constant in initial iterations. Instead, as we show, the transition is completely determined by feature learning: RFM gradually learns block-circulant features to solve modular arithmetic. Paralleling the results for RFM, we show that neural networks that solve modular arithmetic also learn block-circulant features. Furthermore, we present theoretical evidence that RFM uses such block-circulant features to implement the Fourier Multiplication Algorithm, which prior work posited as the generalizing solution neural networks learn on these tasks. Our results demonstrate that emergence can result purely from learning task-relevant features and is not specific to neural architectures nor gradient descent-based optimization methods. Furthermore, our work provides more evidence for AGOP as a key mechanism for feature learning in neural networks.",
    "link": "/venue/36hVB7DEB0@OpenReview",
    "published_date": null,
    "conference": "ICML.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "Algorithm Development in Neural Networks: Insights from the Streaming Parity Task",
    "authors": [
      "Loek van Rossem",
      "Andrew Saxe"
    ],
    "affiliations": [],
    "summary": "Even when massively overparameterized, deep neural networks show a remarkable ability to generalize. Research on this phenomenon has focused on generalization within distribution, via smooth interpolation. Yet in some settings neural networks also learn to extrapolate to data far beyond the bounds of the original training set, sometimes even allowing for infinite generalization, implying that an algorithm capable of solving the task has been learned. Here we undertake a case study of the learning dynamics of recurrent neural networks trained on the streaming parity task in order to develop an effective theory of algorithm development. The streaming parity task is a simple but nonlinear task defined on sequences up to arbitrary length. We show that, with sufficient finite training experience, RNNs exhibit a phase transition to perfect infinite generalization. Using an effective theory for the representational dynamics, we find an implicit representational merger effect which can be interpreted as the construction of a finite automaton that reproduces the task. Overall, our results disclose one mechanism by which neural networks can generalize infinitely from finite training experience.",
    "link": "/venue/3go0lhfxd0@OpenReview",
    "published_date": null,
    "conference": "ICML.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "What Limits Virtual Agent Application? OmniBench: A Scalable Multi-Dimensional Benchmark for Essential Virtual Agent Capabilities",
    "authors": [
      "Wendong Bu",
      "Yang Wu",
      "Qifan Yu",
      "Minghe Gao",
      "Bingchen Miao",
      "Zhenkui Zhang",
      "Kaihang Pan",
      "liyunfei",
      "Mengze Li",
      "Wei Ji",
      "Juncheng Li",
      "Siliang Tang",
      "Yueting Zhuang"
    ],
    "affiliations": [],
    "summary": "As multimodal large language models (MLLMs) advance, MLLM-based virtual agents have demonstrated remarkable performance. However, existing benchmarks face significant limitations, including uncontrollable task complexity, extensive manual annotation, and a lack of multidimensional evaluation. In response to these challenges, we introduce OmniBench, a self-generating, graph-based benchmark with an automated pipeline for synthesizing tasks of controllable complexity through subtask composition. To evaluate the diverse capabilities of virtual agents on the graph, we further present OmniEval, a multidimensional evaluation framework that includes subtask-level evaluation, graph-based metrics, and comprehensive tests across 10 capabilities. Our synthesized dataset contains 36k graph-structured tasks across 20 scenarios, achieving a 91% human acceptance rate. Training on our graph-structured data shows that it improves generalization across environments. We conduct multidimensional evaluations for virtual agents, revealing their performance across various capabilities and paving the way for future advancements. Our project is available at https://omni-bench.github.io.",
    "link": "/venue/4tFSKOY2mT@OpenReview",
    "published_date": null,
    "conference": "ICML.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "rStar-Math: Small LLMs Can Master Math Reasoning with Self-Evolved Deep Thinking",
    "authors": [
      "Xinyu Guan",
      "Li Lyna Zhang",
      "Yifei Liu",
      "Ning Shang",
      "Youran Sun",
      "Yi Zhu",
      "Fan Yang",
      "Mao Yang"
    ],
    "affiliations": [],
    "summary": "We present rStar-Math to demonstrate that small language models (SLMs) can rival or even surpass the math reasoning capability of OpenAI o1, without distillation from superior models. rStar-Math achieves this by exercising ``deep thinking'' through Monte Carlo Tree Search (MCTS), where a math policy SLM performs test-time search guided by an SLM-based process reward model. rStar-Math introduces three innovations to tackle the challenges in training the two SLMs: (1) a novel code-augmented CoT data synthesis method, which performs extensive MCTS rollouts to generate step-by-step verified reasoning trajectories used to train the policy SLM; (2) a novel process reward model training method that avoids naïve step-level score annotation, yielding a more effective process preference model (PPM); (3) a self-evolution recipe in which the policy SLM and PPM are built from scratch and iteratively evolved to improve reasoning capabilities. Through 4 rounds of self-evolution with millions of synthesized solutions for 747k math problems, rStar-Math boosts SLMs' math reasoning to state-of-the-art levels. On MATH benchmark, it improves Qwen2.5-Math-7B from 58.8% to 90.0%, surpassing o1-preview by +4.5%. On the USA Math Olympiad (AIME), rStar-Math solves an average of 53.3% (8/15) of problems, ranking among the top 20% of the brightest high school math students. Code and data are available at https://github.com/microsoft/rStar.",
    "link": "/venue/5zwF1GizFa@OpenReview",
    "published_date": null,
    "conference": "ICML.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "Theoretical Limitations of Ensembles in the Age of Overparameterization",
    "authors": [
      "Niclas Dern",
      "John Cunningham",
      "Geoff Pleiss"
    ],
    "affiliations": [],
    "summary": "Classic ensembles generalize better than any single component model. In contrast, recent empirical studies find that modern ensembles of (overparameterized) neural networks may not provide any inherent generalization advantage over single but larger neural networks. This paper clarifies how modern overparameterized ensembles differ from their classic underparameterized counterparts, using ensembles of random feature (RF) regressors as a basis for developing theory. In contrast to the underparameterized regime, where ensembling typically induces regularization and increases generalization, we prove with minimal assumptions that infinite ensembles of overparameterized RF regressors become pointwise equivalent to (single) infinite-width RF regressors, and finite width ensembles rapidly converge to single models with the same parameter budget. These results, which are exact for ridgeless models and approximate for small ridge penalties, imply that overparameterized ensembles and single large models exhibit nearly identical generalization. We further characterize the predictive variance amongst ensemble members, demonstrating that it quantifies the expected effects of increasing capacity rather than capturing any conventional notion of uncertainty. Our results challenge common assumptions about the advantages of ensembles in overparameterized settings, prompting a reconsideration of how well intuitions from underparameterized ensembles transfer to deep ensembles and the overparameterized regime.",
    "link": "/venue/Cf0N07E1vu@OpenReview",
    "published_date": null,
    "conference": "ICML.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "EmbodiedBench: Comprehensive Benchmarking Multi-modal Large Language Models for Vision-Driven Embodied Agents",
    "authors": [
      "Rui Yang",
      "Hanyang(Jeremy) Chen",
      "Junyu Zhang",
      "Mark Zhao",
      "Cheng Qian",
      "Kangrui Wang",
      "Qineng Wang",
      "Teja Koripella",
      "Marziyeh Movahedi",
      "Manling Li",
      "Heng Ji",
      "Huan Zhang",
      "Tong Zhang"
    ],
    "affiliations": [],
    "summary": "Leveraging Multi-modal Large Language Models (MLLMs) to create embodied agents offers a promising avenue for tackling real-world tasks. While language-centric embodied agents have garnered substantial attention, MLLM-based embodied agents remain underexplored due to the lack of comprehensive evaluation frameworks. To bridge this gap, we introduce EmbodiedBench, an extensive benchmark designed to evaluate vision-driven embodied agents.EmbodiedBench features: (1) a diverse set of 1,128 testing tasks across four environments, ranging from high-level semantic tasks (e.g., household) to low-level tasks involving atomic actions (e.g., navigation and manipulation); and (2) six meticulously curated subsets evaluating essential agent capabilities like commonsense reasoning, complex instruction understanding, spatial awareness, visual perception, and long-term planning.Through extensive experiments, we evaluated 24 leading proprietary and open-source MLLMs within EmbodiedBench. Our findings reveal that: MLLMs excel at high-level tasks but struggle with low-level manipulation, with the best model, GPT-4o, scoring only $28.9\\%$ on average. EmbodiedBench provides a multifaceted standardized evaluation platform that not only highlights existing challenges but also offers valuable insights to advance MLLM-based embodied agents. Our code and dataset are available at [https://embodiedbench.github.io](https://embodiedbench.github.io).",
    "link": "/venue/DgGF2LEBPS@OpenReview",
    "published_date": null,
    "conference": "ICML.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "Train for the Worst, Plan for the Best: Understanding Token Ordering in Masked Diffusions",
    "authors": [
      "Jaeyeon Kim",
      "Kulin Shah",
      "Vasilis Kontonis",
      "Sham Kakade",
      "Sitan Chen"
    ],
    "affiliations": [],
    "summary": "In recent years, masked diffusion models (MDMs) have emerged as a promising alternative approach for generative modeling over discrete domains. Compared to autoregressive models (ARMs), MDMs trade off complexity at training time with flexibility at inference time. At training time, they must learn to solve an exponentially large number of infilling problems, but at inference time, they can decode tokens in essentially arbitrary order. In this work we closely examine these two competing effects. On the training front, we theoretically and empirically demonstrate that MDMs indeed train on computationally intractable subproblems compared to their autoregressive counterparts. On the inference front, we show that a suitable strategy for adaptively choosing the token decoding order significantly enhances the capabilities of MDMs, allowing them to sidestep hard subproblems. On logic puzzles like Sudoku, we show that adaptive inference can boost solving accuracy in pretrained MDMs from $<7$\\% to $\\approx 90$\\%, even outperforming ARMs that were explicitly trained via teacher forcing to learn the right order of decoding.",
    "link": "/venue/DjJmre5IkP@OpenReview",
    "published_date": null,
    "conference": "ICML.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "CollabLLM: From Passive Responders to Active Collaborators",
    "authors": [
      "Shirley Wu",
      "Michel Galley",
      "Baolin Peng",
      "Hao Cheng",
      "Gavin Li",
      "Yao Dou",
      "Weixin Cai",
      "James Zou",
      "Jure Leskovec",
      "Jianfeng Gao"
    ],
    "affiliations": [],
    "summary": "Large Language Models are typically trained with next-turn rewards, limiting their ability to optimize for long-term interaction. As a result, they often respond passively to ambiguous or open-ended user requests, failing to help users reach their ultimate intents and leading to inefficient conversations. To address these limitations, we introduce CollabLLM, a novel and general training framework that enhances multiturn human-LLM collaboration. Its key innovation is a collaborative simulation that estimates the long-term contribution of responsesusing Multiturn-aware Rewards. By reinforcement fine-tuning these rewards, CollabLLM goes beyond responding to user requests, and actively uncovers user intent and offers insightful suggestions—a key step towards more human-centered AI. We also devise a multiturn interaction benchmark with three challenging tasks such as document creation. CollabLLM significantly outperforms our baselines with averages of 18.5% higher task performance and 46.3% improved interactivity by LLM judges. Finally, we conduct a large user study with 201 judges, where CollabLLM increases user satisfaction by 17.6% and reduces user spent time by 10.4%.",
    "link": "/venue/DmH4HHVb3y@OpenReview",
    "published_date": null,
    "conference": "ICML.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "Generative Social Choice: The Next Generation",
    "authors": [
      "Niclas Boehmer",
      "Sara Fish",
      "Ariel Procaccia"
    ],
    "affiliations": [],
    "summary": "A key task in certain democratic processes is to produce a concise slate of statements that proportionally represents the full spectrum of user opinions. This task is similar to committee elections, but unlike traditional settings, the candidate set comprises all possible statements of varying lengths, and so it can only be accessed through specific queries. Combining social choice and large language models, prior work has approached this challenge through a framework of generative social choice. We extend the framework in two fundamental ways, providing theoretical guarantees even in the face of approximately optimal queries and a budget limit on the overall length of the slate. Using GPT-4o to implement queries, we showcase our approach on datasets related to city improvement measures and drug reviews, demonstrating its effectiveness in generating representative slates from unstructured user opinions.",
    "link": "/venue/E1E6T7KHlR@OpenReview",
    "published_date": null,
    "conference": "ICML.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "Hierarchical Refinement: Optimal Transport to Infinity and Beyond",
    "authors": [
      "Peter Halmos",
      "Julian Gold",
      "Xinhao Liu",
      "Benjamin Raphael"
    ],
    "affiliations": [],
    "summary": "Optimal transport (OT) has enjoyed great success in machine learning as a principled way to align datasets via a least-cost correspondence, driven in large part by the runtime efficiency of the Sinkhorn algorithm (Cuturi, 2013). However, Sinkhorn has quadratic space complexity in the number of points, limiting scalability to larger datasets. Low-rank OT achieves linear-space complexity, but by definition, cannot compute a one-to-one correspondence between points. When the optimal transport problem is an assignment problem between datasets then an optimal mapping, known as the _Monge map_, is guaranteed to be a bijection. In this setting, we show that the factors of an optimal low-rank coupling co-cluster each point with its image under the Monge map. We leverage this invariant to derive an algorithm, _Hierarchical Refinement_ (`HiRef`), that dynamically constructs a multiscale partition of each dataset using low-rank OT subproblems, culminating in a bijective coupling. Hierarchical Refinement uses linear space and has log-linear runtime, retaining the space advantage of low-rank OT while overcoming its limited resolution. We demonstrate the advantages of Hierarchical Refinement on several datasets, including ones containing over a million points, scaling full-rank OT to problems previously beyond Sinkhorn's reach.",
    "link": "/venue/EBNgREMoVD@OpenReview",
    "published_date": null,
    "conference": "ICML.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "SK-VQA: Synthetic Knowledge Generation at Scale for Training Context-Augmented Multimodal LLMs",
    "authors": [
      "Xin Su",
      "Man Luo",
      "Kris Pan",
      "Tien Pei Chou",
      "Vasudev Lal",
      "Phillip Howard"
    ],
    "affiliations": [],
    "summary": "Multimodal retrieval-augmented generation (RAG) plays a crucial role in domains such as knowledge-based visual question answering (KB-VQA), where models should effectively integrate additional knowledge to generate a response. However, existing vision and language models (VLMs) are not inherently designed for context-augmented generation, limiting their effectiveness in such tasks. While synthetic data generation has recently gained attention for training large VLMs, its application for context-augmented generation remains underexplored. To address this gap, we introduce SKVQA, a large-scale synthetic multimodal dataset containing over 2 million visual question-answer pairs, each associated with external knowledge sources to determine the final answer. Compared to previous datasets, SKVQA exhibits 11× more unique questions, greater domain diversity, and a broader spectrum of image sources. Through human evaluations, we confirm the high quality of the generated question-answer pairs and their contextual relevance. Extensive experiments show that SKVQA serves both as a challenging benchmark for knowledge-based VQA and as an effective training resource for adapting generative multimodal models to context-augmented generation. Our results further indicate that models trained on SKVQA demonstrate enhanced generalization in both context-aware VQA and multimodal RAG settings.",
    "link": "/venue/EVwMw2lVlw@OpenReview",
    "published_date": null,
    "conference": "ICML.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "In-Context Denoising with One-Layer Transformers: Connections between Attention and Associative Memory Retrieval",
    "authors": [
      "Matthew Smart",
      "Alberto Bietti",
      "Anirvan Sengupta"
    ],
    "affiliations": [],
    "summary": "We introduce in-context denoising, a task that refines the connection between attention-based architectures and dense associative memory (DAM) networks, also known as modern Hopfield networks. Using a Bayesian framework, we show theoretically and empirically that certain restricted denoising problems can be solved optimally even by a single-layer transformer. We demonstrate that a trained attention layer processes each denoising prompt by performing a single gradient descent update on a context-aware DAM energy landscape, where context tokens serve as associative memories and the query token acts as an initial state. This one-step update yields better solutions than exact retrieval of either a context token or a spurious local minimum, providing a concrete example of DAM networks extending beyond the standard retrieval paradigm. Overall, this work solidifies the link between associative memory and attention mechanisms first identified by Ramsauer et al., and demonstrates the relevance of associative memory models in the study of in-context learning.",
    "link": "/venue/F08lzoBgad@OpenReview",
    "published_date": null,
    "conference": "ICML.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "Scaling Collapse Reveals Universal Dynamics in Compute-Optimally Trained Neural Networks",
    "authors": [
      "Shikai Qiu",
      "Lechao Xiao",
      "Andrew Wilson",
      "Jeffrey Pennington",
      "Atish Agarwala"
    ],
    "affiliations": [],
    "summary": "Understanding neural network training dynamics at scale is an important open problem. Although realistic model architectures, optimizers, and data interact in complex ways that make predictive theory challenging, we show that compute-optimally trained models exhibit remarkably precise collective regularities. Specifically, loss curves from models of varying sizes collapse onto a single universal curve when training compute and loss are normalized to unity at the end of training. With learning rate decay, discrepancies between normalized curves fall below the noise floor of individual models' loss curves across random seeds, yielding an exceptionally tight collapse we term \"supercollapse.\" We observe supercollapse across learning rate schedules, datasets, and architectures, including transformers trained on next-token prediction. This collapse breaks down when hyperparameters are scaled suboptimally, providing a practical indicator of proper scaling. We explain these phenomena by connecting collapse to the power-law structure in typical neural scaling laws, and analyzing a simple but effective model of SGD noise dynamics that accurately captures how learning rate schedules deform loss curves away from power laws while preserving universality, and why learning rate decay suppresses variance to enable supercollapse.",
    "link": "/venue/Fvq9ogLnLN@OpenReview",
    "published_date": null,
    "conference": "ICML.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "Orthogonal Subspace Decomposition for Generalizable AI-Generated Image Detection",
    "authors": [
      "Zhiyuan Yan",
      "Jiangming Wang",
      "Peng Jin",
      "Ke-Yue Zhang",
      "Chengchun Liu",
      "Shen Chen",
      "Taiping Yao",
      "Shouhong Ding",
      "Baoyuan Wu",
      "Li Yuan"
    ],
    "affiliations": [],
    "summary": "Detecting AI-generated images (AIGIs), such as natural images or face images, has become increasingly important yet challenging. In this paper, we start from a new perspective to excavate the reason behind the failure generalization in AIGI detection, named the asymmetry phenomenon, where a naively trained detector tends to favor overfitting to the limited and monotonous fake patterns, causing the feature space to become highly constrained and low-ranked, which is proved seriously limiting the expressivity and generalization. One potential remedy is incorporating the pre-trained knowledge within the vision foundation models (higher-ranked) to expand the feature space, alleviating the model's overfitting to fake. To this end, we employ Singular Value Decomposition (SVD) to decompose the original feature space into two orthogonal subspaces. By freezing the principal components and adapting only the remained components, we preserve the pre-trained knowledge while learning fake patterns. Compared to existing full-parameters and LoRA-based tuning methods, we explicitly ensure orthogonality, enabling the higher rank of the whole feature space, effectively minimizing overfitting and enhancing generalization. We finally identify a crucial insight: our method implicitly learns a vital prior that fakes are actually derived from the real, indicating a hierarchical relationship rather than independence. Modeling this prior, we believe, is essential for achieving superior generalization. Our codes are publicly available at https://github.com/YZY-stack/Effort-AIGI-Detection.",
    "link": "/venue/GFpjO8S8Po@OpenReview",
    "published_date": null,
    "conference": "ICML.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "Flowing Datasets with Wasserstein over Wasserstein Gradient Flows",
    "authors": [
      "Clément Bonet",
      "Christophe Vauthier",
      "Anna Korba"
    ],
    "affiliations": [],
    "summary": "Many applications in machine learning involve data represented as probability distributions. The emergence of such data requires radically novel techniques to design tractable gradient flows on probability distributions over this type of (infinite-dimensional) objects. For instance, being able to flow labeled datasets is a core task for applications ranging from domain adaptation to transfer learning or dataset distillation. In this setting, we propose to represent each class by the associated conditional distribution of features, and to model the dataset as a mixture distribution supported on these classes (which are themselves probability distributions), meaning that labeled datasets can be seen as probability distributions over probability distributions. We endow this space with a metric structure from optimal transport, namely the Wasserstein over Wasserstein (WoW) distance, derive a differential structure on this space, and define WoW gradient flows. The latter enables to design dynamics over this space that decrease a given objective functional. We apply our framework to transfer learning and dataset distillation tasks, leveraging our gradient flow construction as well as novel tractable functionals that take the form of Maximum Mean Discrepancies with Sliced-Wasserstein based kernels between probability distributions.",
    "link": "/venue/I1OHPb4zWo@OpenReview",
    "published_date": null,
    "conference": "ICML.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "Learning dynamics in linear recurrent neural networks",
    "authors": [
      "Alexandra Proca",
      "Clémentine Dominé",
      "Murray Shanahan",
      "Pedro Mediano"
    ],
    "affiliations": [],
    "summary": "Recurrent neural networks (RNNs) are powerful models used widely in both machine learning and neuroscience to learn tasks with temporal dependencies and to model neural dynamics. However, despite significant advancements in the theory of RNNs, there is still limited understanding of their learning process and the impact of the temporal structure of data. Here, we bridge this gap by analyzing the learning dynamics of linear RNNs (LRNNs) analytically, enabled by a novel framework that accounts for task dynamics. Our mathematical result reveals four key properties of LRNNs: (1) Learning of data singular values is ordered by both scale and temporal precedence, such that singular values that are larger and occur later are learned faster. (2) Task dynamics impact solution stability and extrapolation ability. (3) The loss function contains an effective regularization term that incentivizes small weights and mediates a tradeoff between recurrent and feedforward computation. (4) Recurrence encourages feature learning, as shown through a novel derivation of the neural tangent kernel for finite-width LRNNs. As a final proof-of-concept, we apply our theoretical framework to explain the behavior of LRNNs performing sensory integration tasks. Our work provides a first analytical treatment of the relationship between the temporal dependencies in tasks and learning dynamics in LRNNs, building a foundation for understanding how complex dynamic behavior emerges in cognitive models.",
    "link": "/venue/KGOcrIWYnx@OpenReview",
    "published_date": null,
    "conference": "ICML.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "EcomScriptBench: A Multi-task Benchmark for E-commerce Script Planning via Step-wise Intention-Driven Product Association",
    "authors": [
      "Weiqi Wang",
      "Limeng Cui",
      "Xin Liu",
      "Sreyashi Nag",
      "Wenju Xu",
      "Chen Luo",
      "Sheikh Muhammad Sarwar",
      "Yang Li",
      "Hansu Gu",
      "Hui Liu",
      "Changlong Yu",
      "Jiaxin Bai",
      "Yifan Gao",
      "Haiyang Zhang",
      "Qi He",
      "Shuiwang Ji",
      "Yangqiu Song"
    ],
    "affiliations": [],
    "summary": "Goal-oriented script planning, or the ability to devise coherent sequences of actions toward specific goals, is commonly employed by humans to plan for typical activities. In e-commerce, customers increasingly seek LLM-based assistants to generate scripts and recommend products at each step, thereby facilitating convenient and efficient shopping experiences. However, this capability remains underexplored due to several challenges, including the inability of LLMs to simultaneously conduct script planning and product retrieval, difficulties in matching products caused by semantic discrepancies between planned actions and search queries, and a lack of methods and benchmark data for evaluation. In this paper, we step forward by formally defining the task of E-commerce Script Planning (EcomScript) as three sequential subtasks. We propose a novel framework that enables the scalable generation of product-enriched scripts by associating products with each step based on the semantic similarity between the actions and their purchase intentions. By applying our framework to real-world e-commerce data, we construct the very first large-scale EcomScript dataset, EcomScriptBench, which includes 605,229 scripts sourced from 2.4 million products. Human annotations are then conducted to provide gold labels for a sampled subset, forming an evaluation benchmark. Extensive experiments reveal that current (L)LMs face significant challenges with EcomScript tasks, even after fine-tuning, while injecting product purchase intentions improves their performance.",
    "link": "/venue/2025.acl-long.1@ACL",
    "published_date": null,
    "conference": "ACL.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "GraphNarrator: Generating Textual Explanations for Graph Neural Networks",
    "authors": [
      "Bo Pan",
      "Zhen Xiong",
      "Guanchen Wu",
      "Zheng Zhang",
      "Yifei Zhang",
      "Yuntong Hu",
      "Liang Zhao"
    ],
    "affiliations": [],
    "summary": "Graph representation learning has garnered significant attention due to its broad applications in various domains, such as recommendation systems and social network analysis. Despite advancements in graph learning methods, challenges still remain in explainability when graphs are associated with semantic features. In this paper, we present GraphNarrator, the first method designed to generate natural language explanations for Graph Neural Networks. GraphNarrator employs a generative language model that maps input-output pairs to explanations reflecting the model’s decision-making process. To address the lack of ground truth explanations to train the model, we propose first generating pseudo-labels that capture the model’s decisions from saliency-based explanations, then using Expert Iteration to iteratively train the pseudo-label generator based on training objectives on explanation quality. The high-quality pseudo-labels are finally utilized to train an end-to-end explanation generator model. Extensive experiments are conducted to demonstrate the effectiveness of GraphNarrator in producing faithful, concise, and human-preferred natural language explanations.",
    "link": "/venue/2025.acl-long.2@ACL",
    "published_date": null,
    "conference": "ACL.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "M-RewardBench: Evaluating Reward Models in Multilingual Settings",
    "authors": [
      "Srishti Gureja",
      "Lester James Validad Miranda",
      "Shayekh Bin Islam",
      "Rishabh Maheshwary",
      "Drishti Sharma",
      "Gusti Triandi Winata",
      "Nathan Lambert",
      "Sebastian Ruder",
      "Sara Hooker",
      "Marzieh Fadaee"
    ],
    "affiliations": [],
    "summary": "Reward models (RMs) have driven the state-of-the-art performance of LLMs today by enabling the integration of human feedback into the language modeling process. However, RMs are primarily trained and evaluated in English, and their capabilities in multilingual settings remain largely understudied. In this work, we conduct a systematic evaluation of several reward models in multilingual settings. We first construct the first-of-its-kind multilingual RM evaluation benchmark, M-RewardBench, consisting of 2.87k preference instances for 23 typologically diverse languages, that tests the chat, safety, reasoning, and translation capabilities of RMs. We then rigorously evaluate a wide range of reward models on M-RewardBench, offering fresh insights into their performance across diverse languages. We identify a significant gap in RMs’ performances between English and non-English languages and show that RM preferences can change substantially from one language to another. We also present several findings on how different multilingual aspects impact RM performance. Specifically, we show that the performance of RMs is improved with improved translation quality. Similarly, we demonstrate that the models exhibit better performance for high-resource languages. We release M-RewardBench dataset and the codebase in this study to facilitate a better understanding of RM evaluation in multilingual settings.",
    "link": "/venue/2025.acl-long.3@ACL",
    "published_date": null,
    "conference": "ACL.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "ELABORATION: A Comprehensive Benchmark on Human-LLM Competitive Programming",
    "authors": [
      "Xinwei Yang",
      "Zhaofeng Liu",
      "Chen Huang",
      "Jiashuai Zhang",
      "Tong Zhang",
      "Yifan Zhang",
      "Wenqiang Lei"
    ],
    "affiliations": [],
    "summary": "While recent research increasingly emphasizes the value of human-LLM collaboration in competitive programming and proposes numerous empirical methods, a comprehensive understanding remains elusive due to the fragmented nature of existing studies and their use of diverse, application-specific human feedback. Thus, our work serves a three-fold purpose: First, we present the first taxonomy of human feedback consolidating the entire programming process, which promotes fine-grained evaluation. Second, we introduce ELABORATIONSET, a novel programming dataset specifically designed for human-LLM collaboration, meticulously annotated to enable large-scale simulated human feedback and facilitate cost-effective real human interaction studies. Third, we introduce ELABORATION, a novel benchmark to facilitate a thorough assessment of human-LLM competitive programming. With ELABORATION, we pinpoint strengthes and weaknesses of existing methods, thereby setting the foundation for furture improvement. Our dataset and code will be openly released.",
    "link": "/venue/2025.acl-long.4@ACL",
    "published_date": null,
    "conference": "ACL.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "The Impossibility of Fair LLMs",
    "authors": [
      "Jacy Reese Anthis",
      "Kristian Lum",
      "Michael Ekstrand",
      "Avi Feller",
      "Chenhao Tan"
    ],
    "affiliations": [],
    "summary": "The rise of general-purpose artificial intelligence (AI) systems, particularly large language models (LLMs), has raised pressing moral questions about how to reduce bias and ensure fairness at scale. Researchers have documented a sort of “bias” in the significant correlations between demographics (e.g., race, gender) in LLM prompts and responses, but it remains unclear how LLM fairness could be evaluated with more rigorous definitions, such as group fairness or fair representations. We analyze a variety of technical fairness frameworks and find inherent challenges in each that make the development of a fair LLM intractable. We show that each framework either does not logically extend to the general-purpose AI context or is infeasible in practice, primarily due to the large amounts of unstructured training data and the many potential combinations of human populations, use cases, and sensitive attributes. These inherent challenges would persist for general-purpose AI, including LLMs, even if empirical challenges, such as limited participatory input and limited measurement methods, were overcome. Nonetheless, fairness will remain an important type of model evaluation, and there are still promising research directions, particularly the development of standards for the responsibility of LLM developers, context-specific evaluations, and methods of iterative, participatory, and AI-assisted evaluation that could scale fairness across the diverse contexts of modern human-AI interaction.",
    "link": "/venue/2025.acl-long.5@ACL",
    "published_date": null,
    "conference": "ACL.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "Intuitive Fine-Tuning: Towards Simplifying Alignment into a Single Process",
    "authors": [
      "Ermo Hua",
      "Biqing Qi",
      "Kaiyan Zhang",
      "Kai Tian",
      "Xingtai Lv",
      "Ning Ding",
      "Bowen Zhou"
    ],
    "affiliations": [],
    "summary": "Supervised Fine-Tuning (SFT) and Preference Optimization (PO) are key processes for aligning Language Models (LMs) with human preferences post pre-training. While SFT excels in efficiency and PO in effectiveness, they are often combined sequentially without integrating their optimization objectives. This approach ignores the opportunities to bridge their paradigm gap and take the strengths from both. In this paper, we interpret SFT and PO with two sub-processes — *Preference Estimation* and *Transition Optimization* — defined at token level within the Markov Decision Process (MDP). This modeling shows that SFT is only a special case of PO with inferior estimation and optimization. PO estimates the model’s preference by its entire generation, while SFT only scores model’s subsequent predicted tokens based on prior tokens from ground truth answer. These priors deviates from model’s distribution, hindering the preference estimation and transition optimization. Building on this view, we introduce **Intuitive Fine-Tuning (IFT)** to integrate SFT and PO into a single process. Through a temporal residual connection, IFT brings better estimation and optimization by capturing LMs’ intuitive sense of its entire answers. But it solely relies on a single policy and the same volume of non-preference-labeled data as SFT. Our experiments show that IFT performs comparably or even superiorly to SFT and some typical PO methods across several tasks, particularly those requires generation, reasoning, and fact-following abilities. An explainable Frozen Lake game further validates the effectiveness of IFT for getting competitive policy.",
    "link": "/venue/2025.acl-long.6@ACL",
    "published_date": null,
    "conference": "ACL.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "Bias in Language Models: Beyond Trick Tests and Towards RUTEd Evaluation",
    "authors": [
      "Kristian Lum",
      "Jacy Reese Anthis",
      "Kevin Robinson",
      "Chirag Nagpal",
      "Alexander Nicholas D’Amour"
    ],
    "affiliations": [],
    "summary": "Standard bias benchmarks used for large language models (LLMs) measure the association between social attributes in model inputs and single-word model outputs. We test whether these benchmarks are robust to lengthening the model outputs via a more realistic user prompt, in the commonly studied domain of gender-occupation bias, as a step towards measuring Realistic Use and Tangible Effects (i.e., RUTEd evaluations). From the current literature, we adapt three standard metrics of next-word prediction (neutrality, skew, and stereotype), and we develop analogous RUTEd evaluations in three contexts of real-world LLM use: children’s bedtime stories, user personas, and English language learning exercises. We find that standard bias metrics have no significant correlation with long-form output metrics. For example, selecting the least biased model based on the standard “trick tests” coincides with selecting the least biased model based on longer output no more than random chance. There may not yet be evidence to justify standard benchmarks as reliable proxies of real-world biases, and we encourage further development of context-specific RUTEd evaluations.",
    "link": "/venue/2025.acl-long.7@ACL",
    "published_date": null,
    "conference": "ACL.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "Sliding Windows Are Not the End: Exploring Full Ranking with Long-Context Large Language Models",
    "authors": [
      "Wenhan Liu",
      "Xinyu Ma",
      "Yutao Zhu",
      "Ziliang Zhao",
      "Shuaiqiang Wang",
      "Dawei Yin",
      "Zhicheng Dou"
    ],
    "affiliations": [],
    "summary": "Large Language Models (LLMs) have shown exciting performance in listwise passage ranking. Due to the limited input length, existing methods often adopt the sliding window strategy. Such a strategy, though effective, is inefficient as it involves repetitive and serialized processing, which usually re-evaluates relevant passages multiple times. As a result, it incurs redundant API costs, which are proportional to the number of inference tokens. The development of long-context LLMs enables the full ranking of all passages within a single inference, avoiding redundant API costs. In this paper, we conduct a comprehensive study of long-context LLMs for ranking tasks in terms of efficiency and effectiveness. Surprisingly, our experiments reveal that full ranking with long-context LLMs can deliver superior performance in the supervised fine-tuning setting with a huge efficiency improvement. Furthermore, we identify two limitations of fine-tuning the full ranking model based on existing methods: (1) sliding window strategy fails to produce a full ranking list as a training label, and (2) the language modeling loss cannot emphasize top-ranked passage IDs in the label. To alleviate these issues, we propose a new complete listwise label construction approach and a novel importance-aware learning objective for full ranking. Experiments show the superior performance of our method over baselines.",
    "link": "/venue/2025.acl-long.8@ACL",
    "published_date": null,
    "conference": "ACL.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "The Impact of Auxiliary Patient Data on Automated Chest X-Ray Report Generation and How to Incorporate It",
    "authors": [
      "Aaron Nicolson",
      "Shengyao Zhuang",
      "Jason Dowling",
      "Bevan Koopman"
    ],
    "affiliations": [],
    "summary": "This study investigates the integration of diverse patient data sources into multimodal language models for automated chest X-ray (CXR) report generation. Traditionally, CXR report generation relies solely on data from a patient’s CXR exam, overlooking valuable information from patient electronic health records. Utilising the MIMIC-CXR and MIMIC-IV-ED datasets, we investigate the use of patient data from emergency department (ED) records — such as vital signs measured and medicines reconciled during an ED stay — for CXR report generation, with the aim of enhancing diagnostic accuracy. We also investigate conditioning CXR report generation on the clinical history section of radiology reports, which has been overlooked in the literature. We introduce a novel approach to transform these heterogeneous data sources into patient data embeddings that prompt a multimodal language model (CXRMate-ED). Our comprehensive evaluation indicates that using a broader set of patient data significantly enhances diagnostic accuracy. The model, training code, and dataset are publicly available.",
    "link": "/venue/2025.acl-long.9@ACL",
    "published_date": null,
    "conference": "ACL.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "CLEME2.0: Towards Interpretable Evaluation by Disentangling Edits for Grammatical Error Correction",
    "authors": [
      "Jingheng Ye",
      "Zishan Xu",
      "Yinghui Li",
      "Linlin Song",
      "Qingyu Zhou",
      "Hai-Tao Zheng",
      "Ying Shen",
      "Wenhao Jiang",
      "Hong-Gee Kim",
      "Ruitong Liu",
      "Xin Su",
      "Zifei Shan"
    ],
    "affiliations": [],
    "summary": "The paper focuses on the interpretability of Grammatical Error Correction (GEC) evaluation metrics, which received little attention in previous studies. To bridge the gap, we introduce **CLEME2.0**, a reference-based metric describing four fundamental aspects of GEC systems: hit-correction, wrong-correction, under-correction, and over-correction. They collectively contribute to exposing critical qualities and locating drawbacks of GEC systems. Evaluating systems by combining these aspects also leads to superior human consistency over other reference-based and reference-less metrics. Extensive experiments on two human judgment datasets and six reference datasets demonstrate the effectiveness and robustness of our method, achieving a new state-of-the-art result. Our codes are released at https://github.com/THUKElab/CLEME.",
    "link": "/venue/2025.acl-long.10@ACL",
    "published_date": null,
    "conference": "ACL.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "StrucText-Eval: Evaluating Large Language Model’s Reasoning Ability in Structure-Rich Text",
    "authors": [
      "Zhouhong Gu",
      "Haoning Ye",
      "Xingzhou Chen",
      "Zeyang Zhou",
      "Hongwei Feng",
      "Yanghua Xiao"
    ],
    "affiliations": [],
    "summary": "The effective utilization of structured data, integral to corporate data strategies, has been challenged by the rise of large language models (LLMs) capable of processing unstructured information. This shift prompts the question: can LLMs interpret structured data directly in its unstructured form? We propose an automatic evaluation data generation method for assessing LLMs’ reasoning capabilities on structure-rich text to explore this. Our approach supports 8 structured languages and 29 tasks, generating data with adjustable complexity through controllable nesting and structural width. We introduce StrucText-Eval, a benchmark containing 5,800 pre-generated and annotated samples designed to evaluate how well LLMs understand and reason through structured text. StrucText-Eval is divided into two suites: a regular Test suite (3,712 samples) and a Test-Hard suite (2,088 samples), the latter emphasizing the gap between human and model performance on more complex tasks. Experimental results show that while open-source LLMs achieve a maximum accuracy of 74.9% on the standard dataset, their performance drops significantly to 45.8% on the harder dataset. In contrast, human participants reach an accuracy of 92.6% on StrucText-Eval-Hard, highlighting LLMs’ current limitations in handling intricate structural information.",
    "link": "/venue/2025.acl-long.11@ACL",
    "published_date": null,
    "conference": "ACL.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "Literature Meets Data: A Synergistic Approach to Hypothesis Generation",
    "authors": [
      "Haokun Liu",
      "Yangqiaoyu Zhou",
      "Mingxuan Li",
      "Chenfei Yuan",
      "Chenhao Tan"
    ],
    "affiliations": [],
    "summary": "AI holds promise for transforming scientific processes, including hypothesis generation. Prior work on hypothesis generation can be broadly categorized into theory-driven and data-driven approaches. While both have proven effective in generating novel and plausible hypotheses, it remains an open question whether they can complement each other. To address this, we develop the first method that combines literature-based insights with data to perform LLM-powered hypothesis generation. We apply our method on five different datasets and demonstrate that integrating literature and data outperforms other baselines (8.97% over few-shot, 15.75% over literature-based alone, and 3.37% over data-driven alone). Additionally, we conduct the first human evaluation to assess the utility of LLM-generated hypotheses in assisting human decision-making on two challenging tasks: deception detection and AI generated content detection. Our results show that human accuracy improves significantly by 7.44% and 14.19% on these tasks, respectively. These findings suggest that integrating literature-based and data-driven approaches provides a comprehensive and nuanced framework for hypothesis generation and could open new avenues for scientific inquiry.",
    "link": "/venue/2025.acl-long.12@ACL",
    "published_date": null,
    "conference": "ACL.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "GAPO: Learning Preferential Prompt through Generative Adversarial Policy Optimization",
    "authors": [
      "Zhouhong Gu",
      "Xingzhou Chen",
      "Xiaoran Shi",
      "Tao Wang",
      "Suhang Zheng",
      "Tianyu Li",
      "Hongwei Feng",
      "Yanghua Xiao"
    ],
    "affiliations": [],
    "summary": "Recent advances in large language models have highlighted the critical need for precise control over model outputs through predefined constraints. While existing methods attempt to achieve this through either direct instruction-response synthesis or preferential response optimization, they often struggle with constraint understanding and adaptation. This limitation becomes particularly evident when handling fine-grained constraints, leading to either hallucination or brittle performance. We introduce Generative Adversarial Policy Optimization (GAPO), a novel framework that combines GAN-based training dynamics with an encoder-only reward model to progressively learn and adapt to increasingly complex constraints. GAPO leverages adversarial training to automatically generate training samples of varying difficulty while utilizing the encoder-only architecture to better capture prompt-response relationships. Extensive experiments demonstrate GAPO’s superior performance across multiple benchmarks, particularly in scenarios requiring fine-grained constraint handling, where it significantly outperforms existing methods like PPO, DPO, and KTO. Our results suggest that GAPO’s unique approach to preferential prompt learning offers a more robust and effective solution for controlling LLM outputs.",
    "link": "/venue/2025.acl-long.13@ACL",
    "published_date": null,
    "conference": "ACL.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "Tree-of-Evolution: Tree-Structured Instruction Evolution for Code Generation in Large Language Models",
    "authors": [
      "Ziyang Luo",
      "Kaixin Li",
      "Hongzhan Lin",
      "Yuchen Tian",
      "Mohan Kankanhalli",
      "Jing Ma"
    ],
    "affiliations": [],
    "summary": "Data synthesis has become a crucial research area in large language models (LLMs), especially for generating high-quality instruction fine-tuning data to enhance downstream performance. In code generation, a key application of LLMs, manual annotation of code instruction data is costly. Recent methods, such as Code Evol-Instruct and OSS-Instruct, leverage LLMs to synthesize large-scale code instruction data, significantly improving LLM coding capabilities. However, these approaches face limitations due to unidirectional synthesis and randomness-driven generation, which restrict data quality and diversity. To overcome these challenges, we introduce Tree-of-Evolution (ToE), a novel framework that models code instruction synthesis process with a tree structure, exploring multiple evolutionary paths to alleviate the constraints of unidirectional generation. Additionally, we propose optimization-driven evolution, which refines each generation step based on the quality of the previous iteration. Experimental results across five widely-used coding benchmarks—HumanEval, MBPP, EvalPlus, LiveCodeBench, and BigCodeBench—demonstrate that base models fine-tuned on just 75k data synthesized by our method achieve comparable or superior performance to the state-of-the-art open-weight Code LLM, Qwen2.5-Coder-Instruct, which was fine-tuned on millions of samples.",
    "link": "/venue/2025.acl-long.14@ACL",
    "published_date": null,
    "conference": "ACL.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "Delving into Multilingual Ethical Bias: The MSQAD with Statistical Hypothesis Tests for Large Language Models",
    "authors": [
      "Seunguk Yu",
      "Juhwan Choi",
      "YoungBin Kim"
    ],
    "affiliations": [],
    "summary": "Despite the recent strides in large language models, studies have underscored the existence of social biases within these systems. In this paper, we delve into the validation and comparison of the ethical biases of LLMs concerning globally discussed and potentially sensitive topics, hypothesizing that these biases may arise from language-specific distinctions. Introducing the Multilingual Sensitive Questions & Answers Dataset (**MSQAD**), we collected news articles from Human Rights Watch covering 17 topics, and generated socially sensitive questions along with corresponding responses in multiple languages. We scrutinized the biases of these responses across languages and topics, employing two statistical hypothesis tests. The results showed that the null hypotheses were rejected in most cases, indicating biases arising from cross-language differences. It demonstrates that ethical biases in responses are widespread across various languages, and notably, these biases were prevalent even among different LLMs. By making the proposed MSQAD openly available, we aim to facilitate future research endeavors focused on examining cross-language biases in LLMs and their variant models.",
    "link": "/venue/2025.acl-long.15@ACL",
    "published_date": null,
    "conference": "ACL.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "ReSCORE: Label-free Iterative Retriever Training for Multi-hop Question Answering with Relevance-Consistency Supervision",
    "authors": [
      "Dosung Lee",
      "Wonjun Oh",
      "Boyoung Kim",
      "Minyoung Kim",
      "Joonsuk Park",
      "Paul Hongsuck Seo"
    ],
    "affiliations": [],
    "summary": "Multi-hop question answering (MHQA) involves reasoning across multiple documents to answer complex questions. Dense retrievers typically outperform sparse methods like BM25 by leveraging semantic embeddings in many tasks; however, they require labeled query-document pairs for fine-tuning, which poses a significant challenge in MHQA due to the complexity of the reasoning steps. To overcome this limitation, we introduce Retriever Supervision with Consistency and Relevance (ReSCORE), a novel method for training dense retrievers for MHQA without the need for labeled documents. ReSCORE leverages large language models to measure document-question relevance with answer consistency and utilizes this information to train a retriever within an iterative question-answering framework. Evaluated on three MHQA benchmarks, our extensive experiments demonstrate the effectiveness of ReSCORE, with significant improvements in retrieval performance that consequently lead to state-of-the-art Exact Match and F1 scores for MHQA.",
    "link": "/venue/2025.acl-long.16@ACL",
    "published_date": null,
    "conference": "ACL.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "FACT-AUDIT: An Adaptive Multi-Agent Framework for Dynamic Fact-Checking Evaluation of Large Language Models",
    "authors": [
      "Hongzhan Lin",
      "Yang Deng",
      "Yuxuan Gu",
      "Wenxuan Zhang",
      "Jing Ma",
      "See-Kiong Ng",
      "Tat-Seng Chua"
    ],
    "affiliations": [],
    "summary": "Large Language Models (LLMs) have significantly advanced the fact-checking studies. However, existing automated fact-checking evaluation methods rely on static datasets and classification metrics, which fail to automatically evaluate the justification production and uncover the nuanced limitations of LLMs in fact-checking. In this work, we introduce FACT-AUDIT, an agent-driven framework that adaptively and dynamically assesses LLMs’ fact-checking capabilities. Leveraging importance sampling principles and multi-agent collaboration, FACT-AUDIT generates adaptive and scalable datasets, performs iterative model-centric evaluations, and updates assessments based on model-specific responses. By incorporating justification production alongside verdict prediction, this framework provides a comprehensive and evolving audit of LLMs’ factual reasoning capabilities, to investigate their trustworthiness. Extensive experiments demonstrate that FACT-AUDIT effectively differentiates among state-of-the-art LLMs, providing valuable insights into model strengths and limitations in model-centric fact-checking analysis.",
    "link": "/venue/2025.acl-long.17@ACL",
    "published_date": null,
    "conference": "ACL.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "Statistical Deficiency for Task Inclusion Estimation",
    "authors": [
      "Loïc Fosse",
      "Frederic Bechet",
      "Benoit Favre",
      "Géraldine Damnati",
      "Gwénolé Lecorvé",
      "Maxime Darrin",
      "Philippe Formont",
      "Pablo Piantanida"
    ],
    "affiliations": [],
    "summary": "Tasks are central in machine learning, as they are the most natural objects to assess the capabilities of current models. The trend is to build general models able to address any task. Even though transfer learning and multitask learning try to leverage the underlying task space, no well-founded tools are available to study its structure. This study proposes a theoretically grounded setup to define the notion of task and to compute the inclusion between two tasks from a statistical deficiency point of view. We propose a tractable proxy as information sufficiency to estimate the degree of inclusion between tasks, show its soundness on synthetic data, and use it to reconstruct empirically the classic NLP pipeline.",
    "link": "/venue/2025.acl-long.18@ACL",
    "published_date": null,
    "conference": "ACL.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "Towards Robust and Efficient Federated Low-Rank Adaptation with Heterogeneous Clients",
    "authors": [
      "Jabin Koo",
      "Minwoo Jang",
      "Jungseul Ok"
    ],
    "affiliations": [],
    "summary": "Federated fine-tuning for Large Language Models (LLMs) has recently gained attention due to the heavy communication overhead of transmitting large model updates. Low Rank Adaptation (LoRA) has been proposed as a solution, yet its application in federated learning is complicated by discordance in aggregation. Existing methods addressing this discordance often suffer from performance degradation at low ranks in heterogeneous data settings. In response, we introduce LoRA-A^2 (Low Rank Adaptation with Alternating freeze and Adaptive rank selection), which demonstrates robustness in challenging settings with low ranks and high data heterogeneity. Our experimental findings reveal that LoRA-A^2 maintains performance even under extreme heterogeneity and low rank conditions, achieving up to a 99.8% reduction in uploaded parameters compared to full fine-tuning without compromising performance. This adaptive mechanism boosts robustness and communication efficiency in federated fine-tuning, enabling the practical deployment of LLMs in resource-constrained environments.",
    "link": "/venue/2025.acl-long.19@ACL",
    "published_date": null,
    "conference": "ACL.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "LLM-Powered Test Case Generation for Detecting Bugs in Plausible Programs",
    "authors": [
      "Kaibo Liu",
      "Zhenpeng Chen",
      "Yiyang Liu",
      "Jie M. Zhang",
      "Mark Harman",
      "Yudong Han",
      "Yun Ma",
      "Yihong Dong",
      "Ge Li",
      "Gang Huang"
    ],
    "affiliations": [],
    "summary": "Detecting tricky bugs in plausible programs, those that pass existing test suites yet still contain bugs, remains a significant challenge in software testing. To address this problem, we propose TrickCatcher, an LLM-powered approach to generating test cases for uncovering bugs in plausible programs. TrickCatcher operates in three stages: First, it uses an LLM to generate program variants based on the program under test (PUT) and its specification. Second, it employs an LLM to construct an input generator from the specification for producing test inputs. Finally, these inputs are executed on both the PUT and its program variants to detect inconsistencies in their outputs. We evaluate TrickCatcher on two datasets, TrickyBugs and EvalPlus, which include 366 human-written and 151 AI-generated plausible programs with tricky bugs. TrickCatcher achieves recall, precision, and F1 scores that are 1.80×, 2.65×, and 1.66× those of the state-of-the-art baselines, respectively. Code and data used are available at https://github.com/RinCloud/TrickCatcher/.",
    "link": "/venue/2025.acl-long.20@ACL",
    "published_date": null,
    "conference": "ACL.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "Capture the Key in Reasoning to Enhance CoT Distillation Generalization",
    "authors": [
      "Chengwei Dai",
      "Kun Li",
      "Wei Zhou",
      "Songlin Hu"
    ],
    "affiliations": [],
    "summary": "As Large Language Models (LLMs) scale up and gain powerful Chain-of-Thoughts (CoTs) reasoning abilities, practical resource constraints drive efforts to distill these capabilities into more compact Smaller Language Models (SLMs). We find that CoTs consist mainly of simple reasoning forms, with a small proportion (4.7%) of key reasoning steps that truly impact conclusions. However, previous distillation methods typically involve supervised fine-tuning student SLMs only on correct CoTs data produced by teacher LLMs, resulting in students struggling to learn the key, instead imitating the teacher’s reasoning forms and making errors or omissions in reasoning. To address these issues, drawing an analogy to human learning, where analyzing mistakes according to correct solutions often reveals the crucial steps leading to successes or failures, we propose mistakE-Driven key reasonIng step distillaTion (EDIT), a novel method that further aids SLMs learning key reasoning steps rather than mere simple fine-tuning. Firstly, to expose the crucial steps in CoTs, we carefully design specific prompts to generate dual CoTs data with similar reasoning paths but divergent conclusions. Then, we apply the minimum edit distance algorithm on the dual CoTs data to locate these key steps and optimize the likelihood on these tokens. Extensive experiments and analysis validate the effectiveness of EDIT across both in-domain(IND) and out-of-domain(OOD) benchmark reasoning datasets.",
    "link": "/venue/2025.acl-long.21@ACL",
    "published_date": null,
    "conference": "ACL.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "How to Enable Effective Cooperation Between Humans and NLP Models: A Survey of Principles, Formalizations, and Beyond",
    "authors": [
      "Chen Huang",
      "Yang Deng",
      "Wenqiang Lei",
      "Jiancheng Lv",
      "Tat-Seng Chua",
      "Jimmy Huang"
    ],
    "affiliations": [],
    "summary": "With the advancement of large language models (LLMs), intelligent models have evolved from mere tools to autonomous agents with their own goals and strategies for cooperating with humans. This evolution has birthed a novel paradigm in NLP, i.e., human-model cooperation, that has yielded remarkable progress in numerous NLP tasks in recent years. In this paper, we take the first step to present a thorough review of human-model cooperation, exploring its principles, formalizations, and open challenges. In particular, we introduce a new taxonomy that provides a unified perspective to summarize existing approaches. Also, we discuss potential frontier areas and their corresponding challenges. We regard our work as an entry point, paving the way for more breakthrough research in this regard.",
    "link": "/venue/2025.acl-long.22@ACL",
    "published_date": null,
    "conference": "ACL.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "Enhancing Hyperbole and Metaphor Detection with Their Bidirectional Dynamic Interaction and Emotion Knowledge",
    "authors": [
      "Li Zheng",
      "Sihang Wang",
      "Hao Fei",
      "Zuquan Peng",
      "Fei Li",
      "Jianming Fu",
      "Chong Teng",
      "Donghong Ji"
    ],
    "affiliations": [],
    "summary": "Text-based hyperbole and metaphor detection are of great significance for natural language processing (NLP) tasks. However, due to their semantic obscurity and expressive diversity, it is rather challenging to identify them. Existing methods mostly focus on superficial text features, ignoring the associations of hyperbole and metaphor as well as the effect of implicit emotion on perceiving these rhetorical devices. To implement these hypotheses, we propose an emotion-guided hyperbole and metaphor detection framework based on bidirectional dynamic interaction (EmoBi). Firstly, the emotion analysis module deeply mines the emotion connotations behind hyperbole and metaphor. Next, the emotion-based domain mapping module identifies the target and source domains to gain a deeper understanding of the implicit meanings of hyperbole and metaphor. Finally, the bidirectional dynamic interaction module enables the mutual promotion between hyperbole and metaphor. Meanwhile, a verification mechanism is designed to ensure detection accuracy and reliability. Experiments show that EmoBi outperforms all baseline methods on four datasets. Specifically, compared to the current SoTA, the F1 score increased by 28.1% for hyperbole detection on the TroFi dataset and 23.1% for metaphor detection on the HYPO-L dataset. These results, underpinned by in-depth analyses, underscore the effectiveness and potential of our approach for advancing hyperbole and metaphor detection.",
    "link": "/venue/2025.acl-long.23@ACL",
    "published_date": null,
    "conference": "ACL.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "UniICL: An Efficient ICL Framework Unifying Compression, Selection, and Generation",
    "authors": [
      "Jun Gao",
      "Qi Lv",
      "Zili Wang",
      "Tianxiang Wu",
      "Ziqiang Cao",
      "Wenjie Li"
    ],
    "affiliations": [],
    "summary": "In-context learning (ICL) enhances the reasoning abilities of Large Language Models (LLMs) by prepending a few demonstrations. It motivates researchers to introduce more examples to provide additional contextual information for the generation. However, existing methods show a significant limitation due to the problem of excessive growth in context length which causes a large hardware burden. Additionally, shallow-relevant examples selected by out-off-shelf tools hinder LLMs from capturing useful contextual information for generation. In this paper, to approach these limitations, we propose UniICL, a novel Unified ICL framework that unifies demonstration compression, demonstration selection, and final response generation. Furthermore, to avoid repeated compression of the same demonstration and boost inference efficiency, we design a tailored compression strategy that allows UniICL caching compression results into Demonstration Bank(DB). Extensive out-of-domain evaluations prove the advantages of UniICL in both effectiveness and efficiency.",
    "link": "/venue/2025.acl-long.24@ACL",
    "published_date": null,
    "conference": "ACL.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "BelarusianGLUE: Towards a Natural Language Understanding Benchmark for Belarusian",
    "authors": [
      "Maksim Aparovich",
      "Volha Harytskaya",
      "Vladislav Poritski",
      "Oksana Volchek",
      "Pavel Smrz"
    ],
    "affiliations": [],
    "summary": "In the epoch of multilingual large language models (LLMs), it is still challenging to evaluate the models’ understanding of lower-resourced languages, which motivates further development of expert-crafted natural language understanding benchmarks. We introduce BelarusianGLUE — a natural language understanding benchmark for Belarusian, an East Slavic language, with ≈15K instances in five tasks: sentiment analysis, linguistic acceptability, word in context, Winograd schema challenge, textual entailment. A systematic evaluation of BERT models and LLMs against this novel benchmark reveals that both types of models approach human-level performance on easier tasks, such as sentiment analysis, but there is a significant gap in performance between machine and human on a harder task — Winograd schema challenge. We find the optimal choice of model type to be task-specific: e.g. BERT models underperform on textual entailment task but are competitive for linguistic acceptability. We release the datasets (https://hf.co/datasets/maaxap/BelarusianGLUE) and evaluation code (https://github.com/maaxap/BelarusianGLUE).",
    "link": "/venue/2025.acl-long.25@ACL",
    "published_date": null,
    "conference": "ACL.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "Towards Automated Error Discovery: A Study in Conversational AI",
    "authors": [
      "Dominic Petrak",
      "Thy Thy Tran",
      "Iryna Gurevych"
    ],
    "affiliations": [],
    "summary": "Although LLM-based conversational agents demonstrate strong fluency and coherence, they still produce undesirable behaviors (errors) that are challenging to prevent from reaching users during deployment. Recent research leverages large language models (LLMs) to detect errors and guide response-generation models toward improvement. However, current LLMs struggle to identify errors not explicitly specified in their instructions, such as those arising from updates to the response-generation model or shifts in user behavior. In this work, we introduce Automated Error Discovery, a framework for detecting and defining errors in conversational AI, and propose SEEED (Soft Clustering Extended Encoder-Based Error Detection), as an encoder-based approach to its implementation. We enhance the Soft Nearest Neighbor Loss by amplifying distance weighting for negative samples and introduce Label-Based Sample Ranking to select highly contrastive examples for better representation learning. SEEED outperforms adapted baselines—including GPT-4o and Phi-4—across multiple error-annotated dialogue datasets, improving the accuracy for detecting unknown errors by up to 8 points and demonstrating strong generalization to unknown intent detection.",
    "link": "/venue/2025.emnlp-main.1@ACL",
    "published_date": null,
    "conference": "EMNLP.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "Break the Checkbox: Challenging Closed-Style Evaluations of Cultural Alignment in LLMs",
    "authors": [
      "Mohsinul Kabir",
      "Ajwad Abrar",
      "Sophia Ananiadou"
    ],
    "affiliations": [],
    "summary": "A large number of studies rely on closed-style multiple-choice surveys to evaluate cultural alignment in Large Language Models (LLMs). In this work, we challenge this constrained evaluation paradigm and explore more realistic, unconstrained approaches. Using the World Values Survey (WVS) and Hofstede Cultural Dimensions as case studies, we demonstrate that LLMs exhibit stronger cultural alignment in less constrained settings, where responses are not forced. Additionally, we show that even minor changes, such as reordering survey choices, lead to inconsistent outputs, exposing the limitations of closed-style evaluations. Our findings advocate for more robust and flexible evaluation frameworks that focus on specific cultural proxies, encouraging more nuanced and accurate assessments of cultural alignment in LLMs.",
    "link": "/venue/2025.emnlp-main.2@ACL",
    "published_date": null,
    "conference": "EMNLP.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "Biased Tales: Cultural and Topic Bias in Generating Children’s Stories",
    "authors": [
      "Donya Rooein",
      "Vilém Zouhar",
      "Debora Nozza",
      "Dirk Hovy"
    ],
    "affiliations": [],
    "summary": "Stories play a pivotal role in human communication, shaping beliefs and morals, particularly in children. As parents increasingly rely on large language models (LLMs) to craft bedtime stories, the presence of cultural and gender stereotypes in these narratives raises significant concerns. To address this issue, we present Biased Tales, a comprehensive dataset designed to analyze how biases influence protagonists’ attributes and story elements in LLM-generated stories. Our analysis uncovers striking disparities. When the protagonist is described as a girl (as compared to a boy), appearance-related attributes increase by 55.26%. Stories featuring non-Western children disproportionately emphasize cultural heritage, tradition, and family themes far more than those for Western children. Our findings highlight the role of sociocultural bias in making creative AI use more equitable and diverse.",
    "link": "/venue/2025.emnlp-main.3@ACL",
    "published_date": null,
    "conference": "EMNLP.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "Large Language Models as Realistic Microservice Trace Generators",
    "authors": [
      "Donghyun Kim",
      "Sriram Ravula",
      "Taemin Ha",
      "Alex Dimakis",
      "Daehyeok Kim",
      "Aditya Akella"
    ],
    "affiliations": [],
    "summary": "Workload traces are essential to understand complex computer systems’ behavior and manage processing and memory resources. Since real-world traces are hard to obtain, synthetic trace generation is a promising alternative. This paper proposes a first-of-a-kind approach that relies on training a large language model (LLM) to generate synthetic workload traces, specifically microservice call graphs. To capture complex and arbitrary hierarchical structures and implicit constraints in such traces, we propose to train LLMs to generate recursively, making call graph generation a sequence of more manageable steps. To further enforce learning constraints on the traces and generate uncommon situations, we apply additional instruction tuning steps to align our model with the desired trace features. With this method, we train TraceLLM, an LLM for microservice trace generation, and demonstrate that it produces diverse, realistic traces under varied conditions, outperforming existing approaches in both accuracy and validity. The synthetically generated traces can effectively replace real data to optimize important microservice management tasks. Additionally, TraceLLM adapts to downstream trace-related tasks, such as predicting key trace features and infilling missing data.",
    "link": "/venue/2025.emnlp-main.4@ACL",
    "published_date": null,
    "conference": "EMNLP.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "JUDGEBERT: Assessing Legal Meaning Preservation Between Sentences",
    "authors": [
      "David Beauchemin",
      "Michelle Albert-Rochette",
      "Richard Khoury",
      "Pierre-Luc Déziel"
    ],
    "affiliations": [],
    "summary": "Simplifying text while preserving its meaning is a complex yet essential task, especially in sensitive domain applications like legal texts. When applied to a specialized field, like the legal domain, preservation differs significantly from its role in regular texts. This paper introduces FrJUDGE, a new dataset to assess legal meaning preservation between two legal texts. It also introduces JUDGEBERT, a novel evaluation metric designed to assess legal meaning preservation in French legal text simplification. JUDGEBERT demonstrates a superior correlation with human judgment compared to existing metrics. It also passes two crucial sanity checks, while other metrics did not: For two identical sentences, it always returns a score of 100%; on the other hand, it returns 0% for two unrelated sentences. Our findings highlight its potential to transform legal NLP applications, ensuring accuracy and accessibility for text simplification for legal practitioners and lay users.",
    "link": "/venue/2025.emnlp-main.5@ACL",
    "published_date": null,
    "conference": "EMNLP.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "QFrCoLA: a Quebec-French Corpus of Linguistic Acceptability Judgments",
    "authors": [
      "David Beauchemin",
      "Richard Khoury"
    ],
    "affiliations": [],
    "summary": "Large and Transformer-based language models perform outstandingly in various downstream tasks. However, there is limited understanding regarding how these models internalize linguistic knowledge, so various linguistic benchmarks have recently been proposed to facilitate syntactic evaluation of language models across languages. This paper introduces QFrCoLA (Quebec-French Corpus of Linguistic Acceptability Judgments), a normative binary acceptability judgments dataset comprising 25,153 in-domain and 2,675 out-of-domain sentences. Our study leverages the QFrCoLA dataset and seven other linguistic binary acceptability judgment corpora to benchmark seven language models. The results demonstrate that, on average, fine-tuned Transformer-based LM are strong baselines for most languages and that zero-shot binary classification large language models perform poorly on the task. However, for the QFrCoLA benchmark, on average, a fine-tuned Transformer-based LM outperformed other methods tested. It also shows that pre-trained cross-lingual LLMs selected for our experimentation do not seem to have acquired linguistic judgment capabilities during their pre-training for Quebec French. Finally, our experiment results on QFrCoLA show that our dataset, built from examples that illustrate linguistic norms rather than speakers’ feelings, is similar to linguistic acceptability judgment; it is a challenging dataset that can benchmark LM on their linguistic judgment capabilities.",
    "link": "/venue/2025.emnlp-main.6@ACL",
    "published_date": null,
    "conference": "EMNLP.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "Revisiting LLM Value Probing Strategies: Are They Robust and Expressive?",
    "authors": [
      "Siqi Shen",
      "Mehar Singh",
      "Lajanugen Logeswaran",
      "Moontae Lee",
      "Honglak Lee",
      "Rada Mihalcea"
    ],
    "affiliations": [],
    "summary": "The value orientation of Large Language Models (LLMs) has been extensively studied, as it can shape user experiences across demographic groups.However, two key challenges remain: (1) the lack of systematic comparison across value probing strategies, despite the Multiple Choice Question (MCQ) setting being vulnerable to perturbations, and (2) the uncertainty over whether probed values capture in-context information or predict models’ real-world actions.In this paper, we systematically compare three widely used value probing methods: token likelihood, sequence perplexity, and text generation.Our results show that all three methods exhibit large variances under non-semantic perturbations in prompts and option formats, with sequence perplexity being the most robust overall.We further introduce two tasks to assess expressiveness: demographic prompting, testing whether probed values adapt to cultural context; and value–action agreement, testing the alignment of probed values with value-based actions.We find that demographic context has little effect on the text generation method, and probed values only weakly correlate with action preferences across all methods.Our work highlights the instability and the limited expressive power of current value probing methods, calling for more reliable LLM value representations.",
    "link": "/venue/2025.emnlp-main.7@ACL",
    "published_date": null,
    "conference": "EMNLP.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "A Systematic Analysis of Base Model Choice for Reward Modeling",
    "authors": [
      "Kian Ahrabian",
      "Pegah Jandaghi",
      "Negar Mokhberian",
      "Sai Praneeth Karimireddy",
      "Jay Pujara"
    ],
    "affiliations": [],
    "summary": "Reinforcement learning from human feedback (RLHF) and, at its core, reward modeling have become a crucial part of training powerful large language models (LLMs). One commonly overlooked factor in training high-quality reward models (RMs) is the effect of the base model, which is becoming more challenging to choose given the rapidly growing pool of LLMs. In this work, we present a systematic analysis of the effect of base model selection on reward modeling performance. Our results show that the performance can be improved by up to 14% compared to the most common (i.e., default) choice. Moreover, we showcase the strong statistical relation between some existing benchmarks and downstream performances. We also demonstrate that the results from a small set of benchmarks could be combined to boost the model selection (+18% on average in the top 5-10). Lastly, we illustrate the impact of different post-training steps on the final performance and explore using estimated data distributions to reduce performance prediction error.",
    "link": "/venue/2025.emnlp-main.8@ACL",
    "published_date": null,
    "conference": "EMNLP.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "Comparing Specialised Small and General Large Language Models on Text Classification: 100 Labelled Samples to Achieve Break-Even Performance",
    "authors": [
      "Branislav Pecher",
      "Ivan Srba",
      "Maria Bielikova"
    ],
    "affiliations": [],
    "summary": "When solving NLP tasks with limited labelled data, researchers typically either use a general large language model without further update, or use a small number of labelled samples to tune a specialised smaller model. In this work, we answer an important question – how many labelled samples are required for the specialised small models to outperform general large models, while taking the performance variance into consideration. By observing the behaviour of fine-tuning, instruction-tuning, prompting and in-context learning on 8 language models, we identify such performance break-even points across 8 representative text classification tasks of varying characteristics. We show that the specialised models often need only few samples (on average 100) to be on par or better than the general ones. At the same time, the number of required labels strongly depends on the dataset or task characteristics, with fine-tuning on binary datasets requiring significantly more samples. When performance variance is taken into consideration, the number of required labels increases on average by 100 - 200%. Finally, larger models do not consistently lead to better performance and lower variance, with 4-bit quantisation having negligible impact.",
    "link": "/venue/2025.emnlp-main.9@ACL",
    "published_date": null,
    "conference": "EMNLP.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "Is the Top Still Spinning? Evaluating Subjectivity in Narrative Understanding",
    "authors": [
      "Melanie Subbiah",
      "Akankshya Mishra",
      "Grace Kim",
      "Liyan Tang",
      "Greg Durrett",
      "Kathleen McKeown"
    ],
    "affiliations": [],
    "summary": "Determining faithfulness of a claim to a source document is an important problem across many domains. This task is generally treated as a binary judgment of whether the claim is supported or unsupported in relation to the source. In many cases, though, whether a claim is supported can be ambiguous. For instance, it may depend on making inferences from given evidence, and different people can reasonably interpret the claim as either supported or unsupported based on their agreement with those inferences. Forcing binary labels upon such claims lowers the reliability of evaluation. In this work, we reframe the task to manage the subjectivity involved with factuality judgments of ambiguous claims. We introduce LLM-generated edits of summaries as a method of providing a nuanced evaluation of claims: how much does a summary need to be edited to be unambiguous? Whether a claim gets rewritten and how much it changes can be used as an automatic evaluation metric, the Ambiguity Rewrite Metric (ARM), with a much richer feedback signal than a binary judgment of faithfulness. We focus on the area of narrative summarization as it is particularly rife with ambiguity and subjective interpretation. We show that ARM produces a 21% absolute improvement in annotator agreement on claim faithfulness, indicating that subjectivity is reduced.",
    "link": "/venue/2025.emnlp-main.10@ACL",
    "published_date": null,
    "conference": "EMNLP.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "MathTutorBench: A Benchmark for Measuring Open-ended Pedagogical Capabilities of LLM Tutors",
    "authors": [
      "Jakub Macina",
      "Nico Daheim",
      "Ido Hakimi",
      "Manu Kapur",
      "Iryna Gurevych",
      "Mrinmaya Sachan"
    ],
    "affiliations": [],
    "summary": "Evaluating the pedagogical capabilities of AI-based tutoring models is critical for making guided progress in the field. Yet, we lack a reliable, easy-to-use, and simple-to-run evaluation that reflects the pedagogical abilities of models. To fill this gap, we present MathTutorBench, an open-source benchmark for holistic tutoring model evaluation. MathTutorBench contains a collection of datasets and metrics that broadly cover tutor abilities as defined by learning sciences research in dialog-based teaching. To score the pedagogical quality of open-ended teacher responses, we train a reward model and show it can discriminate expert from novice teacher responses with high accuracy. We evaluate a wide set of closed- and open-weight models on MathTutorBench and find that subject expertise, indicated by solving ability, does not immediately translate to good teaching. Rather, pedagogy and subject expertise appear to form a trade-off that is navigated by the degree of tutoring specialization of the model. Furthermore, tutoring appears to become more challenging in longer dialogs, where simpler questioning strategies begin to fail. We release the benchmark, code, and leaderboard openly to enable rapid benchmarking of future models.",
    "link": "/venue/2025.emnlp-main.11@ACL",
    "published_date": null,
    "conference": "EMNLP.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "Preemptive Detection and Correction of Misaligned Actions in LLM Agents",
    "authors": [
      "Haishuo Fang",
      "Xiaodan Zhu",
      "Iryna Gurevych"
    ],
    "affiliations": [],
    "summary": "Deploying LLM-based agents in real-life applications often faces a critical challenge: the misalignment between agents’ behavior and user intent. Such misalignment may lead agents to unintentionally execute some critical actions that carry negative outcomes (e.g., accidentally triggering a buy-now in web shopping), resulting in undesirable or even irreversible consequences. Although addressing these issues is crucial, the preemptive detection and correction of misaligned actions remains relatively underexplored. To fill this gap, we introduce InferAct, a novel approach that leverages the belief reasoning ability of LLMs, grounded in Theory-of-Mind, to detect misaligned actions. Once the misalignment is detected, InferAct alerts users for timely correction, preventing adverse outcomes and enhancing the reliability of LLM agents’ decision-making processes. Experiments on three widely used tasks demonstrate InferAct achieves up to 20% improvements on Marco-F1 against baselines in misaligned action detection. An in-depth evaluation of misalignment correction further highlights InferAct‘s effectiveness in improving agent alignment.",
    "link": "/venue/2025.emnlp-main.12@ACL",
    "published_date": null,
    "conference": "EMNLP.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "Fingerprinting LLMs through Survey Item Factor Correlation: A Case Study on Humor Style Questionnaire",
    "authors": [
      "Simon Münker"
    ],
    "affiliations": [],
    "summary": "LLMs increasingly engage with psychological instruments, yet how they represent constructs internally remains poorly understood. We introduce a novel approach to “fingerprinting” LLMs through their factor correlation patterns on standardized psychological assessments to deepen the understanding of LLMs constructs representation. Using the Humor Style Questionnaire as a case study, we analyze how six LLMs represent and correlate humor-related constructs to survey participants. Our results show that they exhibit little similarity to human response patterns. In contrast, participants’ subsamples demonstrate remarkably high internal consistency. Exploratory graph analysis further confirms that no LLM successfully recovers the four constructs of the Humor Style Questionnaire. These findings suggest that despite advances in natural language capabilities, current LLMs represent psychological constructs in fundamentally different ways than humans, questioning the validity of application as human simulacra.",
    "link": "/venue/2025.emnlp-main.13@ACL",
    "published_date": null,
    "conference": "EMNLP.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "Gradient-Attention Guided Dual-Masking Synergetic Framework for Robust Text-based Person Retrieval",
    "authors": [
      "Tianlu Zheng",
      "Yifan Zhang",
      "Xiang An",
      "Ziyong Feng",
      "Kaicheng Yang",
      "Qichuan Ding"
    ],
    "affiliations": [],
    "summary": "Although Contrastive Language-Image Pre-training (CLIP) exhibits strong performance across diverse vision tasks, its application to person representation learning faces two critical challenges: (i) the scarcity of large-scale annotated vision-language data focused on person-centric images, and (ii) the inherent limitations of global contrastive learning, which struggles to maintain discriminative local features crucial for fine-grained matching while remaining vulnerable to noisy text tokens. This work advances CLIP for person representation learning through synergistic improvements in data curation and model architecture. First, we develop a noise-resistant data construction pipeline that leverages the in-context learning capabilities of MLLMs to automatically filter and caption web-sourced images. This yields WebPerson, a large-scale dataset of 5M high-quality person-centric image-text pairs. Second, we introduce the GA-DMS (Gradient-Attention Guided Dual-Masking Synergetic) framework, which improves cross-modal alignment by adaptively masking noisy textual tokens based on the gradient-attention similarity score. Additionally, we incorporate masked token prediction objectives that compel the model to predict informative text tokens, enhancing fine-grained semantic representation learning. Extensive experiments show that GA-DMS achieves state-of-the-art performance across multiple benchmarks. The data and pre-trained models are released at https://github.com/Multimodal-Representation-Learning-MRL/GA-DMS.",
    "link": "/venue/2025.emnlp-main.14@ACL",
    "published_date": null,
    "conference": "EMNLP.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "From Problem-Solving to Teaching Problem-Solving: Aligning LLMs with Pedagogy using Reinforcement Learning",
    "authors": [
      "David Dinucu-Jianu",
      "Jakub Macina",
      "Nico Daheim",
      "Ido Hakimi",
      "Iryna Gurevych",
      "Mrinmaya Sachan"
    ],
    "affiliations": [],
    "summary": "Large language models (LLMs) can transform education, but their optimization for direct question-answering often undermines effective pedagogy which requires strategically withholding answers. To mitigate this, we propose an online reinforcement learning (RL)-based alignment framework that can quickly adapt LLMs into effective tutors using simulated student-tutor interactions by emphasizing pedagogical quality and guided problem-solving over simply giving away answers. We use our method to train a 7B parameter tutor model without human annotations which reaches similar performance to larger proprietary models like LearnLM. We introduce a controllable reward weighting to balance pedagogical support and student solving accuracy, allowing us to trace the Pareto frontier between these two objectives. Our models better preserve reasoning capabilities than single-turn SFT baselines and can optionally enhance interpretability through thinking tags that expose the model’s instructional planning.",
    "link": "/venue/2025.emnlp-main.15@ACL",
    "published_date": null,
    "conference": "EMNLP.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "CompKBQA: Component-wise Task Decomposition for Knowledge Base Question Answering",
    "authors": [
      "Yuhang Tian",
      "Dandan Song",
      "Zhijing Wu",
      "Pan Yang",
      "Changzhi Zhou",
      "Jun Yang",
      "Hao Wang",
      "Huipeng Ma",
      "Chenhao Li",
      "Luan Zhang"
    ],
    "affiliations": [],
    "summary": "Knowledge Base Question Answering (KBQA) aims to extract accurate answers from the Knowledge Base (KB). Traditional Semantic Parsing (SP)-based methods are widely used but struggle with complex queries. Recently, large language models (LLMs) have shown promise in improving KBQA performance. However, the challenge of generating error-free logical forms remains, as skeleton, topic Entity, and relation Errors still frequently occur. To address these challenges, we propose CompKBQA(Component-wise Task Decomposition for Knowledge Base Question Answering), a novel framework that optimizes the process of fine-tuning a LLM for generating logical forms by enabling the LLM to progressively learn relevant sub-tasks like skeleton generation, topic entity generation, and relevant relations generation. Additionally, we propose R3, which retrieves and incorporates KB information into the process of logical form generation. Experimental evaluations on two benchmark KBQA datasets, WebQSP and CWQ, demonstrate that CompKBQA achieves state-of-the-art performance, highlighting the importance of task decomposition and KB-aware learning.",
    "link": "/venue/2025.emnlp-main.16@ACL",
    "published_date": null,
    "conference": "EMNLP.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "Permutative Preference Alignment from Listwise Ranking of Human Judgments",
    "authors": [
      "Yang Zhao",
      "Yixin Wang",
      "Mingzhang Yin"
    ],
    "affiliations": [],
    "summary": "Aligning Large Language Models (LLMs) with human preferences is crucial in ensuring desirable and controllable model behaviors. Current methods, such as Reinforcement Learning from Human Feedback (RLHF) and Direct Preference Optimization (DPO), rely on the Bradley-Terry (B-T) model to maximize the likelihood of pairwise choices. However, when multiple responses are available, the B-T model fails to guarantee an accurate list ranking of the responses. To address this issue, we propose Permutative Preference Alignment (PPA), a novel offline listwise approach that incorporates the Normalized Discounted Cumulative Gain (NDCG)—a widely-used ranking metric—as an alternative training objective for LLM alignment. We develop an end-to-end alignment algorithm by approximating NDCG with a differentiable surrogate loss. Experiments demonstrate that PPA outperforms existing pairwise and listwise methods on evaluation sets and general benchmarks such as AlpacaEval. Furthermore, we show that NDCG-based approaches improve ranking accuracy more effectively than B-T-based methods and provide a theoretical explanation for this improvement.",
    "link": "/venue/2025.emnlp-main.17@ACL",
    "published_date": null,
    "conference": "EMNLP.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "ToneCraft: Cantonese Lyrics Generation with Harmony of Tones and Pitches",
    "authors": [
      "Junyu Cheng",
      "Chang Pan",
      "Shuangyin Li"
    ],
    "affiliations": [],
    "summary": "Lyrics generation has garnered increasing attention within the artificial intelligence community. Our task focuses on generating harmonious Cantonese lyrics. Unlike other languages, Cantonese has a unique system of nine contours and six tones, making it essential to satisfy the harmony rules that ensure the alignment between the melody and the tonal contours of the lyrics when composing lyrics. Current research has not yet addressed the challenge of generating lyrics that adhere to Cantonese harmony rules. To tackle this issue, we propose ToneCraft, a novel framework for generating Cantonese lyrics that ensures tonal and melodic harmony. It enables LLMs to generate lyrics with a fixed character count while aligning with tonal and melodic structures. We present an algorithm that combines character-level control, melodic guidance, and a task-specific loss to achieve tonal harmony without compromising generation flexibility and quality. By incorporating domain-specific expertise, we leverage pure lyric datasets to train our model, eliminating the need for aligned data. Both objective evaluations and subjective assessments show that our generated lyrics align with melodic contours significantly better than existing methods. All code and data are available at: https://github.com/purepasser-by/ToneCraft.",
    "link": "/venue/2025.emnlp-main.18@ACL",
    "published_date": null,
    "conference": "EMNLP.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "SensorLLM: Aligning Large Language Models with Motion Sensors for Human Activity Recognition",
    "authors": [
      "Zechen Li",
      "Shohreh Deldari",
      "Linyao Chen",
      "Hao Xue",
      "Flora D. Salim"
    ],
    "affiliations": [],
    "summary": "We introduce SensorLLM, a two-stage framework that enables Large Language Models (LLMs) to perform human activity recognition (HAR) from sensor time-series data. Despite their strong reasoning and generalization capabilities, LLMs remain underutilized for motion sensor data due to the lack of semantic context in time-series, computational constraints, and challenges in processing numerical inputs. SensorLLM addresses these limitations through a Sensor-Language Alignment stage, where the model aligns sensor inputs with trend descriptions. Special tokens are introduced to mark channel boundaries. This alignment enables LLMs to capture numerical variations, channel-specific features, and data of varying durations, without requiring human annotations. In the subsequent Task-Aware Tuning stage, we refine the model for HAR classification, achieving performance that matches or surpasses state-of-the-art methods. Our results demonstrate that SensorLLM evolves into an effective sensor learner, reasoner, and classifier through human-intuitive Sensor-Language Alignment, generalizing across diverse HAR datasets. We believe this work establishes a foundation for future research on time-series and text alignment, paving the way for foundation models in sensor data analysis. Our codes are available at https://github.com/zechenli03/SensorLLM.",
    "link": "/venue/2025.emnlp-main.19@ACL",
    "published_date": null,
    "conference": "EMNLP.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "MixLoRA-DSI: Dynamically Expandable Mixture-of-LoRA Experts for Rehearsal-Free Generative Retrieval over Dynamic Corpora",
    "authors": [
      "Tuan-Luc Huynh",
      "Thuy-Trang Vu",
      "Weiqing Wang",
      "Trung Le",
      "Dragan Gasevic",
      "Yuan-Fang Li",
      "Thanh-Toan Do"
    ],
    "affiliations": [],
    "summary": "Continually updating model-based indexes in generative retrieval with new documents remains challenging, as full retraining is computationally expensive and impractical under resource constraints. We propose MixLoRA-DSI, a novel framework that combines an expandable mixture of Low-Rank Adaptation experts with a layer-wise out-of-distribution (OOD)-driven expansion strategy. Instead of allocating new experts for each new corpus, our proposed expansion strategy enables sublinear parameter growth by selectively introducing new experts only when significant number of OOD documents are detected. Experiments on NQ320k and MS MARCO Passage demonstrate that MixLoRA-DSI outperforms full-model update baselines, with minimal parameter overhead and substantially lower training costs.",
    "link": "/venue/2025.emnlp-main.20@ACL",
    "published_date": null,
    "conference": "EMNLP.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "ViClaim: A Multilingual Multilabel Dataset for Automatic Claim Detection in Videos",
    "authors": [
      "Patrick Giedemann",
      "Pius von Däniken",
      "Jan Milan Deriu",
      "Alvaro Rodrigo",
      "Anselmo Peñas",
      "Mark Cieliebak"
    ],
    "affiliations": [],
    "summary": "The growing influence of video content as a medium for communication and misinformation underscores the urgent need for effective tools to analyze claims in multilingual and multi-topic settings. Existing efforts in misinformation detection largely focus on written text, leaving a significant gap in addressing the complexity of spoken text in video transcripts. We introduce ViClaim, a dataset of 1,798 annotated video transcripts across three languages (English, German, Spanish) and six topics. Each sentence in the transcripts is labeled with three claim-related categories: fact-check-worthy, fact-non-check-worthy, or opinion. We developed a custom annotation tool to facilitate the highly complex annotation process. Experiments with state-of-the-art multilingual language models demonstrate strong performance in cross-validation (macro F1 up to 0.896) but reveal challenges in generalization to unseen topics, particularly for distinct domains. Our findings highlight the complexity of claim detection in video transcripts. ViClaim offers a robust foundation for advancing misinformation detection in video-based communication, addressing a critical gap in multimodal analysis.",
    "link": "/venue/2025.emnlp-main.21@ACL",
    "published_date": null,
    "conference": "EMNLP.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "DeepResearcher: Scaling Deep Research via Reinforcement Learning in Real-world Environments",
    "authors": [
      "Yuxiang Zheng",
      "Dayuan Fu",
      "Xiangkun Hu",
      "Xiaojie Cai",
      "Lyumanshan Ye",
      "Pengrui Lu",
      "Pengfei Liu"
    ],
    "affiliations": [],
    "summary": "Large Language Models (LLMs) with web search capabilities show significant potential for deep research, yet current methods—brittle prompt engineering or RAG-based reinforcement learning in controlled environments—fail to capture real-world complexities. In this paper, we introduce DeepResearcher, the first comprehensive framework for end-to-end training of LLM-based deep research agents through scaling reinforcement learning (RL) in real-world environments with authentic web search interactions. Unlike RAG approaches reliant on fixed corpora, DeepResearcher trains agents to navigate the noisy, dynamic open web. We implement a specialized multi-agent architecture where browsing agents extract relevant information from various webpage structures and overcoming significant technical challenges. Extensive experiments on open-domain research tasks demonstrate that DeepResearcher achieves substantial improvements of up to 28.9 points over prompt engineering-based baselines and up to 7.2 points over RAG-based RL agents. Our qualitative analysis reveals emergent cognitive behaviors from end-to-end RL training, such as planning, cross-validation, self-reflection for research redirection, and maintain honesty when unable to find definitive answers. Our results highlight that end-to-end training in real-world web environments is fundamental for developing robust research capabilities aligned with real-world applications. The source codefor DeepResearcher is released at: https://github.com/GAIR-NLP/DeepResearcher.",
    "link": "/venue/2025.emnlp-main.22@ACL",
    "published_date": null,
    "conference": "EMNLP.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "Mixture of Length and Pruning Experts for Knowledge Graphs Reasoning",
    "authors": [
      "Enjun Du",
      "Siyi Liu",
      "Yongqi Zhang"
    ],
    "affiliations": [],
    "summary": "Knowledge Graph (KG) reasoning, which aims to infer new facts from structured knowledge repositories, plays a vital role in Natural Language Processing (NLP) systems. Its effectiveness critically depends on constructing informative and contextually relevant reasoning paths. However, existing graph neural networks (GNNs) often adopt rigid, query-agnostic path-exploration strategies, limiting their ability to adapt to diverse linguistic contexts and semantic nuances. To address these limitations, we propose MoKGR, a mixture-of-experts framework that personalizes path exploration through two complementary components: (1) a mixture of length experts that adaptively selects and weights candidate path lengths according to query complexity, providing query-specific reasoning depth; and (2) a mixture of pruning experts that evaluates candidate paths from a complementary perspective, retaining the most informative paths for each query. Through comprehensive experiments on diverse benchmark, MoKGR demonstrates superior performance in both transductive and inductive settings, validating the effectiveness of personalized path exploration in KGs reasoning.",
    "link": "/venue/2025.emnlp-main.23@ACL",
    "published_date": null,
    "conference": "EMNLP.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "MPRF: Interpretable Stance Detection through Multi-Path Reasoning Framework",
    "authors": [
      "ZhaoDan Zhang",
      "Jin Zhang",
      "Hui Xu",
      "Jiafeng Guo",
      "Xueqi Cheng"
    ],
    "affiliations": [],
    "summary": "Stance detection, a critical task in Natural Language Processing (NLP), aims to identify the attitude expressed in text toward specific targets. Despite advancements in Large Language Models (LLMs), challenges such as limited interpretability and handling nuanced content persist. To address these issues, we propose the Multi-Path Reasoning Framework (MPRF), a novel framework that generates, evaluates, and integrates multiple reasoning paths to improve accuracy, robustness, and transparency in stance detection. Unlike prior work that relies on single-path reasoning or static explanations, MPRF introduces a structured end-to-end pipeline: it first generates diverse reasoning paths through predefined perspectives, then dynamically evaluates and optimizes each path using LLM-based scoring, and finally fuses the results via weighted aggregation to produce interpretable and reliable predictions. Extensive experiments on the SEM16, VAST, and PStance datasets demonstrate that MPRF outperforms existing models. Ablation studies further validate the critical role of MPRF’s components, highlighting its effectiveness in enhancing interpretability and handling complex stance detection tasks.",
    "link": "/venue/2025.emnlp-main.24@ACL",
    "published_date": null,
    "conference": "EMNLP.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "Analyzing the Effects of Supervised Fine-Tuning on Model Knowledge from Token and Parameter Levels",
    "authors": [
      "Junjie Ye",
      "Yuming Yang",
      "Yang Nan",
      "Shuo Li",
      "Qi Zhang",
      "Tao Gui",
      "Xuanjing Huang",
      "Peng Wang",
      "Zhongchao Shi",
      "Jianping Fan"
    ],
    "affiliations": [],
    "summary": "Large language models (LLMs) acquire substantial world knowledge during pre-training, which is further shaped by post-training techniques such as supervised fine-tuning (SFT). However, the impact of SFT on a model’s knowledge remains underexplored, limiting our ability to control knowledge behavior in fine-tuned models. To address this gap, we evaluate closed-book question answering (CBQA) performance across five LLMs from the LLaMA-2 and LLaMA-3 families. Surprisingly, models fine-tuned on 1,920 samples perform up to 14% worse than those fine-tuned on only 240 samples. Furthermore, varying the level of knowledge mastery in the fine-tuning data leads to performance fluctuations of over 12%. To investigate these effects, we analyze model behavior at both the token and parameter levels. Our analysis reveals that up to 90% of parameter updates during SFT do not contribute to knowledge enhancement. Restoring these updates can improve performance on the CBQA task, depending on the characteristics of the fine-tuning data. These insights offer practical guidance for developing fine-tuning strategies that more effectively strengthen model knowledge.",
    "link": "/venue/2025.emnlp-main.25@ACL",
    "published_date": null,
    "conference": "EMNLP.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "Camera Resection from Known Line Pencils and a Radially Distorted Scanline",
    "authors": [
      "Juan C. Dibene",
      "Enrique Dunn"
    ],
    "affiliations": [],
    "summary": "We present a marker-based geometric estimation framework for the absolute pose of a camera by analyzing the 1D observations in a single radially distorted pixel scanline.We leverage a pair of known co-planar pencils of lines, along with lens distortion parameters, to propose an ensemble of solvers exploring the space of estimation strategies applicable to our setup.First, we present a minimal algebraic solver requiring only six measurements and yielding eight solutions, which relies on the intersection of two conics defined by one of the pencils of lines.Then, we present a unique closed-form geometric solver from seven measurements.Finally, we present an homography-based formulation amenable to linear least-squares from eight or more measurements.Our geometric framework constitutes a theoretical analysis on the minimum geometric context necessary to solve in closed form for the absolute pose of a single camera from a single radially distorted scanline.",
    "link": "/venue/Dibene_Camera_Resection_from_Known_Line_Pencils_and_a_Radially_Distorted@CVPR2025@CVF",
    "published_date": null,
    "conference": "CVPR.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "Seeing Far and Clearly: Mitigating Hallucinations in MLLMs with Attention Causal Decoding",
    "authors": [
      "Feilong Tang",
      "Chengzhi Liu",
      "Zhongxing Xu",
      "Ming Hu",
      "Zile Huang",
      "Haochen Xue",
      "Ziyang Chen",
      "Zelin Peng",
      "Zhiwei Yang",
      "Sijin Zhou",
      "Wenxue Li",
      "Yulong Li",
      "Wenxuan Song",
      "Shiyan Su",
      "Wei Feng",
      "Jionglong Su",
      "Mingquan Lin",
      "Yifan Peng",
      "Xuelian Cheng",
      "Imran Razzak",
      "Zongyuan Ge"
    ],
    "affiliations": [],
    "summary": "Recent advancements in multimodal large language models (MLLMs) have significantly improved performance in visual question answering. However, they often suffer from hallucinations. In this work, hallucinations are categorized into two main types: initial hallucinations and snowball hallucinations. We argue that adequate contextual information can be extracted directly from the token interaction process. Inspired by causal inference in decoding strategy, we propose to leverage causal masks to establish information propagation between multimodal tokens. The hypothesis is that insufficient interaction between those tokens may lead the model to rely on outlier tokens, overlooking dense and rich contextual cues. Therefore, we propose to intervene in the propagation process by tackling outlier tokens to enhance in-context inference. With this goal, we present FarSight, a versatile plug-and-play decoding strategy to reduce attention interference from outlier tokens merely by optimizing the causal mask. The heart of our method is effective token propagation. We design an attention register structure within the upper triangular matrix of the causal mask, dynamically allocating attention capture attention diverted to outlier tokens. Moreover, a positional awareness encoding method with a diminishing masking rate is proposed, allowing the model to attend to further preceding tokens, especially for video sequence tasks. With extensive experiments, FarSight demonstrates significant hallucination-mitigating performance across different MLLMs on both image and video benchmarks, proving its effectiveness.",
    "link": "/venue/Tang_Seeing_Far_and_Clearly_Mitigating_Hallucinations_in_MLLMs_with_Attention@CVPR2025@CVF",
    "published_date": null,
    "conference": "CVPR.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "UniAP: Unifying Inter- and Intra-Layer Automatic Parallelism by Mixed Integer Quadratic Programming",
    "authors": [
      "Hao Lin",
      "Ke Wu",
      "Jie Li",
      "Jun Li",
      "Wu-Jun Li"
    ],
    "affiliations": [],
    "summary": "Distributed learning is commonly used for training deep learning models, especially large models. In distributed learning, manual parallelism (MP) methods demand considerable human effort and have limited flexibility. Hence, automatic parallelism (AP) methods have recently been proposed for automating the parallel strategy optimization process. Existing AP methods suffer from sub-optimal solutions because they do not jointly optimize the two categories of parallel strategies (i.e., inter-layer parallelism and intra-layer parallelism). In this paper, we propose a novel AP method called UniAP, which unifies inter- and intra-layer automatic parallelism by mixed integer quadratic programming. To the best of our knowledge, UniAP is the first parallel method that can jointly optimize the two categories of parallel strategies to find an optimal solution. Experimental results show that UniAP outperforms state-of-the-art methods by up to 3.80$\\times$ in throughput and reduces strategy optimization time by up to 107$\\times$ across five Transformer-based models.",
    "link": "/venue/Lin_UniAP_Unifying_Inter-_and_Intra-Layer_Automatic_Parallelism_by_Mixed_Integer@CVPR2025@CVF",
    "published_date": null,
    "conference": "CVPR.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "SegEarth-OV: Towards Training-Free Open-Vocabulary Segmentation for Remote Sensing Images",
    "authors": [
      "Kaiyu Li",
      "Ruixun Liu",
      "Xiangyong Cao",
      "Xueru Bai",
      "Feng Zhou",
      "Deyu Meng",
      "Zhi Wang"
    ],
    "affiliations": [],
    "summary": "Current remote sensing semantic segmentation methods are mostly built on the close-set assumption, meaning that the model can only recognize pre-defined categories that exist in the training set. However, in practical Earth observation, there are countless unseen categories, and manual annotation is impractical. To address this challenge, we first attempt to introduce training-free open-vocabulary semantic segmentation (OVSS) into the remote sensing context. However, due to the sensitivity of remote sensing images to low-resolution features, distorted target shapes and ill-fitting boundaries are exhibited in the prediction mask. To tackle these issues, we propose a simple and universal upsampler, i.e. SimFeatUp, to restore lost spatial information of deep features. Specifically, SimFeatUp only needs to learn from a few unlabeled images, and can upsample arbitrary remote sensing image features. Furthermore, based on the observation of the abnormal response of patch tokens to the [CLS] token in CLIP, we propose to execute a simple subtraction operation to alleviate the global bias in patch tokens. Extensive experiments are conducted on 17 remote sensing datasets of 4 tasks, including semantic segmentation, building extraction, road detection, and flood detection. Our method achieves an average of 5.8\\%, 8.2\\%, 4.0\\%, and 15.3\\% improvement over state-of-the-art methods on the 4 tasks.",
    "link": "/venue/Li_SegEarth-OV_Towards_Training-Free_Open-Vocabulary_Segmentation_for_Remote_Sensing_Images@CVPR2025@CVF",
    "published_date": null,
    "conference": "CVPR.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "VGGT: Visual Geometry Grounded Transformer",
    "authors": [
      "Jianyuan Wang",
      "Minghao Chen",
      "Nikita Karaev",
      "Andrea Vedaldi",
      "Christian Rupprecht",
      "David Novotny"
    ],
    "affiliations": [],
    "summary": "We present VGGN, a feed-forward neural network that infers directly all key 3D attributes of a scene, such as camera poses, point maps, depth maps, and 3D point tracks, from few or hundreds of its views. Unlike recent alternatives, VGGN does not need to use visual geometry optimization techniques to refine the results in post-processing, obtaining all quantities of interest directly. This approach is simple and more efficient, reconstructing hundreds of images in seconds. We train VGGN on a large number of publicly available datasets with 3D annotations and demonstrate its ability to achieve state-of-the-art results in multiple 3D tasks, including camera pose estimation, multi-view depth estimation, dense point cloud reconstruction, and 3D point tracking. This is a step forward in 3D computer vision, where models have been typically constrained to and specialized for single tasks. We extensively evaluate our method on unseen datasets to demonstrate its superior performance. We will release the code and trained model.",
    "link": "/venue/Wang_VGGT_Visual_Geometry_Grounded_Transformer@CVPR2025@CVF",
    "published_date": null,
    "conference": "CVPR.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "Reconstructing Humans with a Biomechanically Accurate Skeleton",
    "authors": [
      "Yan Xia",
      "Xiaowei Zhou",
      "Etienne Vouga",
      "Qixing Huang",
      "Georgios Pavlakos"
    ],
    "affiliations": [],
    "summary": "In this paper, we introduce a method for reconstructing humans in 3D from a single image using a biomechanically accurate skeleton model. To achieve this, we train a transformer that takes an image as input and estimates the parameters of the model. Due to the lack of training data for this task, we build a pipeline to generate pseudo ground truth data and implement a training procedure that iteratively refines these pseudo labels for improved accuracy. Compared to state-of-the-art methods in 3D human pose estimation, our model achieves competitive performance on standard benchmarks, while it significantly outperforms them in settings with extreme 3D poses and viewpoints. This result highlights the benefits of using a biomechanical skeleton with realistic degrees of freedom for robust pose estimation. Additionally, we show that previous models frequently violate joint angle limits, leading to unnatural rotations. In contrast, our approach leverages the biomechanically plausible degrees of freedom leading to more realistic joint rotation estimates. We validate our approach across multiple human pose estimation benchmarks. We will make all code, models and data publicly available upon publication.",
    "link": "/venue/Xia_Reconstructing_Humans_with_a_Biomechanically_Accurate_Skeleton@CVPR2025@CVF",
    "published_date": null,
    "conference": "CVPR.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "CraftsMan3D: High-fidelity Mesh Generation with 3D Native Diffusion and Interactive Geometry Refiner",
    "authors": [
      "Weiyu Li",
      "Jiarui Liu",
      "Hongyu Yan",
      "Rui Chen",
      "Yixun Liang",
      "Xuelin Chen",
      "Ping Tan",
      "Xiaoxiao Long"
    ],
    "affiliations": [],
    "summary": "We present a novel generative 3D modeling system, coined CraftsMan, which can generate high-fidelity 3D geometries with highly varied shapes, regular mesh topologies, and detailed surfaces, and, notably, allows for refining the geometry in an interactive manner. Despite the significant advancements in 3D generation, existing methods still struggle with lengthy optimization processes, self-occlusion, irregular mesh topologies, and difficulties in accommodating user edits, consequently impeding their widespread adoption and implementation in 3D modeling softwares. Our work is inspired by the craftsman, who usually roughs out the holistic figure of the work first and elaborates the surface details subsequently. Specifically, we first introduce a robust data preprocessing pipeline that utilizes visibility check and winding mumber to maximize the use of existing 3D data. Leveraging this data, we employ a 3D-native DiT model that directly models the distribution of 3D data in latent space, generating coarse geometries with regular mesh topology in seconds. Subsequently, a normal-based geometry refiner enhances local surface details, which can be applied automatically or interactively with user input. Extensive experiments demonstrate that our method achieves high efficacy in producing superior quality 3D assets compared to existing methods.",
    "link": "/venue/Li_CraftsMan3D_High-fidelity_Mesh_Generation_with_3D_Native_Diffusion_and_Interactive@CVPR2025@CVF",
    "published_date": null,
    "conference": "CVPR.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "DNF: Unconditional 4D Generation with Dictionary-based Neural Fields",
    "authors": [
      "Xinyi Zhang",
      "Naiqi Li",
      "Angela Dai"
    ],
    "affiliations": [],
    "summary": "While remarkable success has been achived through diffusion-based 3D generative models for shapes, 4D generative modeling remains challenging due to the complexity of object deformations over time. We propose DNF, a new 4D representation for unconditional generative modeling that efficiently models deformable shapes with disentangled shape and motion while capturing high-fidelity details in the deforming objects. To achieve this, we propose a dictionary learning approach to disentangle 4D motion from shape as neural fields.Both shape and motion are represented as learned latent spaces, where each deformable shape is represented by its shape and motion global latent codes, shape-specific coefficient vectors, and shared dictionary information. This captures both shape-specific detail and global shared information in the learned dictionary. Our dictionary-based representation well balances fidelity, contiguity and compression -- combined with a transformer-based diffusion model, our method is able to generate effective, high-fidelity 4D animations.",
    "link": "/venue/Zhang_DNF_Unconditional_4D_Generation_with_Dictionary-based_Neural_Fields@CVPR2025@CVF",
    "published_date": null,
    "conference": "CVPR.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "Removing Reflections from RAW Photos",
    "authors": [
      "Eric Kee",
      "Adam Pikielny",
      "Kevin Blackburn-Matzen",
      "Marc Levoy"
    ],
    "affiliations": [],
    "summary": "We describe a system to remove real-world reflections from images for consumer photography. Our system operates on linear (RAW) photos, and accepts an optional contextual photo looking in the opposite direction (e.g., the \"selfie\" camera on a mobile device). This optional photo helps disambiguate what should be considered the reflection. The system is trained solely on synthetic mixtures of real-world RAW images, which we combine using a reflection simulation that is photometrically and geometrically accurate. Our system comprises a base model that accepts the captured photo and optional context photo as input, and runs at 256p, followed by an up-sampling model that transforms 256p images to full resolution. The system can produce images for review at 1K in 4.5 to 6.5 seconds on a MacBook or iPhone 14 Pro. We test on RAW photos that were captured in the field and embody typical consumer photos, and show that our RAW-image simulation yields SOTA performance.",
    "link": "/venue/Kee_Removing_Reflections_from_RAW_Photos@CVPR2025@CVF",
    "published_date": null,
    "conference": "CVPR.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "Enhancing SAM with Efficient Prompting and Preference Optimization for Semi-supervised Medical Image Segmentation",
    "authors": [
      "Aishik Konwer",
      "Zhijian Yang",
      "Erhan Bas",
      "Cao Xiao",
      "Prateek Prasanna",
      "Parminder Bhatia",
      "Taha Kass-Hout"
    ],
    "affiliations": [],
    "summary": "Foundational models such as the Segment Anything Model$~$(SAM) are gaining traction in medical imaging segmentation, supporting multiple downstream tasks. However, such models are supervised in nature, still relying on large annotated datasets or prompts supplied by experts. Conventional techniques such as active learning to alleviate such limitations are limited in scope and still necessitate continuous human involvement and complex domain knowledge for label refinement or establishing reward ground truth. To address these challenges, we propose an enhanced Segment Anything Model (SAM) framework that utilizes annotation-efficient prompts generated in a fully unsupervised fashion, while still capturing essential semantic, location, and shape information through contrastive language-image pretraining and visual question answering. We adopt the direct preference optimization technique to design an optimal policy that enables the model to generate high-fidelity segmentations with simple ratings or rankings provided by a virtual annotator simulating the human annotation process. State-of-the-art performance of our framework in tasks such as lung segmentation, breast tumor segmentation, and organ segmentation across various modalities, including X-ray, ultrasound, and abdominal CT, justifies its effectiveness in low-annotation data scenarios.",
    "link": "/venue/Konwer_Enhancing_SAM_with_Efficient_Prompting_and_Preference_Optimization_for_Semi-supervised@CVPR2025@CVF",
    "published_date": null,
    "conference": "CVPR.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "DesignDiffusion: High-Quality Text-to-Design Image Generation with Diffusion Models",
    "authors": [
      "Zhendong Wang",
      "Jianmin Bao",
      "Shuyang Gu",
      "Dong Chen",
      "Wengang Zhou",
      "Houqiang Li"
    ],
    "affiliations": [],
    "summary": "In this paper, we present DesignDiffusion, a simple yet effective framework for the novel task of synthesizing design images from textual descriptions. A primary challenge lies in generating accurate and style-consistent textual and visual content. Existing works in a related task of visual text generation often focus on generating text within given specific regions, which limits the creativity of generation models, resulting in style or color inconsistencies between textual and visual elements if applied to design image generation. To address this issue, we propose an end-to-end, one-stage diffusion-based framework that avoids intricate components like position and layout modeling. Specifically, the proposed framework directly synthesizes textual and visual design elements from user prompts. It utilizes a distinctive character embedding derived from the visual text to enhance the input prompt, along with a character localization loss for enhanced supervision during text generation. Furthermore, we employ a self-play Direct Preference Optimization fine-tuning strategy to improve the quality and accuracy of the synthesized visual text. Extensive experiments demonstrate that DesignDiffusion achieves state-of-the-art performance in design image generation.",
    "link": "/venue/Wang_DesignDiffusion_High-Quality_Text-to-Design_Image_Generation_with_Diffusion_Models@CVPR2025@CVF",
    "published_date": null,
    "conference": "CVPR.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "Learning Audio-guided Video Representation with Gated Attention for Video-Text Retrieval",
    "authors": [
      "Boseung Jeong",
      "Jicheol Park",
      "Sungyeon Kim",
      "Suha Kwak"
    ],
    "affiliations": [],
    "summary": "Video-text retrieval, the task of retrieving videos based on a textual query or vice versa, is of paramount importance for video understanding and multimodal information retrieval. Recent methods in this area rely primarily on visual and textual features and often ignore audio, although it helps enhance overall comprehension of video content.Moreover, traditional models that incorporate audio blindly utilize the audio input regardless of whether it is useful or not, resulting in suboptimal video representation. To address these limitations, we propose a novel video-text retrieval framework, Audio-guided VIdeo representation learning with GATEd attention (AVIGATE), that effectively leverages audio cues through a gated attention mechanism that selectively filters out uninformative audio signals.In addition, we propose an adaptive margin-based contrastive loss to deal with the inherently unclear positive-negative relationship between video and text, which facilitates learning better video-text alignment.Our extensive experiments demonstrate that AVIGATE achieves state-of-the-art performance on all the public benchmarks.",
    "link": "/venue/Jeong_Learning_Audio-guided_Video_Representation_with_Gated_Attention_for_Video-Text_Retrieval@CVPR2025@CVF",
    "published_date": null,
    "conference": "CVPR.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "RandAR: Decoder-only Autoregressive Visual Generation in Random Orders",
    "authors": [
      "Ziqi Pang",
      "Tianyuan Zhang",
      "Fujun Luan",
      "Yunze Man",
      "Hao Tan",
      "Kai Zhang",
      "William T. Freeman",
      "Yu-Xiong Wang"
    ],
    "affiliations": [],
    "summary": "We introduce RandAR, a decoder-only visual autoregressive (AR) model capable of generatng images in arbitrary token orders. Unlike previous decoder-only AR models that rely on a predefined generation order, RandAR removes this inductive bias, unlocking new capabilities in decoder-only generation. Our essential design enabling random order is to insert a \"position instruction token\" before each image token to be predicted, representing the spatial location of the next image token. Trained on randomly permuted token sequences -- a more challenging task than fixed-order generation, RandAR achieves comparable performance to conventional raster-order counterpart. More importantly, decoder-only transformers trained from random orders acquire new capabilities. For the efficiency bottleneck of AR models, RandAR adopts parallel decoding with KV-Cache at inference time, enjoying 2.5x acceleration without sacrificing generation quality. Additionally, RandAR supports in-painting, outpainting and resolution extrapolation in a zero-shot manner.We hope RandAR inspires new directions for decoder-only visual generation models and broadens their applications across diverse scenarios.",
    "link": "/venue/Pang_RandAR_Decoder-only_Autoregressive_Visual_Generation_in_Random_Orders@CVPR2025@CVF",
    "published_date": null,
    "conference": "CVPR.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "MegaSaM: Accurate, Fast and Robust Structure and Motion from Casual Dynamic Videos",
    "authors": [
      "Zhengqi Li",
      "Richard Tucker",
      "Forrester Cole",
      "Qianqian Wang",
      "Linyi Jin",
      "Vickie Ye",
      "Angjoo Kanazawa",
      "Aleksander Holynski",
      "Noah Snavely"
    ],
    "affiliations": [],
    "summary": "We present a system that allows for accurate, fast, and robust estimation of camera parameters and depth maps from casual monocular videos of dynamic scenes. Most conventional structure from motion and monocular SLAM techniques assume input videos that feature predominantly static scenes with large amounts of parallax. Such methods tend to produce erroneous estimates in the absence of these conditions. Recent neural network based approaches attempt to overcome these challenges; however, such methods are either computationally expensive or brittle when run on dynamic videos with uncontrolled camera motion or unknown field of view. We demonstrate the surprising effectiveness of the deep visual SLAM framework, and with careful modifications to its training and inference schemes, this system can scale to real-world videos of complex dynamic scenes with unconstrained camera paths, including videos with little camera parallax. Extensive experiments on both synthetic and real videos demonstrate that our system is significantly more accurate and robust at camera pose and depth estimation when compared with prior and concurrent work, with faster or comparable running times.",
    "link": "/venue/Li_MegaSaM_Accurate_Fast_and_Robust_Structure_and_Motion_from_Casual@CVPR2025@CVF",
    "published_date": null,
    "conference": "CVPR.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "FoundationStereo: Zero-Shot Stereo Matching",
    "authors": [
      "Bowen Wen",
      "Matthew Trepte",
      "Joseph Aribido",
      "Jan Kautz",
      "Orazio Gallo",
      "Stan Birchfield"
    ],
    "affiliations": [],
    "summary": "Tremendous progress has been made in deep stereo matching to excel on benchmark datasets through per-domain fine-tuning. However, achieving strong zero-shot generalization — a hallmark of foundation models in other computer vision tasks — remains challenging for stereo matching. We introduce StereoAnything, a foundation model for stereo depth estimation designed to achieve strong zero-shot generalization. To this end, we first construct a large-scale (1M stereo pairs) synthetic training dataset featuring large diversity and high photorealism, followed by an automatic self-curation pipeline to remove ambiguous samples. We then design a number of network architecture components to enhance scalability, including a side-tuning feature backbone that adapts rich monocular priors from vision foundation models to mitigate the sim-to-real gap, and long-range context reasoning for effective cost volume filtering. Together, these components lead to strong robustness and accuracy across domains, establishing a new standard in zero-shot stereo depth estimation.",
    "link": "/venue/Wen_FoundationStereo_Zero-Shot_Stereo_Matching@CVPR2025@CVF",
    "published_date": null,
    "conference": "CVPR.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "Motion Prompting: Controlling Video Generation with Motion Trajectories",
    "authors": [
      "Daniel Geng",
      "Charles Herrmann",
      "Junhwa Hur",
      "Forrester Cole",
      "Serena Zhang",
      "Tobias Pfaff",
      "Tatiana Lopez-Guevara",
      "Yusuf Aytar",
      "Michael Rubinstein",
      "Chen Sun",
      "Oliver Wang",
      "Andrew Owens",
      "Deqing Sun"
    ],
    "affiliations": [],
    "summary": "Motion control is crucial for generating expressive and compelling video content; however, most existing video generation models rely mainly on text prompts for control, which struggle to capture the nuances of dynamic actions and temporal compositions. To this end, we train a video generation model conditioned on spatio-temporally sparse _or_ dense motion trajectories. In contrast to prior motion conditioning work, this flexible representation can encode any number of trajectories, object-specific or global scene motion, and temporally sparse motion; due to its flexibility we refer to this conditioning as _motion prompts_. While users may directly specify sparse trajectories, we also show how to translate high-level user requests into detailed, semi-dense motion prompts, a process we term _motion prompt expansion_. We demonstrate the versatility of our approach through various applications, including camera and object motion control, \"interacting\" with an image, motion transfer, and image editing. Our results showcase emergent behaviors, such as realistic physics, suggesting the potential of motion prompts for probing video models and interacting with future generative world models. Finally, we evaluate quantitatively, conduct a human study, and demonstrate strong performance.",
    "link": "/venue/Geng_Motion_Prompting_Controlling_Video_Generation_with_Motion_Trajectories@CVPR2025@CVF",
    "published_date": null,
    "conference": "CVPR.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "Language-Guided Image Tokenization for Generation",
    "authors": [
      "Kaiwen Zha",
      "Lijun Yu",
      "Alireza Fathi",
      "David A. Ross",
      "Cordelia Schmid",
      "Dina Katabi",
      "Xiuye Gu"
    ],
    "affiliations": [],
    "summary": "Image tokenization, the process of transforming raw image pixels into a compact low-dimensional latent representation, has proven crucial for scalable and efficient image generation. However, mainstream image tokenization methods generally have limited compression rates, making high-resolution image generation computationally expensive. To address this challenge, we propose to leverage language for efficient image tokenization, and we call our method Text-Conditioned Image Tokenization (TexTok). TexTok is a simple yet effective tokenization framework that leverages language to provide high-level semantics. By conditioning the tokenization process on descriptive text captions, TexTok allows the tokenization process to focusing on encoding fine-grained visual details into latent tokens, leading to enhanced reconstruction quality and higher compression rates. Compared to the conventional tokenizer without text conditioning, TexTok achieves average reconstruction FID improvements of 29.2\\% and 48.1\\% on ImageNet 256$\\times$256 and 512$\\times$512 benchmarks respectively, across varying number of tokens. These tokenization improvements consistently translate to 16.3\\% and 34.3\\% average improvements in generation FID. By simply replacing the tokenizer in Diffusion Transformer (DiT) with TexTok, our system can achieve 93.5$\\times$ inference speedup while still outperforming the original DiT using only 32 tokens on ImageNet-512. TexTok with a vanilla DiT generator achieves state-of-the-art FID scores of 1.46 and 1.62 on ImageNet-256 and -512 respectively. Furthermore, we demonstrate TexTok's superiority on the text-to-image generation task, effectively utilizing the off-the-shelf text captions in tokenization.",
    "link": "/venue/Zha_Language-Guided_Image_Tokenization_for_Generation@CVPR2025@CVF",
    "published_date": null,
    "conference": "CVPR.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "Generative Multimodal Pretraining with Discrete Diffusion Timestep Tokens",
    "authors": [
      "Kaihang Pan",
      "Wang Lin",
      "Zhongqi Yue",
      "Tenglong Ao",
      "Liyu Jia",
      "Wei Zhao",
      "Juncheng Li",
      "Siliang Tang",
      "Hanwang Zhang"
    ],
    "affiliations": [],
    "summary": "Recent endeavors in Multimodal Large Language Models (MLLMs) aim to unify visual comprehension and generation by combining LLM and diffusion models, the state-of-the-art in each task, respectively. Existing approaches rely on spatial visual tokens, where image patches are encoded and arranged according to a spatial order (e.g., raster scan). However, we show that spatial tokens lack the recursive structure inherent to languages, hence form an impossible language for LLM to master. In this paper, we build a proper visual language by leveraging diffusion timesteps to learn discrete, recursive visual tokens. Our proposed tokens recursively compensate for the progressive attribute loss in noisy images as timesteps increase, enabling the diffusion model to reconstruct the original image at any timestep. This approach allows us to effectively integrate the strengths of LLMs in autoregressive reasoning and diffusion models in precise image generation, achieving seamless multimodal comprehension and generation within a unified framework. Extensive experiments show that we achieve a new SOTA for multimodal comprehension and generation simultaneously compared with other MLLMs.",
    "link": "/venue/Pan_Generative_Multimodal_Pretraining_with_Discrete_Diffusion_Timestep_Tokens@CVPR2025@CVF",
    "published_date": null,
    "conference": "CVPR.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "Temporally Consistent Object-Centric Learning by Contrasting Slots",
    "authors": [
      "Anna Manasyan",
      "Maximilian Seitzer",
      "Filip Radovic",
      "Georg Martius",
      "Andrii Zadaianchuk"
    ],
    "affiliations": [],
    "summary": "Unsupervised object-centric learning from videos is a promising approach to extract structured representations from large, unlabeled collections of videos. To support downstream tasks like autonomous control, these representations must be both compositional and temporally consistent. Existing approaches based on recurrent processing often lack long-term stability across frames because their training objective does not enforce temporal consistency. In this work, we introduce a novel object-level temporal contrastive loss for video object-centric models that explicitly promotes temporal consistency. Our method significantly improves the temporal consistency of the learned object-centric representations, yielding more reliable video decompositions that facilitate challenging downstream tasks such as unsupervised object dynamics prediction. Furthermore, the inductive bias added by our loss strongly improves object discovery, leading to state-of-the-art results on both synthetic and real-world datasets, outperforming even weakly-supervised methods that leverage motion masks as additional cues.",
    "link": "/venue/Manasyan_Temporally_Consistent_Object-Centric_Learning_by_Contrasting_Slots@CVPR2025@CVF",
    "published_date": null,
    "conference": "CVPR.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "Reanimating Images using Neural Representations of Dynamic Stimuli",
    "authors": [
      "Jacob Yeung",
      "Andrew F. Luo",
      "Gabriel Sarch",
      "Margaret M. Henderson",
      "Deva Ramanan",
      "Michael J. Tarr"
    ],
    "affiliations": [],
    "summary": "While computer vision models have made incredible strides in static image recognition, they still do not match human performance in tasks that require the understanding of complex, dynamic motion. This is notably true for real-world scenarios where embodied agents face complex and motion-rich environments. Our approach leverages state-of-the-art video diffusion models to decouple static image representation from motion generation, enabling us to utilize fMRI brain activity for a deeper understanding of human responses to dynamic visual stimuli. Conversely, we also demonstrate that information about the brain's representation of motion can enhance the prediction of optical flow in artificial systems. Our novel approach leads to four main findings: (1) Visual motion, represented as fine-grained, object-level resolution optical flow, can be decoded from brain activity generated by participants viewing video stimuli; (2) Video encoders outperform image-based models in predicting video-driven brain activity; (3) Brain-decoded motion signals enable realistic video reanimation based only on the initial frame of the video; and (4) We extend prior work to achieve full video decoding from video-driven brain activity. This framework advances our understanding of how the brain represents spatial and temporal information in dynamic visual scenes. Our findings demonstrate the potential of combining brain imaging with video diffusion models for developing more robust and biologically-inspired computer vision systems.",
    "link": "/venue/Yeung_Reanimating_Images_using_Neural_Representations_of_Dynamic_Stimuli@CVPR2025@CVF",
    "published_date": null,
    "conference": "CVPR.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "Towards Universal Dataset Distillation via Task-Driven Diffusion",
    "authors": [
      "Ding Qi",
      "Jian Li",
      "Junyao Gao",
      "Shuguang Dou",
      "Ying Tai",
      "Jianlong Hu",
      "Bo Zhao",
      "Yabiao Wang",
      "Chengjie Wang",
      "Cairong Zhao"
    ],
    "affiliations": [],
    "summary": "Dataset distillation (DD) condenses key information from large-scale datasets into smaller synthetic datasets, reducing storage and computational costs for training networks. However, recent research has primarily focused on image classification tasks, with limited expansion to detection and segmentation. Two key challenges remain: (i) Task Optimization Heterogeneity, where existing methods focus on class-level information and fail to address the diverse needs of detection and segmentation and (ii) Inflexible Image Generation, where current generation methods rely on global updates for single-class targets and lack localized optimization for specific object regions.To address these challenges, we propose a universal dataset distillation framework, named UniDD, a task-driven diffusion model for diverse DD tasks, as illustrated in Fig.1. Our approach operates in two stages: Universal Task Knowledge Mining, which captures task-relevant information through task-specific proxy model training, and Universal Task-Driven Diffusion, where these proxies guide the diffusion process to generate task-specific synthetic images.Extensive experiments across ImageNet-1K, Pascal VOC, and MS COCO demonstrate that UniDD consistently outperforms state-of-the-art methods. In particular, on ImageNet-1K with IPC-10, UniDD surpasses previous diffusion-based methods by 6.1\\%, while also reducing deployment costs.",
    "link": "/venue/Qi_Towards_Universal_Dataset_Distillation_via_Task-Driven_Diffusion@CVPR2025@CVF",
    "published_date": null,
    "conference": "CVPR.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "Identifying and Mitigating Position Bias of Multi-image Vision-Language Models",
    "authors": [
      "Xinyu Tian",
      "Shu Zou",
      "Zhaoyuan Yang",
      "Jing Zhang"
    ],
    "affiliations": [],
    "summary": "The evolution of Large Vision-Language Models (LVLMs) has progressed from single-image understanding to multi-image reasoning. Despite this advancement, our findings indicate that LVLMs struggle to robustly utilize information across multiple images, with predictions significantly affected by the alteration of image positions. To further explore this issue, we introduce Position-wise Question Answering (PQA), a meticulously designed task to quantify reasoning capabilities at each position. Our analysis reveals a pronounced position bias in LVLMs: open-source models excel in reasoning with images positioned later but underperform with those in the middle or at the beginning, while proprietary models like GPT-4o show improved comprehension for images at the beginning and end but struggle with those in the middle. Motivated by these insights, we propose SoFt Attention (SoFA), a simple, training-free approach that mitigates this bias by employing linear interpolation between inter-image causal attention and bidirectional counterparts. Experimental results demonstrate that SoFA effectively reduces position bias and significantly enhances the reasoning performance of existing LVLMs.",
    "link": "/venue/Tian_Identifying_and_Mitigating_Position_Bias_of_Multi-image_Vision-Language_Models@CVPR2025@CVF",
    "published_date": null,
    "conference": "CVPR.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Vision-Language Models",
    "authors": [
      "Matt Deitke",
      "Christopher Clark",
      "Sangho Lee",
      "Rohun Tripathi",
      "Yue Yang",
      "Jae Sung Park",
      "Mohammadreza Salehi",
      "Niklas Muennighoff",
      "Kyle Lo",
      "Luca Soldaini",
      "Jiasen Lu",
      "Taira Anderson",
      "Erin Bransom",
      "Kiana Ehsani",
      "Huong Ngo",
      "YenSung Chen",
      "Ajay Patel",
      "Mark Yatskar",
      "Chris Callison-Burch",
      "Andrew Head",
      "Rose Hendrix",
      "Favyen Bastani",
      "Eli VanderBilt",
      "Nathan Lambert",
      "Yvonne Chou",
      "Arnavi Chheda",
      "Jenna Sparks",
      "Sam Skjonsberg",
      "Michael Schmitz",
      "Aaron Sarnat",
      "Byron Bischoff",
      "Pete Walsh",
      "Chris Newell",
      "Piper Wolters",
      "Tanmay Gupta",
      "Kuo-Hao Zeng",
      "Jon Borchardt",
      "Dirk Groeneveld",
      "Crystal Nam",
      "Sophie Lebrecht",
      "Caitlin Wittlif",
      "Carissa Schoenick",
      "Oscar Michel",
      "Ranjay Krishna",
      "Luca Weihs",
      "Noah A. Smith",
      "Hannaneh Hajishirzi",
      "Ross Girshick",
      "Ali Farhadi",
      "Aniruddha Kembhavi"
    ],
    "affiliations": [],
    "summary": "Today's most advanced vision-language models (VLMs) remain proprietary. The strongest open-weight models rely heavily on synthetic data from proprietary VLMs to achieve good performance, effectively distilling these closed VLMs into open ones. As a result, the community has been missing foundational knowledge about how to build performant VLMs from scratch. We present \\textbf{Molmo}, a new family of VLMs that are state-of-the-art in their class of openness. Our key contribution is a collection of new datasets, including a dataset of highly detailed image captions for pre-training called \\textbf{PixMo}, a free-form image Q\\&A dataset for fine-tuning, and an innovative 2D pointing dataset, all collected without the use of external VLMs. The success of our approach relies on careful modeling choices, a well-tuned training pipeline, and, most critically, the quality of our newly collected datasets. Our best-in-class 72B model not only outperforms others in the class of open weight and data models, but also outperforms larger proprietary models including Claude 3.5 Sonnet, and Gemini 1.5 Pro and Flash, second only to GPT-4o based on both academic benchmarks and on a large human evaluation. Our model weights, new datasets, and source code will all be released.",
    "link": "/venue/Deitke_Molmo_and_PixMo_Open_Weights_and_Open_Data_for_State-of-the-Art@CVPR2025@CVF",
    "published_date": null,
    "conference": "CVPR.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "The PanAf-FGBG Dataset: Understanding the Impact of Backgrounds in Wildlife Behaviour Recognition",
    "authors": [
      "Otto Brookes",
      "Maksim Kukushkin",
      "Majid Mirmehdi",
      "Colleen Stephens",
      "Paula Dieguez",
      "Thurston C. Hicks",
      "Sorrel Jones",
      "Kevin Lee",
      "Maureen S. McCarthy",
      "Amelia Meier",
      "Emmanuelle Normand",
      "Erin G. Wessling",
      "Roman M. Wittig",
      "Kevin Langergraber",
      "Klaus Zuberbühler",
      "Lukas Boesch",
      "Thomas Schmid",
      "Mimi Arandjelovic",
      "Hjalmar Kühl",
      "Tilo Burghardt"
    ],
    "affiliations": [],
    "summary": "Computer vision analysis of camera trap video footage is essential for wildlife conservation, as captured behaviours offer some of the earliest indicators of changes in population health. Recently, several high-impact animal behaviour datasets and methods have been introduced to encourage their use; however, the role of behaviour-correlated background information and its significant effect on out-of-distribution generalisation remain unexplored. In response, we present the PanAf-FGBG dataset, featuring 20 hours of wild chimpanzee behaviours, recorded at over 350 individual camera locations. Uniquely, it pairs every video with a chimpanzee (referred to as a foreground video) with a corresponding background video (with no chimpanzee) from the same camera location. We present two views of the dataset: one with overlapping camera locations and one with disjoint locations. This setup enables, for the first time, direct evaluation of in-distribution and out-of-distribution conditions, and for the impact of backgrounds on behaviour recognition models to be quantified. All clips come with rich behavioural annotations and metadata including unique camera IDs and detailed textual scene descriptions. Additionally, we establish several baselines and present a highly effective latent-space normalisation technique that boosts out-of-distribution performance by +5.42\\% mAP for convolutional and +3.75\\% mAP for transformer-based models. Finally, we provide an in-depth analysis on the role of backgrounds in out-of-distribution behaviour recognition, including the so far unexplored impact of background durations (i.e., the count of background frames within foreground videos). The full dataset, baseline models, and weights will be available at `anonymous'.",
    "link": "/venue/Brookes_The_PanAf-FGBG_Dataset_Understanding_the_Impact_of_Backgrounds_in_Wildlife@CVPR2025@CVF",
    "published_date": null,
    "conference": "CVPR.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "Opportunistic Single-Photon Time of Flight",
    "authors": [
      "Sotiris Nousias",
      "Mian Wei",
      "Howard Xiao",
      "Maxx Wu",
      "Shahmeer Athar",
      "Kevin J. Wang",
      "Anagh Malik",
      "David A. Barmherzig",
      "David B. Lindell",
      "Kyros N. Kutulakos"
    ],
    "affiliations": [],
    "summary": "Scattered light from pulsed lasers is increasingly part of our ambient illumination, as many devices rely on them for active 3D sensing. In this work, we ask: can these “ambient” light signals be detected and leveraged for passive 3D vision? We show that pulsed lasers, despite being weak and fluctuating at MHz to GHz frequencies, leave a distinctive sinc comb pattern in the temporal frequency domain of incident flux that is specific to each laser and invariant to the scene. This enables their passive detection and analysis with a free-running SPAD camera, even when they are unknown, asynchronous, out of sight, and emitting concurrently. We show how to synchronize with such lasers computationally, characterize their pulse emissions, separate their contributions, and—if many are present—localize them in 3D and recover a depth map of the camera’s field of view. We use our camera prototype to demonstrate (1) a first-of-its-kind visualization of asynchronously propagating light pulses from multiple lasers through the same scene, (2) passive estimation of a laser’s MHz-scale pulse repetition frequency with mHz precision, and (3) mm-scale 3D imaging over room-scale distances by passively harvesting photons from two or more out-of-view lasers.",
    "link": "/venue/Nousias_Opportunistic_Single-Photon_Time_of_Flight@CVPR2025@CVF",
    "published_date": null,
    "conference": "CVPR.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "Token Activation Map to Visually Explain Multimodal LLMs",
    "authors": [
      "Yi Li",
      "Hualiang Wang",
      "Xinpeng Ding",
      "Haonan Wang",
      "Xiaomeng Li"
    ],
    "affiliations": [],
    "summary": "Multimodal large language models (MLLMs) are broadly empowering various fields. Despite their advancements, the explainability of MLLMs remains less explored, hindering deeper understanding, model credibility, and effective visualization. Unlike conventional vision models (e.g., CNNs, ViTs, CLIP) that produce a single output, MLLMs generate sequences of tokens progressively, where each generated token depends on the previous context. Therefore, earlier context tokens can introduce redundant activations that interfere with the explanation of later tokens beyond their original information. Existing studies often overlook this issue, but our observations reveal that these redundant correlations can significantly hurt the reliability of explanations. To address this, we propose an estimated causal inference method to mitigate the interference of context to achieve high-quality MLLM explanation, with a novel rank Gaussian filter to further reduce activation noises. We term this method Token Activation Map (TAM) to highlight the consideration of interactions between tokens. TAM also indicates that it excels at explaining multiple tokens of MLLM, which is different from the Class Activation Map (CAM) for a single prediction. Our TAM method significantly outperforms existing SoTA methods, showcasing high-quality visualization results that can be utilized for various scenarios, such as object localization, failure case analysis, video visualization, MLLMs visual comparison, and model understanding (e.g., color, shape, action, location, visual reasoning, multi-turn conversation, etc). The code is available at github.com/xmed-lab/TAM.",
    "link": "/venue/Li_Token_Activation_Map_to_Visually_Explain_Multimodal_LLMs@ICCV2025@CVF",
    "published_date": null,
    "conference": "ICCV.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "NullSwap: Proactive Identity Cloaking Against Deepfake Face Swapping",
    "authors": [
      "Tianyi Wang",
      "Shuaicheng Niu",
      "Harry Cheng",
      "Xiao Zhang",
      "Yinglong Wang"
    ],
    "affiliations": [],
    "summary": "Suffering from performance bottlenecks in passively detecting high-quality Deepfake images due to the advancement of generative models, proactive perturbations offer a promising approach to disabling Deepfake manipulations by inserting signals into benign images. However, existing proactive perturbation approaches remain unsatisfactory in several aspects: 1) visual degradation due to direct element-wise addition; 2) limited effectiveness against face swapping manipulation; 3) unavoidable reliance on white- and grey-box settings to involve generative models during training. In this study, we analyze the essence of Deepfake face swapping and argue the necessity of protecting source identities rather than target images, and we propose NullSwap, a novel proactive defense approach that cloaks source image identities and nullifies face swapping under a pure black-box scenario. We design an Identity Extraction module to obtain facial identity features from the source image, while a Perturbation Block is then devised to generate identity-guided perturbations accordingly. Meanwhile, a Feature Block extracts shallow-level image features, which are then fused with the perturbation in the Cloaking Block for image reconstruction. Furthermore, to ensure adaptability across different identity extractors in face swapping algorithms, we propose Dynamic Loss Weighting to adaptively balance identity losses. Experiments demonstrate the outstanding ability of our approach to fool various identity recognition models, outperforming state-of-the-art proactive perturbations in preventing face swapping models from generating images with correct source identities.",
    "link": "/venue/Wang_NullSwap_Proactive_Identity_Cloaking_Against_Deepfake_Face_Swapping@ICCV2025@CVF",
    "published_date": null,
    "conference": "ICCV.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "CorrCLIP: Reconstructing Patch Correlations in CLIP for Open-Vocabulary Semantic Segmentation",
    "authors": [
      "Dengke Zhang",
      "Fagui Liu",
      "Quan Tang"
    ],
    "affiliations": [],
    "summary": "Open-vocabulary semantic segmentation aims to assign semantic labels to each pixel without being constrained by a predefined set of categories. While Contrastive Language-Image Pre-training (CLIP) excels in zero-shot classification, it struggles to align image patches with category embeddings because of its incoherent patch correlations. This study reveals that inter-class correlations are the main reason for impairing CLIP's segmentation performance. Accordingly, we propose CorrCLIP, which reconstructs the scope and value of patch correlations. Specifically, CorrCLIP leverages the Segment Anything Model (SAM) to define the scope of patch interactions, reducing inter-class correlations. To mitigate the problem that SAM-generated masks may contain patches belonging to different classes, CorrCLIP incorporates self-supervised models to compute coherent similarity values, suppressing the weight of inter-class correlations. Additionally, we introduce two additional branches to strengthen patch features' spatial details and semantic representation. Finally, we update segmentation maps with SAM-generated masks to improve spatial consistency. Based on the improvement across patch correlations, feature representations, and segmentation maps, CorrCLIP achieves superior performance across eight benchmarks. Codes are available at: https://github.com/zdk258/CorrCLIP.",
    "link": "/venue/Zhang_CorrCLIP_Reconstructing_Patch_Correlations_in_CLIP_for_Open-Vocabulary_Semantic_Segmentation@ICCV2025@CVF",
    "published_date": null,
    "conference": "ICCV.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "MaskControl: Spatio-Temporal Control for Masked Motion Synthesis",
    "authors": [
      "Ekkasit Pinyoanuntapong",
      "Muhammad Saleem",
      "Korrawe Karunratanakul",
      "Pu Wang",
      "Hongfei Xue",
      "Chen Chen",
      "Chuan Guo",
      "Junli Cao",
      "Jian Ren",
      "Sergey Tulyakov"
    ],
    "affiliations": [],
    "summary": "Recent advances in motion diffusion models have enabled spatially controllable text-to-motion generation. However, these models struggle to achieve high-precision control while maintaining high-quality motion generation. To address these challenges, we propose MaskControl, the first approach to introduce controllability to the generative masked motion model. Our approach introduces two key innovations. First, Logits Regularizer implicitly perturbs logits at training time to align the distribution of motion tokens with the controlled joint positions, while regularizing the categorical token prediction to ensure high-fidelity generation. Second, Logit Optimization explicitly optimizes the predicted logits during inference time, directly reshaping the token distribution that forces the generated motion to accurately align with the controlled joint positions. Moreover, we introduce Differentiable Expectation Sampling (DES) to combat the non-differential distribution sampling process encountered by logits regularizer and optimization. Extensive experiments demonstrate that MaskControl outperforms state-of-the-art methods, achieving superior motion quality (FID decreases by 77%) and higher control precision (average error 0.91 vs. 1.08). Additionally, MaskControl enables diverse applications, including any-joint-any-frame control, body-part timeline control, and zero-shot objective control. Video visualization can be found at https://anonymous-ai-agent.github.io/CAM",
    "link": "/venue/Pinyoanuntapong_MaskControl_Spatio-Temporal_Control_for_Masked_Motion_Synthesis@ICCV2025@CVF",
    "published_date": null,
    "conference": "ICCV.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "Removing Cost Volumes from Optical Flow Estimators",
    "authors": [
      "Simon Kiefhaber",
      "Stefan Roth",
      "Simone Schaub-Meyer"
    ],
    "affiliations": [],
    "summary": "Cost volumes are used in every modern optical flow estimator, but due to their computational and space complexity, they are often a limiting factor regarding both processing speed and the resolution of input frames. Motivated by our empirical observation that cost volumes lose their importance once all other network parts of, e.g., a RAFT-based pipeline have been sufficiently trained, we introduce a training strategy that allows removing the cost volume from optical flow estimators throughout training. This leads to significantly improved inference speed and reduced memory requirements. Using our training strategy, we create three different models covering different compute budgets. Our most accurate model reaches state-of-the-art accuracy while being 1.2xfaster and having a 6xlower memory footprint than comparable models; our fastest model is capable of processing Full HD frames at 20\\,\\mathrm FPS using only 500\\,\\mathrm MB of GPU memory.",
    "link": "/venue/Kiefhaber_Removing_Cost_Volumes_from_Optical_Flow_Estimators@ICCV2025@CVF",
    "published_date": null,
    "conference": "ICCV.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "LaRender: Training-Free Occlusion Control in Image Generation via Latent Rendering",
    "authors": [
      "Xiaohang Zhan",
      "Dingming Liu"
    ],
    "affiliations": [],
    "summary": "We propose a novel training-free image generation algorithm that precisely controls the occlusion relationships between objects in an image. Existing image generation methods typically rely on prompts to influence occlusion, which often lack precision. While layout-to-image methods provide control over object locations, they fail to address occlusion relationships explicitly. Given a pre-trained image diffusion model, our method leverages volume rendering principles to \"render\" the scene in latent space, guided by occlusion relationships and the estimated transmittance of objects. This approach does not require retraining or fine-tuning the image diffusion model, yet it enables accurate occlusion control due to its physics-grounded foundation. In extensive experiments, our method significantly outperforms existing approaches in terms of occlusion accuracy. Furthermore, we demonstrate that by adjusting the opacities of objects or concepts during rendering, our method can achieve a variety of effects, such as altering the transparency of objects, the density of mass (e.g., forests), the concentration of particles (e.g., rain, fog), the intensity of light, and the strength of lens effects, etc.",
    "link": "/venue/Zhan_LaRender_Training-Free_Occlusion_Control_in_Image_Generation_via_Latent_Rendering@ICCV2025@CVF",
    "published_date": null,
    "conference": "ICCV.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "Automated Model Evaluation for Object Detection via Prediction Consistency and Reliability",
    "authors": [
      "Seungju Yoo",
      "Hyuk Kwon",
      "Joong-Won Hwang",
      "Kibok Lee"
    ],
    "affiliations": [],
    "summary": "Recent advances in computer vision have made training object detectors more efficient and effective; however, assessing their performance in real-world applications still relies on costly manual annotation. To address this limitation, we develop an automated model evaluation (AutoEval) framework for object detection. We propose Prediction Consistency and Reliability (PCR), which leverages the multiple candidate bounding boxes that conventional detectors generate before non-maximum suppression (NMS). PCR estimates detection performance without ground-truth labels by jointly measuring 1) the spatial consistency between boxes before and after NMS, and 2) the reliability of the retained boxes via the confidence scores of overlapping boxes. For a more realistic and scalable evaluation, we construct a meta-dataset by applying image corruptions of varying severity. Experimental results demonstrate that PCR yields more accurate performance estimates than existing AutoEval methods, and the proposed meta-dataset covers a wider range of detection performance. The code is available at https://github.com/YonseiML/autoeval-det.",
    "link": "/venue/Yoo_Automated_Model_Evaluation_for_Object_Detection_via_Prediction_Consistency_and@ICCV2025@CVF",
    "published_date": null,
    "conference": "ICCV.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "LoftUp: Learning a Coordinate-Based Feature Upsampler for Vision Foundation Models",
    "authors": [
      "Haiwen Huang",
      "Anpei Chen",
      "Volodymyr Havrylov",
      "Andreas Geiger",
      "Dan Zhang"
    ],
    "affiliations": [],
    "summary": "Vision foundation models (VFMs) such as DINOv2 and CLIP have achieved impressive results on various downstream tasks, but their limited feature resolution hampers performance in applications requiring pixel-level understanding. Feature upsampling offers a promising direction to address this challenge. In this work, we identify two critical factors for enhancing feature upsampling: the upsampler architecture and the training objective. For the upsampler architecture, we introduce a coordinate-based cross-attention transformer that integrates the high-resolution images with coordinates and low-resolution VFM features to generate sharp, high-quality features. For the training objective, we propose constructing high-resolution pseudo-groundtruth features by leveraging class-agnostic masks and self-distillation. Our approach effectively captures fine-grained details and adapts flexibly to various input and feature resolutions. Through experiments, we demonstrate that our approach significantly outperforms existing feature upsampling techniques across various downstream tasks. Our code is released at https://github.com/andrehuang/loftup.",
    "link": "/venue/Huang_LoftUp_Learning_a_Coordinate-Based_Feature_Upsampler_for_Vision_Foundation_Models@ICCV2025@CVF",
    "published_date": null,
    "conference": "ICCV.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "LOTS of Fashion! Multi-Conditioning for Image Generation via Sketch-Text Pairing",
    "authors": [
      "Federico Girella",
      "Davide Talon",
      "Ziyue Liu",
      "Zanxi Ruan",
      "Yiming Wang",
      "Marco Cristani"
    ],
    "affiliations": [],
    "summary": "Fashion design is a complex creative process that blends visual and textual expressions. Designers convey ideas through sketches, which define spatial structure and design elements, and textual descriptions, capturing material, texture, and stylistic details. In this paper, we present LOcalized Text and Sketch for fashion image generation (LOTS), an approach for compositional sketch-text based generation of complete fashion outlooks. LOTS leverages a global description with paired localized sketch + text information for conditioning and introduces a novel step-based merging strategy for diffusion adaptation. First, a Modularized Pair-Centric representation encodes sketches and text into a shared latent space while preserving independent localized features; then, a Diffusion Pair Guidance phase integrates both local and global conditioning via attention-based guidance within the diffusion model's multi-step denoising process. To validate our method, we build on Fashionpedia to release Sketchy, the first fashion dataset where multiple text-sketch pairs are provided per image. Quantitative results show LOTS achieves state-of-the-art image generation performance on both global and localized metrics, while qualitative examples and a human evaluation study highlight its unprecedented level of design customization.",
    "link": "/venue/Girella_LOTS_of_Fashion_Multi-Conditioning_for_Image_Generation_via_Sketch-Text_Pairing@ICCV2025@CVF",
    "published_date": null,
    "conference": "ICCV.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "Towards a Unified Copernicus Foundation Model for Earth Vision",
    "authors": [
      "Yi Wang",
      "Zhitong Xiong",
      "Chenying Liu",
      "Adam J. Stewart",
      "Thomas Dujardin",
      "Nikolaos Ioannis Bountos",
      "Angelos Zavras",
      "Franziska Gerken",
      "Ioannis Papoutsis",
      "Laura Leal-Taixé",
      "Xiao Xiang Zhu"
    ],
    "affiliations": [],
    "summary": "Advances in Earth observation (EO) foundation models have unlocked the potential of big satellite data to learn generic representations from space, benefiting a wide range of downstream applications crucial to our planet. However, most existing efforts remain limited to fixed spectral sensors, focus solely on the Earth's surface, and overlook valuable metadata beyond imagery. In this work, we take a step towards next-generation EO foundation models with three key components: 1) Copernicus-Pretrain, a massive-scale pretraining dataset that integrates 18.7M aligned images from all major Copernicus Sentinel missions, spanning from the Earth's surface to its atmosphere; 2) Copernicus-FM, a unified foundation model capable of processing any spectral or non-spectral sensor modality using extended dynamic hypernetworks and flexible metadata encoding; and 3) Copernicus-Bench, a systematic evaluation benchmark with 15 hierarchical downstream tasks ranging from preprocessing to specialized applications for each Sentinel mission. Our dataset, model, and benchmark greatly improve the scalability, versatility, and multimodal adaptability of EO foundation models, while also creating new opportunities to connect EO, weather, and climate research. Codes at https://github.com/zhu-xlab/Copernicus-FM.",
    "link": "/venue/Wang_Towards_a_Unified_Copernicus_Foundation_Model_for_Earth_Vision@ICCV2025@CVF",
    "published_date": null,
    "conference": "ICCV.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "E-SAM: Training-Free Segment Every Entity Model",
    "authors": [
      "Weiming Zhang",
      "Dingwen Xiao",
      "Lei Chen",
      "Lin Wang"
    ],
    "affiliations": [],
    "summary": "Entity Segmentation (ES) aims at identifying and segmenting distinct entities within an image without the need for predefined class labels. This characteristic makes ES well-suited to open-world applications with adaptation to diverse and dynamically changing environments, where new and previously unseen entities may appear frequently. Existing ES methods either require large annotated datasets or high training costs, limiting their scalability and adaptability. Recently, the Segment Anything Model (SAM), especially in its Automatic Mask Generation (AMG) mode, has shown potential for holistic image segmentation. However, it struggles with over-segmentation and under-segmentation, making it less effective for ES. In this paper, we introduce E-SAM, a novel training-free framework that exhibits exceptional ES capability. Specifically, we first propose Multi-level Mask Generation (MMG) that hierarchically processes SAM's AMG outputs to generate reliable object-level masks while preserving fine details at other levels. Entity-level Mask Refinement (EMR) then refines these object-level masks into accurate entity-level masks. That is, it separates overlapping masks to address the redundancy issues inherent in SAM's outputs and merges similar masks by evaluating entity-level consistency. Lastly, Under-Segmentation Refinement (USR) addresses under-segmentation by generating additional high-confidence masks fused with EMR outputs to produce the final ES map. These three modules are seamlessly optimized to achieve the best ES without additional training overhead. Extensive experiments demonstrate that E-SAM achieves state-of-the-art performance compared to prior ES methods, demonstrating a significant improvement by +30.1 on benchmark metrics.",
    "link": "/venue/Zhang_E-SAM_Training-Free_Segment_Every_Entity_Model@ICCV2025@CVF",
    "published_date": null,
    "conference": "ICCV.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "Forecasting Continuous Non-Conservative Dynamical Systems in SO(3)",
    "authors": [
      "Lennart Bastian",
      "Mohammad Rashed",
      "Nassir Navab",
      "Tolga Birdal"
    ],
    "affiliations": [],
    "summary": "Modeling the rotation of moving objects is a fundamental task in computer vision, yet SO(3) extrapolation still presents numerous challenges: (1) unknown quantities such as the moment of inertia complicate dynamics, (2) the presence of external forces and torques can lead to non-conservative kinematics, and (3) estimating evolving state trajectories under sparse, noisy observations requires robustness. We propose modeling trajectories of noisy pose estimates on the manifold of 3D rotations in a physically and geometrically meaningful way by leveraging Neural Controlled Differential Equations guided with SO(3) Savitzky-Golay paths. Existing extrapolation methods often rely on energy conservation or constant velocity assumptions, limiting their applicability in real-world scenarios involving non-conservative forces. In contrast, our approach is agnostic to energy and momentum conservation while being robust to input noise, making it applicable to complex, non-inertial systems. Our approach is easily integrated as a module in existing pipelines and generalizes well to trajectories with unknown physical parameters. By learning to approximate object dynamics from noisy states during training, our model attains robust extrapolation capabilities in simulation and various real-world settings.",
    "link": "/venue/Bastian_Forecasting_Continuous_Non-Conservative_Dynamical_Systems_in_SO3@ICCV2025@CVF",
    "published_date": null,
    "conference": "ICCV.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "FixTalk: Taming Identity Leakage for High-Quality Talking Head Generation in Extreme Cases",
    "authors": [
      "Shuai Tan",
      "Bill Gong",
      "Bin Ji",
      "Ye Pan"
    ],
    "affiliations": [],
    "summary": "Talking head generation is gaining significant importance across various domains, with a growing demand for high-quality rendering. However, existing methods often suffer from identity leakage (IL) and rendering artifacts (RA), particularly in extreme cases. Through an in-depth analysis of previous approaches, we identify two key insights: (1) IL arises from identity information embedded within motion features, and (2) this identity information can be leveraged to address RA. Building on these findings, this paper introduces FixTalk, a novel framework designed to simultaneously resolve both issues for high-quality talking head generation. Firstly, we propose an Enhanced Motion Indicator (EMI) to effectively decouple identity information from motion features, mitigating the impact of IL on generated talking heads. To address RA, we introduce an Enhanced Detail Indicator (EDI), which utilizes the leaked identity information to supplement missing details, thus fixing the artifacts. Extensive experiments demonstrate that FixTalk effectively mitigates IL and RA, achieving superior performance compared to state-of-the-art methods.",
    "link": "/venue/Tan_FixTalk_Taming_Identity_Leakage_for_High-Quality_Talking_Head_Generation_in@ICCV2025@CVF",
    "published_date": null,
    "conference": "ICCV.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "TrajectoryCrafter: Redirecting Camera Trajectory for Monocular Videos via Diffusion Models",
    "authors": [
      "Mark Yu",
      "Wenbo Hu",
      "Jinbo Xing",
      "Ying Shan"
    ],
    "affiliations": [],
    "summary": "We present TrajectoryCrafter, a novel approach to redirect camera trajectories for monocular videos. By disentangling deterministic view transformations from stochastic content generation, our method achieves precise control over user-specified camera trajectories. We propose a novel dual-stream conditional video diffusion model that concurrently integrates point cloud renders and source videos as conditions, ensuring accurate view transformations and coherent 4D content generation. Instead of leveraging scarce multi-view videos, we curate a hybrid training dataset combining web-scale monocular videos with static multi-view datasets, by our innovative double-reprojection strategy, significantly fostering robust generalization across diverse scenes. Extensive evaluations on multi-view and large-scale monocular videos demonstrate the superior performance of our method. Code and pre-trained model will be released.",
    "link": "/venue/Yu_TrajectoryCrafter_Redirecting_Camera_Trajectory_for_Monocular_Videos_via_Diffusion_Models@ICCV2025@CVF",
    "published_date": null,
    "conference": "ICCV.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "FlowEdit: Inversion-Free Text-Based Editing Using Pre-Trained Flow Models",
    "authors": [
      "Vladimir Kulikov",
      "Matan Kleiner",
      "Inbar Huberman-Spiegelglas",
      "Tomer Michaeli"
    ],
    "affiliations": [],
    "summary": "Editing real images using a pre-trained text-to-image (T2I) diffusion/flow model often involves inverting the image into its corresponding noise map. However, inversion by itself is typically insufficient for obtaining satisfactory results, and therefore many methods additionally intervene in the sampling process. Such methods achieve improved results but are not seamlessly transferable between model architectures. Here, we introduce FlowEdit, a text-based editing method for pre-trained T2I flow models, which is inversion-free, optimization-free and model agnostic. Our method constructs an ODE that directly maps between the source and target distributions (corresponding to the source and target text prompts) and achieves a lower transport cost than the inversion approach. This leads to state-of-the-art results, as we illustrate with Stable Diffusion 3 and FLUX.",
    "link": "/venue/Kulikov_FlowEdit_Inversion-Free_Text-Based_Editing_Using_Pre-Trained_Flow_Models@ICCV2025@CVF",
    "published_date": null,
    "conference": "ICCV.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "Learning Visual Hierarchies in Hyperbolic Space for Image Retrieval",
    "authors": [
      "Ziwei Wang",
      "Sameera Ramasinghe",
      "Chenchen Xu",
      "Julien Monteil",
      "Loris Bazzani",
      "Thalaiyasingam Ajanthan"
    ],
    "affiliations": [],
    "summary": "Structuring latent representations in a hierarchical manner enables models to learn patterns at multiple levels of abstraction. However, most prevalent image understanding models focus on visual similarity, and learning visual hierarchies is relatively unexplored. In this work, for the first time, we introduce a learning paradigm that can encode user-defined multi-level complex visual hierarchies in hyperbolic space without requiring explicit hierarchical labels. As a concrete example, first, we define a part-based image hierarchy using object-level annotations within and across images. Then, we introduce an approach to enforce the hierarchy using contrastive loss with pairwise entailment metrics. Finally, we discuss new evaluation metrics to effectively measure hierarchical image retrieval. Encoding these complex relationships ensures that the learned representations capture semantic and structural information that transcends mere visual similarity. Experiments in part-based image retrieval show significant improvements in hierarchical retrieval tasks, demonstrating the capability of our model in capturing visual hierarchies.",
    "link": "/venue/Wang_Learning_Visual_Hierarchies_in_Hyperbolic_Space_for_Image_Retrieval@ICCV2025@CVF",
    "published_date": null,
    "conference": "ICCV.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "Dynamic Typography: Bringing Text to Life via Video Diffusion Prior",
    "authors": [
      "Zichen Liu",
      "Yihao Meng",
      "Hao Ouyang",
      "Yue Yu",
      "Bolin Zhao",
      "Daniel Cohen-Or",
      "Huamin Qu"
    ],
    "affiliations": [],
    "summary": "Text animation serves as an expressive medium, transforming static communication into dynamic experiences by infusing words with motion to evoke emotions, emphasize meanings, and construct compelling narratives. Crafting animations that are semantically aware poses significant challenges, demanding expertise in graphic design and animation. We present an automated text animation scheme, termed \"Dynamic Typography\", which deforms letters to convey semantic meaning and infuses them with vibrant movements based on user prompts. The animation is represented by a canonical field that aggregates the semantic content in a canonical shape and a deformation field that applies per-frame motion to deform the canonical shape. Two fields are jointly optimized by the priors from a large pretrained text-to-video diffusion model using score-distillation loss with designed regularization, encouraging the video coherence with the intended textual concept while maintaining legibility and structural integrity throughout the animation process. We demonstrate the generalizability of our approach across various text-to-video models and highlight the superiority of our methodology over baselines. Through quantitative and qualitative evaluations, we demonstrate the effectiveness of our framework in generating coherent text animations that faithfully interpret user prompts while maintaining readability.",
    "link": "/venue/Liu_Dynamic_Typography_Bringing_Text_to_Life_via_Video_Diffusion_Prior@ICCV2025@CVF",
    "published_date": null,
    "conference": "ICCV.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "Uncalibrated Structure from Motion on a Sphere",
    "authors": [
      "Jonathan Ventura",
      "Viktor Larsson",
      "Fredrik Kahl"
    ],
    "affiliations": [],
    "summary": "Spherical motion is a special case of camera motion where the camera moves on the imaginary surface of a sphere with the optical axis normal to the surface. Common sources of spherical motion are a person capturing a stereo panorama with a phone held in an outstretched hand, or a hemi-spherical camera rig used for multi-view scene capture. However, traditional structure-from-motion pipelines tend to fail on spherical camera motion sequences, especially when the camera is facing outward. Building upon prior work addressing the calibrated case, we explore uncalibrated reconstruction from spherical motion, assuming a fixed but unknown focal length parameter. We show that, although two-view spherical motion is always a critical case, self-calibration is possible from three or more views. Through analysis of the relationship between focal length and spherical relative pose, we devise a global structure-from-motion approach for uncalibrated reconstruction. We demonstrate the effectiveness of our approach on real-world captures in various settings, even when the camera motion deviates from perfect spherical motion. Code and data for our method are available at https://github.com/jonathanventura/spherical-sfm.",
    "link": "/venue/Ventura_Uncalibrated_Structure_from_Motion_on_a_Sphere@ICCV2025@CVF",
    "published_date": null,
    "conference": "ICCV.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "Differentiable Room Acoustic Rendering with Multi-View Vision Priors",
    "authors": [
      "Derong Jin",
      "Ruohan Gao"
    ],
    "affiliations": [],
    "summary": "An immersive acoustic experience enabled by spatial audio is just as crucial as the visual aspect in creating realistic virtual environments. However, existing methods for room impulse response estimation rely either on data-demanding learning-based models or computationally expensive physics-based modeling. In this work, we introduce Audio-Visual Differentiable Room Acoustic Rendering (AV-DAR), a framework that leverages visual cues extracted from multi-view images and acoustic beam tracing for physics-based room acoustic rendering. Experiments across six real-world environments from two datasets demonstrate that our multimodal, physics-based approach is efficient, interpretable, and accurate, significantly outperforming a series of prior methods. Notably, on the Real Acoustic Field dataset, AV-DAR achieves comparable performance to models trained on 10 times more data while delivering relative gains ranging from 16.6% to 50.9% when trained at the same scale.",
    "link": "/venue/Jin_Differentiable_Room_Acoustic_Rendering_with_Multi-View_Vision_Priors@ICCV2025@CVF",
    "published_date": null,
    "conference": "ICCV.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "GT-Loc: Unifying When and Where in Images Through a Joint Embedding Space",
    "authors": [
      "David G. Shatwell",
      "Ishan Rajendrakumar Dave",
      "Sirnam Swetha",
      "Mubarak Shah"
    ],
    "affiliations": [],
    "summary": "Timestamp prediction aims to determine when an image was captured using only visual information, supporting applications such as metadata correction, retrieval, and digital forensics. In outdoor scenarios, hourly estimates rely on cues like brightness, hue, and shadow positioning, while seasonal changes and weather inform date estimation. However, these visual cues significantly depend on geographic context, closely linking timestamp prediction to geo-localization. To address this interdependence, we introduce GT-Loc, a novel retrieval-based method that jointly predicts the capture time (hour and month) and geo-location (GPS coordinates) of an image. Our approach employs separate encoders for images, time, and location, aligning their embeddings within a shared high-dimensional feature space. Recognizing the cyclical nature of time, instead of conventional contrastive learning with hard positives and negatives, we propose a temporal metric-learning objective providing soft targets by modeling pairwise time differences over a cyclical toroidal surface. We present new benchmarks demonstrating that our joint optimization surpasses previous time prediction methods, even those using the ground-truth geo-location as an input during inference. Additionally, our approach achieves competitive results on standard geo-localization tasks, and the unified embedding space facilitates compositional and text-based image retrieval.",
    "link": "/venue/Shatwell_GT-Loc_Unifying_When_and_Where_in_Images_Through_a_Joint@ICCV2025@CVF",
    "published_date": null,
    "conference": "ICCV.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "Moto: Latent Motion Token as the Bridging Language for Learning Robot Manipulation from Videos",
    "authors": [
      "Yi Chen",
      "Yuying Ge",
      "Weiliang Tang",
      "Yizhuo Li",
      "Yixiao Ge",
      "Mingyu Ding",
      "Ying Shan",
      "Xihui Liu"
    ],
    "affiliations": [],
    "summary": "Recent developments in Large Language Models (LLMs) pre-trained on extensive corpora have shown significant success in various natural language processing (NLP) tasks with minimal fine-tuning. This success offers new promise for robotics, which has long been constrained by the high cost of action-labeled data. We ask: given the abundant video data containing interaction-related knowledge available as a rich \"corpus\", can a similar generative pre-training approach be effectively applied to enhance robot learning? The key challenge is to identify an effective representation for autoregressive pre-training that benefits robot manipulation tasks.Inspired by the way humans learn new skills through observing dynamic environments, we propose that effective robotic learning should emphasize motion-related knowledge, which is closely tied to low-level actions and is hardware-agnostic, facilitating the transfer of learned motions to actual robot actions. To this end, we introduce Moto, which converts video content into latent Motion Token sequences by a Latent Motion Tokenizer, learning a bridging \"language\" of motion from videos in an unsupervised manner. We pre-train Moto-GPT through motion token autoregression, enabling it to capture diverse visual motion knowledge. After pre-training, Moto-GPT demonstrates the promising ability to produce semantically interpretable motion tokens, predict plausible motion trajectories, and assess trajectory rationality through output likelihood.To transfer learned motion priors to real robot actions, we implement a co-fine-tuning strategy that seamlessly bridges latent motion token prediction and real robot control. Extensive experiments show that the fine-tuned Moto-GPT exhibits superior robustness and efficiency on robot manipulation benchmarks, underscoring its effectiveness in transferring knowledge from video data to downstream visual manipulation tasks.",
    "link": "/venue/Chen_Moto_Latent_Motion_Token_as_the_Bridging_Language_for_Learning@ICCV2025@CVF",
    "published_date": null,
    "conference": "ICCV.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "EVER: Exact Volumetric Ellipsoid Rendering for Real-time View Synthesis",
    "authors": [
      "Alexander Mai",
      "Peter Hedman",
      "George Kopanas",
      "Dor Verbin",
      "David Futschik",
      "Qiangeng Xu",
      "Falko Kuester",
      "Jonathan T. Barron",
      "Yinda Zhang"
    ],
    "affiliations": [],
    "summary": "We present Exact Volumetric Ellipsoid Rendering (EVER), a method for real-time 3D reconstruction.EVER accurately blends an unlimited number of overlapping primitives together in 3D space, eliminating the popping artifacts that 3D Gaussian Splatting (3DGS) and other related methods exhibit.EVER represents a radiance field as a set of constant-density volumetric ellipsoids, which are raytraced by intersecting each primitive twice (once upon ray entrance and another on ray exit) and accumulating the derivatives of the densities and colors along the ray.Because EVER is built around ray tracing, it also enables effects such as defocus blur and fish-eye camera distortion, while still achieving frame rates of 30 FPS at 720p on an NVIDIA RTX4090. We show that our method is more accurate on the challenging large-scale scenes from the Zip-NeRF dataset, where it achieves state of the art SSIM, even higher than Zip-NeRF.",
    "link": "/venue/Mai_EVER_Exact_Volumetric_Ellipsoid_Rendering_for_Real-time_View_Synthesis@ICCV2025@CVF",
    "published_date": null,
    "conference": "ICCV.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "Multi-View 3D Point Tracking",
    "authors": [
      "Frano Rajič",
      "Haofei Xu",
      "Marko Mihajlovic",
      "Siyuan Li",
      "Irem Demir",
      "Emircan Gündoğdu",
      "Lei Ke",
      "Sergey Prokudin",
      "Marc Pollefeys",
      "Siyu Tang"
    ],
    "affiliations": [],
    "summary": "We introduce the first data-driven multi-view 3D point tracker, designed to track arbitrary points in dynamic scenes using multiple camera views. Unlike existing monocular trackers, which struggle with depth ambiguities and occlusion, or prior multi-camera methods that require over 20 cameras and tedious per-sequence optimization, our feed-forward model directly predicts 3D correspondences using a practical number of cameras (e.g., four), enabling robust and accurate online tracking. Given known camera poses and either sensor-based or estimated multi-view depth, our tracker fuses multi-view features into a unified point cloud and applies k-nearest-neighbors correlation alongside a transformer-based update to reliably estimate long-range 3D correspondences, even under occlusion. We train on 5K synthetic multi-view Kubric sequences and evaluate on two real-world benchmarks--Panoptic Studio and DexYCB--achieving median trajectory errors of 3.1 cm and 2.0cm, respectively. Our method generalizes well to diverse camera setups of 1-8 views with varying vantage points and video lengths of 24-150 frames. By releasing our tracker alongside training and evaluation datasets, we aim to set a new standard for multi-view 3D tracking research and provide a practical tool for real-world applications. Project page: https://ethz-vlg.github.io/mvtracker.",
    "link": "/venue/Rajic_Multi-View_3D_Point_Tracking@ICCV2025@CVF",
    "published_date": null,
    "conference": "ICCV.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "Diving into the Fusion of Monocular Priors for Generalized Stereo Matching",
    "authors": [
      "Chengtang Yao",
      "Lidong Yu",
      "Zhidan Liu",
      "Jiaxi Zeng",
      "Yuwei Wu",
      "Yunde Jia"
    ],
    "affiliations": [],
    "summary": "The matching formulation makes it naturally hard for the stereo matching to handle ill-posed regions like occlusions and non-Lambertian surfaces. Fusing monocular priors has been proven helpful for ill-posed matching, but the biased monocular prior learned from small stereo datasets constrains the generalization. Recently, stereo matching has progressed by leveraging the unbiased monocular prior from the vision foundation model (VFM) to improve the generalization in ill-posed regions. We dive into the fusion process and observe three main problems limiting the fusion of the VFM monocular prior. The first problem is the misalignment between affine-invariant relative monocular depth and absolute depth of disparity. Besides, when we use the monocular feature in an iterative update structure, the over-confidence in the disparity update leads to local optima results. A direct fusion of a monocular depth map could alleviate the local optima problem, but noisy disparity results computed at the first several iterations will misguide the fusion. In this paper, we propose a binary local ordering map to guide the fusion, which converts the depth map into a binary relative format, unifying the relative and absolute depth representation. The computed local ordering map is also used to re-weight the initial disparity update, resolving the local optima and noisy problem. In addition, we formulate the final direct fusion of monocular depth to the disparity as a registration problem, where a pixel-wise linear regression module can globally and adaptively align them. Our method fully exploits the monocular prior to support stereo matching results effectively and efficiently. We significantly improve the performance from the experiments when generalizing from SceneFlow to Middlebury and Booster datasets while barely reducing the efficiency.",
    "link": "/venue/Yao_Diving_into_the_Fusion_of_Monocular_Priors_for_Generalized_Stereo@ICCV2025@CVF",
    "published_date": null,
    "conference": "ICCV.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  },
  {
    "id": null,
    "title": "HairCUP: Hair Compositional Universal Prior for 3D Gaussian Avatars",
    "authors": [
      "Byungjun Kim",
      "Shunsuke Saito",
      "Giljoo Nam",
      "Tomas Simon",
      "Jason Saragih",
      "Hanbyul Joo",
      "Junxuan Li"
    ],
    "affiliations": [],
    "summary": "We present a universal prior model for 3D head avatars with explicit hair compositionality. Existing approaches to build generalizable priors for 3D head avatars often adopt a holistic modeling approach, treating the face and hair as an inseparable entity. This overlooks the inherent compositionality of the human head, making it difficult for the model to naturally disentangle face and hair representations, especially when the dataset is limited. Furthermore, such holistic models struggle to support applications like 3D face and hairstyle swapping in a flexible and controllable manner. To address these challenges, we introduce a prior model that explicitly accounts for the compositionality of face and hair, learning their latent spaces separately. A key enabler of this approach is our synthetic hairless data creation pipeline, which removes hair from studio-captured datasets using estimated hairless geometry and texture derived from a diffusion prior. By leveraging a paired dataset of original and synthetic hairless captures, we train disentangled prior models for face and hair, incorporating compositionality as an inductive bias to facilitate effective separation. Our model's inherent compositionality enables seamless transfer of face and hair components between avatars while preserving identity. Additionally, we demonstrate that our model can be fine-tuned in a data-efficient manner using monocular captures to create high-fidelity, hair-compositional 3D head avatars for unseen subjects. These capabilities highlight the practical applicability of our approach in real-world scenarios, paving the way for flexible and expressive 3D avatar generation.",
    "link": "/venue/Kim_HairCUP_Hair_Compositional_Universal_Prior_for_3D_Gaussian_Avatars@ICCV2025@CVF",
    "published_date": null,
    "conference": "ICCV.2025",
    "conference_year": 2025,
    "track": null,
    "source": "paperscool"
  }
]